---
title: "Exploratory Factor Analysis"
toc: true
format:
  html:
    code-fold: true
bibliography: references.bib
---

::: callout-warning
This chapter is still a work in progress.
:::

The goal of an exploratory factor analysis (EFA) is to study the latent factors that underlie responses to a larger number of items. In other words, the goal is to *explore* the data and *reduce* the number of variables. It is a popular technique in the development and validation of assessment instruments.

Unlike confirmatory factor analysis (CFA), EFA is used when there is little or no a priori justification for specifying a particular structural model. This means that there is reasonable uncertainty about the number of underlying factors and which items load on which factor. The hope is to resolve some of that uncertainty empirically.

## Preparation

Before you perform a factor analysis, make sure that the design of your study is suitable for this type of analysis. This means you should have an appropriate sample, sample size, and indicators (items).

### Sample

It may be obvious, but make sure to target participants that are likely to vary in the attitude that you're interested in measuring and that the sample is representative of the population that you're interested in.

### Number of items

At least three items per factor are required for a factor model to be identified; more items per factor results in overidentification of the model. A number of writers recommend that four or more items per factor be included in the factor analysis to ensure an adequate identification of the factors (Comrey & Lee, 1992; Fabrigar et al., 1999; Gorsuch, 1988). @fabrigar1999 suggest to include at least 4 and perhaps as many as six for each factor.

MacCallum et al. (1999) found that in addition to the communality of the measures, the results were more accurate for given sample sizes if there were more measures per factor included in the analysis. Within the range of indicators studied (three to seven per factor), it is better to have more indicators than fewer. The critical point, however, is that these indicators must be reasonably valid and reliable.

Velicer & Fava (1998) suggest 6-10 initial variables per factor is recommended as 25% to 50% will not perform as expected and the end goal should be to have four- or five to-one as a minimum. They say that the bare minimum of three variables per factor is a dangerous goal. They also write that if loading is low, a ratio of 10 or 20 to 1 may be required for model identification.

Factor loadings can be improved by using multiple response (Likert-type) items, as they generally result in higher loadings than two-choice items (Comrey & Montag, 1982; Oswald & Velicer, 1980; Velicer, DiClemente, & Corriveau, 1984; Velicer, Govia, Cherico, & Corriveau, 1985; Velicer & Stevenson, 1978). Likewise, the quality of item writing can affect the size of the loadings, that is, the expression of an item in simple language, restricting the item to a single idea, or using content that is appropriate to a majority of respondents are all ways of improving items. Item quality can also be improved by using item parcels, that is, the total of several items, instead of individual items.

Some additional arguments for why multiple indicators per factor are useful:

-   People read items incorrectly or sometimes just don't understand an item (despite all efforts to make them as clear as possible), so having a sample of items can counteract the idiosyncratic mistakes of respondents

-   With complicated topics, having multiple items also adds clarity about the overall goal of the scale. Each item is an indicator to the participant about what the purpose of the scale is.

### Sample size

Rules of thumb are one way to determine an appropriate sample size, but the general consensus is that rules of thumb are a bad idea [@maccallum1999, @costello2005]. The reason for that is that rules of thumb only involve one of two factors that are relevant for determining the sample size: a set number of participants or a particular participant-to-item ratio. The reality is that the appropriate sample size is dependent on many more factors, such as the level of communalities, loadings, number of variables per factor, and the number of factors (Gagné & Hancock, 2006; MacCallum, Widaman, Preacher, & Hong, 2001; MacCallum et al., 1999; Marsh, Hau, Balla, & Grayson, 1998; Velicer & Fava, 1998).

For example, @maccallum1999 found that results were consistent with population loadings with sample sizes as low as 60 if the communalities of the items were high (.60 or greater).

[Fabrigar et al., 1999](https://journals.sagepub.com/doi/full/10.1177/0095798418771807#); [MacCallum, Widaman, Preacher, & Hong, 2001](https://journals.sagepub.com/doi/full/10.1177/0095798418771807#); [Mundfrom & Shaw, 2005](https://journals.sagepub.com/doi/full/10.1177/0095798418771807#); [Velicer & Fava, 1998](https://journals.sagepub.com/doi/full/10.1177/0095798418771807#)).

@costello2005 found that sample sizes in two years' worth of PsychINFO articles were relatively low. The majority of the studies in their survey (62.9%) had subject to item ratios of 10:1 or less. They also found that a ratio of 20:1 produces error rates well above the standard alpha (5%) level. This is based on a study in which different rules of thumb were used to analyze the factor structure of the Marsh's Self-Description Questionnaire. As this questionnaire might be more typical of psychological scales (at least more typical than many simulated data sets), it shows that sample sizes should be larger rather than smaller. They caution researchers to remember that EFA is a large-sample procedure and that generalizable or replicable results are unlikely if the sample is too small.

@mundfrom2005 performed a simulation study to determine the minimum sample size for a study, taking into account the number of factors, the ratio of items to factors, and the level of communality. They generated a variety of population correlation matrices under several different conditions, repeatedly sampled from these population structures, and determined coefficients of congruence between the sample solutions and population structures.

| Criterion        | Communality     | items/factor | n    |
|------------------|-----------------|--------------|------|
| Excellent (0.98) | High (.6 to .8) | 4            | 500  |
|                  |                 | 6            | 250  |
|                  |                 | 8            | 100  |
|                  | Wide (.2 to .8) | 4            | 900  |
|                  |                 | 6            | 200  |
|                  |                 | 8            | 130  |
|                  | Low (.2 to .4)  | 4            | 1400 |
|                  |                 | 6            | 260  |
|                  |                 | 8            | 130  |
| Good (0.92)      | High (.6 to .8) | 5            | 130  |
|                  |                 | 7            | 55   |
|                  | Wide (.2 to .8) | 5            | 140  |
|                  |                 | 8            | 60   |
|                  | Low (.2 to .4)  | 5            | 200  |
|                  |                 | 8            | 80   |

: Recommended minimum sample sizes by @mundfrom2005

For a similar study, see @velicer1998.

### Number of items

-   A minimum variable sample size is provided by the identifiability requirement of three variables per factor (T. W. Anderson & Rubin, 1956; McDonald & Krane, 1977, 1979; Rindskopf, 1984). from @velicer1998
-   the bare minimum of three variables per factor is a dangerous goal. A more prudent target would be to have four- or fiveto-one as a minimum @velicer1998 Personal experience suggests that, even under the best conditions, 25%50% of the initial items will have to be deleted. Therefore, a target of 6-10 initial variables per factor is recommended. a ratio of 20-30 initial variables per factor represents an appropriate target for extensive oversampling. In cases where the number of participants are few, it's also possible to extremely oversample.
-   , it is important for the latent variable to have at least four indicators (for example: four items). In effect, with only three indicators, the measurement model is just-identified, thus making useless its testability. (Gana & Broc)
-   A number of writers recommend that four or more items per factor be included in the factor analysis to ensure an adequate identification of the factors (Comrey & Lee, 1992; Fabrigar et al., 1999; Gorsuch, 1988). From @russell2002
-   5 or more strongly loading items (.50 or better) indicate a solid factor [@costello2005]

## Data Analysis

### Relevant items

One design issue that is especially important is measured variables to include in the study. If measured variables irrelevant to the domain of interest are included, then spurious common factors might emerge or true common factors might be obscured [@fabrigar1999].

### Sampling Adequacy

The Kaiser--Meyer--Olkin (KMO) test is a statistical measure to determine how suitable the data is for factor analysis. The test measures sampling adequacy for the complete model and each variable in the model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the higher the KMO value, the more suitable the data is.

The KMO value ranges from 0 to 1, with 0.60 considered suitable for factor analysis @tabachnick2013. Kaiser himself [-@kaiser1974] suggested that KMO \> .9 were marvelous, in the .80s, meritorious, in the .70s, middling, in the .60s, mediocre, in the 50s, miserable, and less than .5, unacceptable.

Bartlett's (1954) test of sphericity is a notoriously sensitive test of the hypothesis that the correlations in a correlation matrix are zero. Because of its sensitivity and its dependence on N, the test is likely to be significant with samples of substantial size even if correlations are very low. Therefore, use of the test is recommended only if there are fewer than, say, five cases per variable [@tabachnick2013].

### Principal Components vs. Factor Analysis

An important consideration is whether to use a principal components analysis (PCA) or a factor analysis. PCA is a data reduction technique and while a factor analysis is also a type of data reduction technique, the focus with PCA is more on simply reducing the data, without regard for a theoretical interpretation. With PCA, the variables themselves are of interest, rather than a hypothetical latent construct. Constructs are conceptualized as being causally determined by the observations; that is, CFA reflects a formative model rather than a reflective one [@edwards2000]. If you are a psychologist trying to create a measure of a psychological construct, this is probaly not what you want as principal component scores are "caused" by their indicators in much the same way that sum scores are "caused" by item scores." \[@borsboom2006, p. 426\]. Instead, you likely want the opposite causal relationship in which a latent factor causes the indicator scores.

```{r, PCA-FA-figure}
DiagrammeR::grViz("
  digraph dot {
    graph [
      layout = dot, 
      rankdir = LR, 
      fontname = 'Source Sans Pro', 
      label = 'Illustration of a PCA model (left) and a factor analysis model (right).'
    ]
    
    node [fontname = 'Source Sans Pro']
    node [shape = square]
    var1 [label = 'item 1']
    var2 [label = 'item 2']
    var3 [label = 'item 3']
    var4 [label = 'item 4']
    var5 [label = 'item 1']
    var6 [label = 'item 2']  
    var7 [label = 'item 3']  
    var8 [label = 'item 4']
  
    node [shape = circle]
    construct_a [label = 'construct A']
    construct_b [label = 'construct B']
  
    edge [color = black, minlen = 3]
    {var1 var2 var3 var4} -> construct_a
    construct_b -> {var5 var6 var7 var8}
  
    edge[style = invis, minlen = 1];
    construct_a -> construct_b
  }"
)
```

A related issue is that factor analysis assumes that the total variance can be partitioned into common and unique variance and PCA assumes that the common variances takes up all of total variance. This means that PCA assumes that all variables are measured without error. It is usually more reasonable to assume that you have not measured your set of items perfectly.

Although some argue that the two methods have the same results [@velicer1990], there is evidence that those similarities are mistaken and that factor analysis has better results than PCA [@widaman1993].

**Conclusion**: Do not use PCA.

### Factor Extraction Method

Extraction is the general term for the process of reducing the number of dimensions being analyzed from the number of variables in the data set (and matrix of associations) into a smaller number of factors.

There are multiple factor extract methods, such as:

-   minres

-   unweighted least squares (ULS)

-   generalized least squares (GLS)

-   maximum likelihood

-   principal axis factor(ing)

-   alpha factor(ing)

-   image factor(ing)

It's not entirely clear which factor extraction method is the best and some authors use different terms for some of the methods, making it more difficult to compare them.

Fabrigar, Wegener, MacCallum and Strahan -@fabrigar1999 argued that if data are relatively normally distributed, maximum likelihood is the best choice because "it allows for the computation of a wide range of indexes of the goodness of fit of the model and permits statistical significance testing of factor loadings and correlations among factors and the computation of confidence intervals." (p. 277). In case the data is not generally normally distributed, they recommend principal axis factoring. This is recommended in several sources (@costello2005; @osborne2014).

**Conclusion**: Use the maximum likelihood method if the data is generally normally distributed and to use principal axis factoring if the data is non-normal.

### Rotation methods

The goal of rotation is to clarify the factor structure and make the results of the EFA more interpretable.

Rotation methods can be categorized into one of two categories: orthogonal or oblique. Orthogonal rotations keep axes at a 90 degree angle, forcing the factors to be uncorrelated. Oblique rotations allow angles that are not 90 degree , thus allowing factors to be correlated if that is optimal for the solution.

Orthogonal rotation methods include:

-   varimax

-   quartimax

-   equamax

Oblique rotation methods include:

-   direct oblimin

-   quartimin

-   promax

In the social sciences, we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution. If the factors are truly uncorrelated, orthogonal and oblique rotation produce nearly identical results. Since oblique rotation will reproduce an orthogonal solution but not vice versa, it makes sense to go for oblique rotation.

There is no widely preferred method of oblique rotation; all tend to produce similar results (Fabrigar et al., 1999).

**Conclusion**: Use any oblique rotation.

### Number of factors

There are several methods to determine how many factors to retain:

-   Theory

-   Kaiser criterion (eigenvalues greater than 1; Kaiser, 1960)

-   Scree test (Cattell, 1966)

-   Parallel analysis (Horn, 1965)

-   Velicer's multiple average partial (MAP) procedure (Velicer, 1976)

-   Akaike information criterion (AIC; Akaike, 1974)

-   Bayesian information criterion (BIC; Schwarz, 1978)

-   Comparison data (CD; @ruscio2012)

-   Very Simple Structure (VSS)

-   Likelihood ratio statistic

If the theoretical framework for the instrument is sound, we should start with the expectation that we should see that structure in the data. Instruments are rarely perfect (especially the first time it is examined), and theoretical expectations are not always supported. But unless one is on a fishing expedition in a data set with no a priori ideas about how the analysis should turn out, 12 this is as good a place as any to start.

The default in most statistical software packages is to use the Kaiser criterion. It makes some sense, as an eigenvalue represents the sum of the squared factor loadings in a column, and to get a sum of 1.0 or more, one must have rather large factor loadings to square and sum. However, this is easily achieved with more items and there are now alternative methods. Hence, there is broad consensus in the literature that this is among the least accurate methods for selecting the number of factors to retain (Velicer & Jackson, 1990).

The scree test involves examining the graph of the eigenvalues (available via every software package) and looking for the natural bend or "elbow" in the data where the slope of the curve changes (flattens) markedly. Although the scree plot itself is not considered sufficient to determine how many factors should be extracted (Velicer et al., 2000), many suggest that researchers examine solutions extracting the number of factors ranging from one to two factors above the elbow to one or two below.

Parallel analysis involves generating random uncorrelated data, and comparing eigenvalues from the EFA to those eigenvalues from those random data. Using this process, only factors with eigenvalues that are significantly above the mean (or preferably, the 95 th percentile) of those random eigenvalues should be retained. Several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).

The Minimum Average Partial (MAP) criterion involves partialing out common variance as each successive component is created. As each successive component is partialed out, common variance will decrease to a minimum. Velicer argued that minimum point should be considered the criterion for the number of factors to extract.

VSS involves degrading the initial rotated factor solution by **assuming** that the nonsalient loadings are zero, even though in actuality they rarely are. What VSS does is test how well the factor matrix we **think** abaut and talk about actually fits the correlation matrix. It is not a confirmatory procedure for testing the significance of a particular loading, but rather it is an exploratory procedure for testing the relative utility of **interpreting** the correlation matrix in terms of a family of increasingly more complex factor models. The simplest model tasted by VSS is that each item is of complexity one, and that all items are embedded in a more complex factor matrix of rank k. This is the model most appropriate for scale construction and is the one we use most frequently when we talk about factor solutions. More complicated models may also be evaluated by VSS.

Zwick and Velicer (1986) tested the scree test, Horn's parallel test, and Velicer's MAP test (among others) in simulation studies using a data set with a clear factor structure. Both the parallel test and MAP test seemed to work well.

@ruscio2012 notes that PA is considered to be the method of choice among methodologists and recommend that researchers take advantage of PA as a starting point, perhaps supplemented by CD. They note that researchers can also use more than one method. Similarly, several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).

@osborne2014 notes that MAP has been considered superior to the "classic" criteria, and probably is superior to parallel analysis, although neither is perfect, and all must be used in the context of a search for conceptually interpretable factors. He recommends to use parallel analysis or MAP criteria, along with theory (and any of the classic criteria that suits you and is defensible).

Finally, empirical research suggests that overfactoring introduces much less error to factor loading estimatesihan underfactoring (Fava & Velicer. 1992; Wood et..;!., 1996). Although be wary of Solutions with too many factors mightprompt a researcher to postulate the existence of cons tracts with little theoretical value and thereby develop unnecessarily complex theories.

**Conclusion:** Use multiple criteria, including theory, to make a judgment call about how many factors to extract. When in doubt, favor more factors rather than fewer factors.

## Interpretation

Remember that the goal of exploratory factor analysis is to explore whether your data fits a model that makes sense. Ideally, you have a conceptual or theoretical framework for the analysis- a theory or body of literature guiding the development of an instrument, for example. Even if you do not, the results should be sensible in some way. You should be able to construct a simple narrative describing how each factor, and its constituent variables, makes sense and is easily labeled.

### Factor Loadings

Tabachnick and Fidell (2001) cite .32 as a good rule of thumb for the minimum loading of an item. Others say item loadings above .30 (Costello).

Clark and Watson, 1995 say \> .35 @clark1995

Others say a factor loading coefficient ≥ 0.40 is considered necessary to judge the quality (validity) of an item as an indicator of one factor or another. Some may think that this criterion is quite lax since a factor loading of 0.40 means that only 16% of the explained variance depends on the factor of which one item is the indicator (Gana & Broc)

Some psychometricians recommend that we choose items whose R² is greater than 0.40 (thus showing a factorial factor loading greater than 0.63).

-   A "crossloading" item is an item that loads at .32 or higher on two or more factors. The researcher needs to decide whether a crossloading item should be dropped from the analysis, which may be a good choice if there are several adequate to strong loaders (.50 or better) on each factor.

    A factor with fewer than three items is generally weak and unstable; 5 or more strongly loading items (.50 or better) are desirable and indicate a solid factor.

Comrey and Lee (1992) suggest that loadings in excess of .71 (50% overlapping variance) are considered excellent, .63 (40% overlapping variance) very good, .55 (30% overlapping variance) good, .45 (20% overlapping variance) fair, and .32 (10% overlapping variance) poor.

one must be careful not to prematurely drop poorly performing items, especially when such items were predicted a priori to be strong markers of a given factor.

### 

### Reliability

Note that Cronbach's alpha relies on the assumption of unidimensionality.

Internal consistency:

Recommended cut off points are:

-   .80 in a basic science setting [@clark1995] Nunally 1978

-   .9 or .5 in applied settings Nunally 1978

Test-retest reliability:

## Recommended Reading

The summaries and conclusions above were largely taken from the following sources:

-   Best Practices in Exploratory Factor Analysis by @osborne2014
-   In Search of Underlying Dimensions: The Use (and Abuse) of Factor Analysis in Personality and Social Psychology Bulletin by @russell2002
-   Evaluating the Use of Exploratory Factor Analysis in Psychological Research by @fabrigar1999
-   @costello2005

## Glossary

-   communality: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. *communality*.

-   eigenvalue: the amount of variance explained by a factor; the sum of squared factor loadings

-   h2: See communality.

-   `u2`: 1 - `h2`. residual variance, a.k.a. *uniqueness*

## References
