[["index.html", "How to Science About", " How to Science 2021-12-02 About This is a book to organize my conclusions about topics in methodology and statistics. "],["survey-design.html", "Chapter 1 Survey design 1.1 Response options 1.2 Odd vs. even response options 1.3 Response option labeling", " Chapter 1 Survey design 1.1 Response options The question of how many response options to use centers around two main concerns. The first is that more options means you can obtain a more fine-grained assessment of the characteristic that is being evaluated (e.g., attitude). In other words, your assessment is more precise. However, the question is how the number of options affects the reliability and the validity of the measurement. With more options, it becomes more difficult for people to distinguish between the different options (e.g., is “Strongly agree” reliably different from “Very strongly agree?”). Table 1 shows an overview of various studies in which the topic of response options was addressed. The studies vary in many ways, so the final conclusion should be a holistic interpretation of the results, rather than a simple tallying of the results. Note also that only empirical studies are included and not simulation studies. Simulation studies seem limited because they cannot address the plausible psychological limitation of people being unable to distinguish between many options. Table 1: Overview of empirical studies on the topic of response options. Source Comparisons Conclusion Simms et al. (2019) 2- to 11-point Likert + VAS 6-point Likert Alwin (1997) 7- and 11-point Likert 11-point Likert Eutsler and Lang (2015) 5-, 7-, 9-, and 11-point Likert 7-point Likert Matell and Jacoby (1971) 2-point to 18-point Likert No difference Janhunen (2012) 7-point Likert and 30-point VAR VAR Sung and Wu (2018) 5-point Likert and VAS-RRP VAS-RPP Jaeschke, Singer, and Guyatt (1990) 7-point Likert and VAS No difference (slightly favor 7-point Likert) Kuhlmann, Dantlgraber, and Reips (2017) 5-point Likert and VAS No difference Lewis (2017) 7-, 11-point Likert and VAS No difference Donnellan and Rakhshani (2020) 2- to 7-, and 11-point Likert 5-point Likert or higher Hilbert (2016) 2- and 5-point and VAS It depends Finn, Ben-Porath, and Tellegen (2015) 2- and 4-point Likert 4-point Likert Flamer (1983) 2- and 9-point Likert 9-point Likert Weng (2004) 3- to 9-point Likert 5-point or higher Bendig (1954) 2-, 3-, 5-, 7-, and 9-point Likert No difference (maybe 3-point or higher) Preston and Colman (2000) 2- to 11-point Likert and VAS 7-, 9-, or 10-point Likert Capik and Gozum (2015) 2- and 5-point Likert No difference Cox et al. (2012) 2- and 4-point Likert 4-point Likert Cox et al. (2017) 2- and 4-point Likert Mixed Dawes (2008) 5-, 7-, and 10-point Likert No difference There are also several review papers on the topic. Krosnick and Presser (2010) suggest that 7-point Likert scales are probably optimal. Lietz (2010) concludes a desirable Likert-scale consists of 5 to 8 response options. Similarly, Cox III (1980) recommends to use between 5 and 9 response options. Symonds (1924), in 1924, claims the optimum number is 7. Besides psychometric properties it may also be worth taking into account respondent preference. This involves ease of use of the scale and whether the response options allow for sufficient variation for respondents to express their view. Preston and Colman (2000) found that respondents found scales with 5, 7, and 10 points easy to use (compared to fewer options and a VAS) and that they preferred scales with more response options to allow them to express themselves (7 or more). Other studies also show that respondents favor more options (Cox et al. 2017). Note that if time is of the essence, fewer response options are preferred. Conclusion: It appears that few response options (2 or 3) should definitely be avoided. More response options therefore seems better, but benefits seem to quickly level off. Given other concerns, such as ease of use and interpretability, a 7-point Likert scale seems to be preferred. 1.2 Odd vs. even response options The middle option of a scale can have an ambiguous meaning. Participants may use it to indicate a moderate standing on the issue, to indicate uncertainty, to indicate confusion, or to signal context dependence (e.g., “it depends”). The middle option may also be used for social desirable responding. Conclusion: If it is possible that respondents may have a moderate view, it seems crucial for it to be possible to capture this view. Limitations of a middle option could then be addressed in other ways (e.g., clear questions). 1.3 Response option labeling There are several studies that show all response options should be labelled, rather than only labeling the end points (Weng 2004; Krosnick and Berent 1993). For an example of labels for a 2- to 11-point Likert scale, see Table 1. Table 1: Likert response labels from Simms et al. (2019) Label 2-point 3-point 4-point 5-point 6-point 7-point 8-point 9-point 10-point 11-point Very strongly disagree x x x x Strongly disagree x x x x x x x x Disagree x x x x x x x x x x Mostly disagree x x Slightly disagree x x x x x x Neither agree nor disagree x x x x x Slightly agree x x x x x x Mostly agree x x Agree x x x x x x x x x x Strongly agree x x x x x x x x Very strongly agree x x x x References "],["factor-analysis.html", "Chapter 2 Factor analysis 2.1 Exploratory factor analyses 2.2 Confirmatory factor analysis 2.3 Examples 2.4 R packages 2.5 Useful links", " Chapter 2 Factor analysis 2.1 Exploratory factor analyses The goal of an exploratory factor analysis (EFA) is to study the latent factors that underlie responses to a larger number of measures items. It is a popular technique in the development and validation of assessment instruments. Unlike confirmatory factor analysis, EFA is used when there is little or no a priori justification for specifying a particular structural model. This means that the number of underlying factors is determined mostly empirically, rather than theoretically. 2.1.1 Principal Components vs. Factor Analysis The partitioning of variance differentiates a principal components analysis from what we call common factor analysis. Both methods try to reduce the dimensionality of the dataset down to fewer unobserved variables, but whereas PCA assumes that there common variances takes up all of total variance, common factor analysis assumes that total variance can be partitioned into common and unique variance. It is usually more reasonable to assume that you have not measured your set of items perfectly. The unobserved or latent variable that makes up common variance is called a factor, hence the name factor analysis. The other main difference between PCA and factor analysis lies in the goal of your analysis. If your goal is to simply reduce your variable list down into a linear combination of smaller components then PCA is the way to go. However, if you believe there is some latent construct that defines the interrelationship among items, then factor analysis may be more appropriate. 2.1.2 Sample size 2.1.2.1 Rules of thumb The general consensus seems to be that rules of thumb are a bad idea. The reason for that is that these rules only involve one of two factors. Common rules of thumb consist of going for a set number of participants or a particular participant-to-item ratio. Table: Examples of rules of thumb. Type Source Rule Participant-only Comrey and Lee (1992) 50 = very poor 100 = poor 200 = fair 300 = good 500 = very good 1000+ = excellent Cattell (1978) 250 Participant:item ratio Gorsuch (2015) 5:1 (100 participants minimum) Everitt (1975) 10:1 (optimistically) The reality is that the sample size is dependent on: the study design (e.g., cross-sectional vs. longitudinal) the size of the relationships among the indicators (e.g., high communalities without cross loadings; strong loadings) the reliability of the indicators the scaling (e.g., categorical, continuous) and distribution of the indicators estimator type (e.g., ML, robust ML, WLSMV) the amount and patterns of missing data the size of the model (model complexity) Costello and Osborne (2005) caution researchers to remember that EFA is a large-sample procedure and that generalizable or replicable results are unlikely if the sample is too small. They found that a ratio of 20:1 produces error rates well above the standard alpha (5%) level. 2.1.3 Sampling Adequacy The Kaiser–Meyer–Olkin (KMO) test is a statistical measure to determine how suited data is for factor analysis. The test measures sampling adequacy for each variable in the model and the complete model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the higher the KMO value, the more suited the data is to factor analysis The KMO value ranges from 0 to 1, with 0.60 considered suitable for factor analysis Tabachnick and Fidell (2013). Kaiser himself (1974) suggested that KMO &gt; .9 were marvelous, in the .80s, mertitourious, in the .70s, middling, in the .60s, medicore, in the 50s, miserable, and less than .5, unacceptable. Bartlett’s (1954) test of sphericity is a notoriously sensitive test of the hypothesis that the correlations in a correlation matrix are zero. Because of its sensitivity and its dependence on N, the test is likely to be significant with samples of substantial size even if correlations are very low. Therefore, use of the test is recommended only if there are fewer than, say, five cases per variable (Tabachnick and Fidell 2013). 2.1.4 Outlier removal Should you remove outliers? 2.1.5 Number of factors to retain There are several methods to determine how many factors to retain: Kaiser criterion (eigenvalues greater than 1; Kaiser, 1960) Scree test (Cattell, 1966) Parallel analysis (Horn, 1965) Velicer’s multiple average partial (MAP) procedure (Velicer, 1976) Akaike information criterion (AIC; Akaike, 1974) Bayesian information criterion (BIC; Schwarz, 1978) Comparison data (CD; Ruscio and Brendan Roche (2012)) Very Simple Structure (VSS) The default in most statistical software packages is to retain all factors with eigenvalues greater than 1. There is broad consensus in the literature that this is among the least accurate methods for selecting the number of factors to retain (Velicer &amp; Jackson, 1990). Zwick and Velicer (1986) tested the scree test, Horn’s parallel test, and Velicer’s MAP test (among others) in simulation studies using a data set with a clear factor structure. Both the parallel test and the minimum average partial test seemed to work well. Ruscio and Brendan Roche (2012) notes that PA is considered to be the method of choice among methodologists and recommend that researchers take advantage of PA as a starting point, perhaps supplemented by CD. They note that researchers can also use more than one method. Similarly, several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma &amp; Valero-Mora, 2007; Velicer et al., 2000). Osborne (2014) notes that MAP has been considered superior to the “classic” criteria, and probably is superior to parallel analysis, although neither is perfect, and all must be used in the context of a search for conceptually interpretable factors. He recommends to use parallel analysis or MAP criteria, along with theory (and any of the classic criteria that suits you and is defensible). 2.1.6 Factor Extraction Method There are multiple factor extract methods, such as: minres unweighted least squares generalized least squares maximum likelihood principal axis factoring alpha factoring image factoring. Advice is to use the maximum likelihood method if the data is generally normally distributed and to use principal axis factoring if the data is non-normal (Costello and Osborne (2005), Osborne (2014)). 2.1.7 Rotation methods There are several rotation methods. They can be categorized into one of the categories: orthogonal or oblique. Orthogonal rotation methods include: varimax quartimax equamax Oblique rotation methods include: direct oblimin quartimin promax In the social sciences we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution. If the factors are truly uncorrelated, orthogonal and oblique rotation produce nearly identical results. Since oblique rotation will reproduce an orthogonal solution but not vice versa, we recommend oblique rotation. There is no widely preferred method of oblique rotation; all tend to produce similar results (Fabrigar et al., 1999) 2.1.8 Factor structure Tabachnick and Fidell (2001) cite .32 as a good rule of thumb for the minimum loading of an item. Others say item loadings above .30 (Costello). Others say a factor loading coefficient ≥ 0.40 is considered necessary to judge the quality (validity) of an item as an indicator of one factor or another. Some may think that this criterion is quite lax since a factor loading of 0.40 means that only 16% of the explained variance depends on the factor of which one item is the indicator (Gana &amp; Broc) Some psychometricians (for example, \\[COM 92, TAB 07\\]) recommend that we choose items whose R² is greater than 0.40 (thus showing a factorial factor loading greater than 0.63). A “crossloading” item is an item that loads at .32 or higher on two or more factors. The researcher needs to decide whether a crossloading item should be dropped from the analysis, which may be a good choice if there are several adequate to strong loaders (.50 or better) on each factor. A factor with fewer than three items is generally weak and unstable; 5 or more strongly loading items (.50 or better) are desirable and indicate a solid factor. , it is important for the latent variable to have at least four indicators (for example: four items). In effect, with only three indicators, the measurement model is just-identified, thus making useless its testability. (Gana &amp; Broc) Comrey and Lee (1992) suggest that loadings in excess of .71 (50% overlapping variance) are considered excellent, .63 (40% overlapping variance) very good, .55 (30% overlapping variance) good, .45 (20% overlapping variance) fair, and .32 (10% overlapping variance) poor. 2.2 Confirmatory factor analysis For confirmatory factor analyses there are also several ways to determine the sample size, including: Rules of thumb Satorra-Saris method Monte Carlo approach 2.2.1 Estimator If the data is normally distributed, use the ML estimator. the findings of Chou and Bentler \\[CHO 95\\] make it possible to qualify these remarks. These authors showed that in the presence of a sufficiently large sample, maximum likelihood estimation method and generalized least squares method do not make the results suffer, even when the multivariate normality is slightly violated. In case of violations, the following estimators can be used: MLM MLR: Similar to MLM but better suited for small sample sizes (Gana &amp; Broc). weighted least squares method (WLS): Requires a large sample size (FLO 04) and frequently runs into convergence issues and improper solutions Diagonally Weighted Least Squares (DWLS): For small samples and non-normal data ([JÖR 89) WLSM: Robust alternative to DWLS WLSMV: Robust alternative to DWLS bootstrapping: Not recommended for dichotomous and ordinal measures with few response categories) Mardia calculation to determine violations from normal distribution. It can be noted that not only is this standardized coefficient statistically significant, but its value is greater than 5, which is the threshold value beyond which multivariate normality seems to fail \\[KLI 16\\]. Recommendations concerning the main estimators available in lavaan according to the type of data (Gana &amp; Broc). Data type and normality assumption Recommended estimator Continuous data Approximately normal distribution ML Violation of normality assumption ML (in case of moderate violation) MLM, MLR, Bootstrap Ordinal/categorical data Approximately normal distribution ML (if at least 6 response categories) MLM, MLR (if at least 4 response categories) WLSMV (binary response or 3 response categories) Violation of normality assumption ML (if at least 6 response categories) MLM, MLR (if at least 4 response categories) WLSMV (in case of severe violation) 2.2.2 Fit indices The only advice we can afford to give, however, is to closely follow developments concerning fit indices. In the meantime, you can carefully follow the recommendations of Hu and Bentler \\[HU 99\\] or Schreiber and his co-authors \\[SCH 06\\] who suggest the following guidelines for judging a model goodness-of-fit (based on the hypothesis where the maximum likelihood method is the estimation method): 1) RMSEA value ≤ 0.06, with confidence interval at 90% values should be between 0 and 1.00; 2) SRMR value ≤ 0.08; and 3) CFI and TLI values ≥ 0.95. Fit indices guidance from Gana and Broc (2019) Fit Type Index Benchmark Absolute RMR/SRMR ≤ 0.08 = good fit WRMR ≤ 1.00 = good fit Parsimonious PRATIO Between 0.00 (saturated model) and 1.00 (parsimonious model) RMSEA ≤ 0.05 = very good fit ≤ 0.06 and ≤ 0.08 = good fit AIC Comparative index: the lower value of this index, the better the fit BIC Comparative index: the lower value of this BIC index, the better the fit Incremental CFI ≥ 0.90 and ≤ 0.94 = good fit ≥ 0.95 = very good fit TLI ≥ 0.90 and ≤ 0.94 = good fit ≥ 0.95 = very good fit 2.2.3 Reliability Reliability increases the closer the value gets to 1.00, with an acceptability threshold of 0.70. 2.3 Examples Beyond Sacrificial Harm: A Two-Dimensional Model of Utilitarian Psychology 2.4 R packages lavaan lavaan.survey semTools semPower 2.5 Useful links https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/ References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
