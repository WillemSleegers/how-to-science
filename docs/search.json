[
  {
    "objectID": "chapters/statistics/index.html",
    "href": "chapters/statistics/index.html",
    "title": "Overview",
    "section": "",
    "text": "This section is about statistics-related topics. At the moment there are two chapters. One of exploratory factor analyses and one of confirmatory factor analyses."
  },
  {
    "objectID": "chapters/statistics/factor-analysis/recommended-reading.html",
    "href": "chapters/statistics/factor-analysis/recommended-reading.html",
    "title": "Recommended reading",
    "section": "",
    "text": "https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/"
  },
  {
    "objectID": "chapters/statistics/factor-analysis/examples.html",
    "href": "chapters/statistics/factor-analysis/examples.html",
    "title": "Examples",
    "section": "",
    "text": "- Beyond Sacrificial Harm: A Two-Dimensional Model of Utilitarian Psychology"
  },
  {
    "objectID": "chapters/statistics/factor-analysis/EFA.html",
    "href": "chapters/statistics/factor-analysis/EFA.html",
    "title": "Exploratory Factor Analysis",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\n\n\nThe goal of an exploratory factor analysis (EFA) is to study the latent factors that underlie responses to a larger number of items. In other words, the goal is to explore the data and reduce the number of variables. It is a popular technique in the development and validation of assessment instruments. Unlike confirmatory factor analysis (CFA), EFA is used when there is little or no a priori justification for specifying a particular structural model. This means that there is reasonable uncertainty about the number of underlying factors and which items load on which factor. The hope is to resolve some of that uncertainty empirically.\nNote that if you have a clear theoretical basis about the number of factors and which items should load on which factor, a CFA is more appropriate than EFA because it allows you to directly test your model. You could also do both, but a CFA should be conducted on new data to test its validity.\n\nPrincipal Components vs. Factor Analysis\nAn important consideration is whether to use a principal components analysis (PCA) or a factor analysis. PCA is a data reduction technique and while a factor analysis is also a type of data reduction technique, the focus with PCA is more on simply reducing the data, without regard for a theoretical interpretation. With PCA, the variables themselves are of interest, rather than some hypothetical latent construct. Another way of describing it is that PCA conceptualizes constructs as causally determined by the observations. Again, this is likely not what you want because this means that “principal component scores are”caused” by their indicators in much the same way that sum scores are “caused” by item scores.” [Borsboom (2006), p. 426]. PCA is a formative model and not a reflective model (Edwards & Bagozzi, 2000). Instead, you likely want the opposite causal relationship in which a latent factor causes the indicator scores.\n\n\nCode\nDiagrammeR::grViz(\"\n  digraph dot {\n    graph [\n      layout = dot, \n      rankdir = LR, \n      fontname = Helvetica, \n      label = 'Illustration of a PCA model (left) and a factor analysis model (right).'\n    ]\n    \n    node [fontname = Helvetica]\n    node [shape = square]\n    var1 [label = 'item 1']\n    var2 [label = 'item 2']\n    var3 [label = 'item 3']\n    var4 [label = 'item 4']\n    var5 [label = 'item 1']\n    var6 [label = 'item 2']  \n    var7 [label = 'item 3']  \n    var8 [label = 'item 4']\n  \n    node [shape = circle]\n    construct_a [label = 'construct A']\n    construct_b [label = 'construct B']\n  \n    edge [color = black, minlen = 3]\n    {var1 var2 var3 var4} -> construct_a\n    construct_b -> {var5 var6 var7 var8}\n  \n    edge[style = invis, minlen = 1];\n    construct_a -> construct_b\n  }\"\n)\n\n\n\n\n\n\nA related issue is that factor analysis assumes that total variance can be partitioned into common and unique variance and PCA assumes that the common variances takes up all of total variance. This means that PCA assumes that all variables are measured without error. It is usually more reasonable to assume that you have not measured your set of items perfectly. Although some argue that the two methods have the same results (Velicer & Jackson, 1990), there is evidence that those similarities are mistaken and that factor analysis has better results than PCA (Widaman, 1993).\nConclusion: Do not use PCA.\n\n\nFactor Extraction Method\nExtraction is the general term for the process of reducing the number of dimensions being analyzed from the number of variables in the data set (and matrix of associations) into a smaller number of factors.\nThere are multiple factor extract methods, such as:\n\nminres\nunweighted least squares (ULS)\ngeneralized least squares (GLS)\nmaximum likelihood\nprincipal axis factor(ing)\nalpha factor(ing)\nimage factor(ing)\n\nIt’s not entirely clear which factor extraction method is the best and some authors use different terms for some of the methods, making it more difficult to compare them.\nFabrigar, Wegener, MacCallum and Strahan -Fabrigar et al. (1999) argued that if data are relatively normally distributed, maximum likelihood is the best choice because “it allows for the computation of a wide range of indexes of the goodness of fit of the model and permits statistical significance testing of factor loadings and correlations among factors and the computation of confidence intervals.” (p. 277). In case the data is not generally normally distributed, they recommend principal axis factoring. This is recommended in several sources (Costello & Osborne (2005); Osborne (2014)).\nConclusion: Use the maximum likelihood method if the data is generally normally distributed and to use principal axis factoring if the data is non-normal.\n\n\nRotation methods\nThe goal of rotation is to clarify the factor structure and make the results of the EFA more interpretable.\nRotation methods can be categorized into one of two categories: orthogonal or oblique. Orthogonal rotations keep axes at a 90 degree angle, forcing the factors to be uncorrelated. Oblique rotations allow angles that are not 90 degree , thus allowing factors to be correlated if that is optimal for the solution.\nOrthogonal rotation methods include:\n\nvarimax\nquartimax\nequamax\n\nOblique rotation methods include:\n\ndirect oblimin\nquartimin\npromax\n\nIn the social sciences, we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution. If the factors are truly uncorrelated, orthogonal and oblique rotation produce nearly identical results. Since oblique rotation will reproduce an orthogonal solution but not vice versa, it makes sense to go for oblique rotation.\nThere is no widely preferred method of oblique rotation; all tend to produce similar results (Fabrigar et al., 1999).\nConclusion: Use any oblique rotation.\n\n\nNumber of factors to retain\nThere are several methods to determine how many factors to retain:\n\nTheory\nKaiser criterion (eigenvalues greater than 1; Kaiser, 1960)\nScree test (Cattell, 1966)\nParallel analysis (Horn, 1965)\nVelicer’s multiple average partial (MAP) procedure (Velicer, 1976)\nAkaike information criterion (AIC; Akaike, 1974)\nBayesian information criterion (BIC; Schwarz, 1978)\nComparison data (CD; Ruscio & Brendan Roche (2012))\nVery Simple Structure (VSS)\n\nIf the theoretical framework for the instrument is sound, we should start with the expectation that we should see that structure in the data. Instruments are rarely perfect (especially the first time it is examined), and theoretical expectations are not always supported. But unless one is on a fishing expedition in a data set with no a priori ideas about how the analysis should turn out, 12 this is as good a place as any to start.\nThe default in most statistical software packages is to use the Kaiser criterion. It makes some sense, as an eigenvalue represents the sum of the squared factor loadings in a column, and to get a sum of 1.0 or more, one must have rather large factor loadings to square and sum. However, this is easily achieved with more items and there are now alternative methods. Hence, there is broad consensus in the literature that this is among the least accurate methods for selecting the number of factors to retain (Velicer & Jackson, 1990).\nThe scree test involves examining the graph of the eigenvalues (available via every software package) and looking for the natural bend or “elbow” in the data where the slope of the curve changes (flattens) markedly. Although the scree plot itself is not considered sufficient to determine how many factors should be extracted (Velicer et al., 2000), many suggest that researchers examine solutions extracting the number of factors ranging from one to two factors above the elbow to one or two below.\nParallel analysis involves generating random uncorrelated data, and comparing eigenvalues from the EFA to those eigenvalues from those random data. Using this process, only factors with eigenvalues that are significantly above the mean (or preferably, the 95 th percentile) of those random eigenvalues should be retained. Several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).\nThe Minimum Average Partial (MAP) criterion involves partialing out common variance as each successive component is created. As each successive component is partialed out, common variance will decrease to a minimum. Velicer argued that minimum point should be considered the criterion for the number of factors to extract.\nVSS involves degrading the initial rotated factor solution by assuming that the nonsalient loadings are zero, even though in actuality they rarely are. What VSS does is test how well the factor matrix we think abaut and talk about actually fits the correlation matrix. It is not a confirmatory procedure for testing the significance of a particular loading, but rather it is an exploratory procedure for testing the relative utility of interpreting the correlation matrix in terms of a family of increasingly more complex factor models. The simplest model tasted by VSS is that each item is of complexity one, and that all items are embedded in a more complex factor matrix of rank k. This is the model most appropriate for scale construction and is the one we use most frequently when we talk about factor solutions. More complicated models may also be evaluated by VSS.\nZwick and Velicer (1986) tested the scree test, Horn’s parallel test, and Velicer’s MAP test (among others) in simulation studies using a data set with a clear factor structure. Both the parallel test and MAP test seemed to work well.\nRuscio & Brendan Roche (2012) notes that PA is considered to be the method of choice among methodologists and recommend that researchers take advantage of PA as a starting point, perhaps supplemented by CD. They note that researchers can also use more than one method. Similarly, several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).\nOsborne (2014) notes that MAP has been considered superior to the “classic” criteria, and probably is superior to parallel analysis, although neither is perfect, and all must be used in the context of a search for conceptually interpretable factors. He recommends to use parallel analysis or MAP criteria, along with theory (and any of the classic criteria that suits you and is defensible).\nConclusion: Use multiple criteria and theory to make a judgment call about how many factors to extract.\n\n\nSample\nMake sure to target participants that are likely to vary in the attitude that you’re interested in measuring.\nMake sure the sample is representative of the population that you’re interested in.\n\n\nSample size\n\nRules of thumb\nThe general consensus seems to be that rules of thumb are a bad idea. The reason for that is that these rules only involve one of two factors. Common rules of thumb consist of going for a set number of participants or a particular participant-to-item ratio.\n\nTable: Examples of rules of thumb.\n\n\n\n\n\n\n\nType\nSource\nRule\n\n\n\n\nParticipant-only\n\n\n\n\n\n(comrey1992?)\n50 = very poor\n100 = poor\n200 = fair\n300 = good\n500 = very good\n1000+ = excellent\n\n\n\nBarrett & Kline, 1981\n50\n\n\n\nAleamoni, 1976\n400\n\n\nParticipant:item ratio\n\n\n\n\n\n(gorsuch2015?), p.332\n5:1 (100 participants minimum)\n\n\n\nHatcher 1994, p.73\n5:1 (minimum)\n\n\n\n(everitt1975?)\n10:1 (optimistically)\n\n\n\nNunally (1978), p.421\n10:1\n\n\n\nThe reality is that the sample size is dependent on:\n\nthe study design (e.g., cross-sectional vs. longitudinal)\nthe size of the relationships among the indicators (e.g., high communalities without cross loadings; strong loadings)\nthe reliability of the indicators\nthe scaling (e.g., categorical, continuous) and distribution of the indicators\nestimator type (e.g., ML, robust ML, WLSMV)\nthe amount and patterns of missing data\nthe size of the model (model complexity)\n\nFor example, MacCallum, Widaman, Zhang, and Hong (1999) found that results were consistent with population loadings with sample sizes as low as 60 cases if the communalities of the items were high (.60 or greater). With lower communality levels (around .50), samples of 100 to 200 cases were required.\nCostello and Osborne (2005) caution researchers to remember that EFA is a large-sample procedure and that generalizable or replicable results are unlikely if the sample is too small. They found that a ratio of 20:1 produces error rates well above the standard alpha (5%) level. They also say that researchers should realize that EFA is exploratory and should be used only for exploring data, not hypothesis or theory testing.\n\n\nNumber of indicators\nAt least three items per factor are required for a factor model to be identified; more items per factor results in overidentification of the model. A number of writers recommend that four or more items per factor be included in the factor analysis to ensure an adequate identification of the factors (Comrey & Lee, 1992; Fabrigar et al., 1999; Gorsuch, 1988).\nMacCallum et al. (1999) found that in addition to the communality of the measures, the results were more accurate for given sample sizes if there were more measures per factor included in the analysis.\nSome additional arguments for why multiple indicators per factor are useful:\n\nPeople read items incorrectly or sometimes just don’t understand an item (despite all efforts to make them as clear as possible), so having a sample of items can counteract the idiosyncratic mistakes of respondents\nWith complicated topics, having multiple items also adds clarity about the overall goal of the scale. Each item is an indicator to the participant about what the purpose of the scale is.\n\n\n\n\nSampling Adequacy\nThe Kaiser–Meyer–Olkin (KMO) test is a statistical measure to determine how suited data is for factor analysis. The test measures sampling adequacy for each variable in the model and the complete model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the higher the KMO value, the more suited the data is to factor analysis\nThe KMO value ranges from 0 to 1, with 0.60 considered suitable for factor analysis (tabachnick2013?). Kaiser himself (kaiser1974?) suggested that KMO > .9 were marvelous, in the .80s, mertitourious, in the .70s, middling, in the .60s, medicore, in the 50s, miserable, and less than .5, unacceptable.\nBartlett’s (1954) test of sphericity is a notoriously sensitive test of the hypothesis that the correlations in a correlation matrix are zero. Because of its sensitivity and its dependence on N, the test is likely to be significant with samples of substantial size even if correlations are very low. Therefore, use of the test is recommended only if there are fewer than, say, five cases per variable (tabachnick2013?).\n\n\nOutlier removal\nShould you remove outliers?\n\n\nFactor structure\nTabachnick and Fidell (2001) cite .32 as a good rule of thumb for the minimum loading of an item. Others say item loadings above .30 (Costello).\nOthers say a factor loading coefficient ≥ 0.40 is considered necessary to judge the quality (validity) of an item as an indicator of one factor or another. Some may think that this criterion is quite lax since a factor loading of 0.40 means that only 16% of the explained variance depends on the factor of which one item is the indicator (Gana & Broc)\nSome psychometricians (for example, \\[COM 92, TAB 07\\]) recommend that we choose items whose R² is greater than 0.40 (thus showing a factorial factor loading greater than 0.63).\n\nA “crossloading” item is an item that loads at .32 or higher on two or more factors. The researcher needs to decide whether a crossloading item should be dropped from the analysis, which may be a good choice if there are several adequate to strong loaders (.50 or better) on each factor.\nA factor with fewer than three items is generally weak and unstable; 5 or more strongly loading items (.50 or better) are desirable and indicate a solid factor.\n\nComrey and Lee (1992) suggest that loadings in excess of .71 (50% overlapping variance) are considered excellent, .63 (40% overlapping variance) very good, .55 (30% overlapping variance) good, .45 (20% overlapping variance) fair, and .32 (10% overlapping variance) poor.\n\nNumber of indicators to keep\n\n, it is important for the latent variable to have at least four indicators (for example: four items). In effect, with only three indicators, the measurement model is just-identified, thus making useless its testability. (Gana & Broc)\n\n\n\n\nInterpretation\nRemember that the goal of exploratory factor analysis is to explore whether your data fits a model that makes sense. Ideally, you have a conceptual or theoretical framework for the analysis- a theory or body of literature guiding the development of an instrument, for example. Even if you do not, the results should be sensible in some way. You should be able to construct a simple narrative describing how each factor, and its constituent variables, makes sense and is easily labeled.\n\n\nAdditional reading\nThe summaries and conclusions above were largely taken from the following sources:\n\nBest Practices in Exploratory Factor Analysis by Osborne (2014)\nIn Search of Underlying Dimensions: The Use (and Abuse) of Factor Analysis in Personality and Social Psychology Bulletin by Russell (2002)\nEvaluating the Use of Exploratory Factor Analysis in Psychological Research by Fabrigar et al. (1999)\n\n\n\n\n\nBorsboom, D. (2006). The attack of the psychometricians. Psychometrika, 71(3), 425. https://doi.org/10.1007/s11336-006-1447-6\n\n\nCostello, A., & Osborne, J. (2005). Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis. Practical Assessment, Research, and Evaluation, 10(1). https://doi.org/10.7275/jyj1-4868\n\n\nEdwards, J. R., & Bagozzi, R. P. (2000). On the nature and direction of relationships between constructs and measures. Psychological Methods, 5(2), 155–174. https://doi.org/10.1037/1082-989X.5.2.155\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4(3), 272–299. https://doi.org/10.1037/1082-989X.4.3.272\n\n\nOsborne, J. W. (2014). Best practices in exploratory factor analysis. Createspace publishing.\n\n\nRuscio, J., & Brendan Roche. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24(2), 282–292. https://doi.org/10.1037/a0025697\n\n\nRussell, D. W. (2002). In search of underlying dimensions: The use (and abuse) of factor analysis in Personality and Social Psychology Bulletin. Personality and Social Psychology Bulletin, 28(12), 1629–1646. https://doi.org/10.1177/014616702237645\n\n\nVelicer, W. F., & Jackson, D. N. (1990). Component analysis versus common factor analysis: Some issues in selecting an appropriate procedure. Multivariate Behavioral Research, 25(1), 1–28. https://doi.org/10.1207/s15327906mbr2501_1\n\n\nWidaman, K. F. (1993). Common factor analysis versus principal component analysis: Differential bias in representing model parameters? Multivariate Behavioral Research, 28(3), 263–311. https://doi.org/10.1207/s15327906mbr2803_1"
  },
  {
    "objectID": "chapters/statistics/factor-analysis/CFA.html",
    "href": "chapters/statistics/factor-analysis/CFA.html",
    "title": "CFA",
    "section": "",
    "text": "For confirmatory factor analyses there are also several ways to determine the sample size, including:\n\nRules of thumb\nSatorra-Saris method\nMonte Carlo approach\n\n\nNumber of indicators\nSome argue that when it comes to the number of indicators, more is better @marsh1998.\n\n\nEstimator\nIf the data is normally distributed, use the ML estimator.\nthe findings of Chou and Bentler \\[CHO 95\\] make it possible to qualify these remarks. These authors showed that in the presence of a sufficiently large sample, maximum likelihood estimation method and generalized least squares method do not make the results suffer, even when the multivariate normality is slightly violated.\nIn case of violations, the following estimators can be used:\n\nMLM\nMLR: Similar to MLM but better suited for small sample sizes (Gana & Broc).\nweighted least squares method (WLS): Requires a large sample size (FLO 04) and frequently runs into convergence issues and improper solutions\nDiagonally Weighted Least Squares (DWLS): For small samples and non-normal data ([JÖR 89)\nWLSM: Robust alternative to DWLS\nWLSMV: Robust alternative to DWLS\nbootstrapping: Not recommended for dichotomous and ordinal measures with few response categories)\n\nMardia calculation to determine violations from normal distribution. It can be noted that not only is this standardized coefficient statistically significant, but its value is greater than 5, which is the threshold value beyond which multivariate normality seems to fail \\[KLI 16\\].\n\nRecommendations concerning the main estimators available in lavaan according to the type of data (Gana & Broc).\n\n\n\n\n\n\nData type and normality assumption\nRecommended estimator\n\n\n\n\nContinuous data\n\n\n\nApproximately normal distribution\nML\n\n\nViolation of normality assumption\nML (in case of moderate violation)\n\n\n\nMLM, MLR, Bootstrap\n\n\nOrdinal/categorical data\n\n\n\nApproximately normal distribution\nML (if at least 6 response categories)\n\n\n\nMLM, MLR (if at least 4 response categories)\n\n\n\nWLSMV (binary response or 3 response categories)\n\n\nViolation of normality assumption\nML (if at least 6 response categories)\n\n\n\nMLM, MLR (if at least 4 response categories)\n\n\n\nWLSMV (in case of severe violation)\n\n\n\n\n\nFit indices\nThe only advice we can afford to give, however, is to closely follow developments concerning fit indices. In the meantime, you can carefully follow the recommendations of Hu and Bentler \\[HU 99\\] or Schreiber and his co-authors \\[SCH 06\\] who suggest the following guidelines for judging a model goodness-of-fit (based on the hypothesis where the maximum likelihood method is the estimation method): 1) RMSEA value ≤ 0.06, with confidence interval at 90% values should be between 0 and 1.00; 2) SRMR value ≤ 0.08; and 3) CFI and TLI values ≥ 0.95.\n\nFit indices guidance from @gana2019\n\n\n\n\n\n\n\nFit Type\nIndex\nBenchmark\n\n\n\n\nAbsolute\nRMR/SRMR\n≤ 0.08 = good fit\n\n\n\nWRMR\n≤ 1.00 = good fit\n\n\nParsimonious\nPRATIO\nBetween 0.00 (saturated model) and 1.00 (parsimonious model)\n\n\n\nRMSEA\n≤ 0.05 = very good fit\n≤ 0.06 and ≤ 0.08 = good fit\n\n\n\nAIC\nComparative index: the lower value of this index, the better the fit\n\n\n\nBIC\nComparative index: the lower value of this BIC index, the better the fit\n\n\nIncremental\nCFI\n≥ 0.90 and ≤ 0.94 = good fit\n≥ 0.95 = very good fit\n\n\n\nTLI\n≥ 0.90 and ≤ 0.94 = good fit\n≥ 0.95 = very good fit\n\n\n\n\n\nReliability\nReliability increases the closer the value gets to 1.00, with an acceptability threshold of 0.70."
  },
  {
    "objectID": "chapters/statistics/factor-analysis/r-packages.html",
    "href": "chapters/statistics/factor-analysis/r-packages.html",
    "title": "R-packages",
    "section": "",
    "text": "- lavaan\n- lavaan.survey\n- semTools\n- semPower"
  },
  {
    "objectID": "chapters/surveys/recommended-reading.html",
    "href": "chapters/surveys/recommended-reading.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "The following papers are useful overview or review papers that are, in my mind, particular useful to read.\n\nMeasure twice, cut down error: A process for enhancing the validity of survey scales by @gehlbach2011"
  },
  {
    "objectID": "chapters/surveys/response-options.html#section",
    "href": "chapters/surveys/response-options.html#section",
    "title": "Survey design",
    "section": "",
    "text": "Response options\nThe question of how many response options to use centers around two main concerns. The first is that more options means you can obtain a more fine-grained assessment of the characteristic that is being evaluated (e.g., attitude). In other words, your assessment is more precise. However, the question is how the number of options affects the reliability and the validity of the measurement. With more options, it becomes more difficult for people to distinguish between the different options (e.g., is “Strongly agree” reliably different from “Very strongly agree”?).\nTable 1 shows an overview of various studies in which the topic of response options was addressed. The studies vary in many ways, so the final conclusion should be a holistic interpretation of the results, rather than a simple tallying of the results. Note also that only empirical studies are included and not simulation studies. Simulation studies seem limited because they cannot address the plausible psychological limitation of people being unable to distinguish between many options.\n\nTable 1: Overview of empirical studies on the topic of response options.\n\n\n\n\n\n\n\nSource\nComparisons\nConclusion\n\n\n\n\n@donnellan2020\n2- to 7-, and 11-point Likert\n5-point Likert or higher\n\n\n@simms2019\n2- to 11-point Likert + VAS\n6-point Likert\n\n\n@sung2018\n5-point Likert and VAS-RRP\nVAS-RPP\n\n\n@cox2017\n2- and 4-point Likert\nMixed\n\n\n@lewis2017\n7-, 11-point Likert and VAS\nNo difference\n\n\n@kuhlmann2017\n5-point Likert and VAS\nNo difference\n\n\n@hilbert2016\n2- and 5-point and VAS\nIt depends\n\n\n@capik2015\n2- and 5-point Likert\nNo difference\n\n\n@eutsler2015\n5-, 7-, 9-, and 11-point Likert\n7-point Likert\n\n\n@finn2015\n2- and 4-point Likert\n4-point Likert\n\n\n@revilla2014\n5-, 7-, 11-point Likert\n5-point Likert\n\n\n@cox2012\n2- and 4-point Likert\n4-point Likert\n\n\n@janhunen2012\n7-point Likert and 30-point VAR\nVAR\n\n\n@dawes2008\n5-, 7-, and 10-point Likert\nNo difference\n\n\n@weng2004\n3- to 9-point Likert\n5-point or higher\n\n\n@preston2000\n2- to 11-point Likert and VAS\n7-, 9-, or 10-point Likert\n\n\n@alwin1997\n7- and 11-point Likert\n11-point Likert\n\n\n@jaeschke1990\n7-point Likert and VAS\nNo difference\n(slightly favor 7-point Likert)\n\n\n@flamer1983\n2- and 9-point Likert\n9-point Likert\n\n\n@matell1971\n2-point to 18-point Likert\nNo difference\n\n\n@bendig1954\n2-, 3-, 5-, 7-, and 9-point Likert\nNo difference\n(maybe 3-point or higher)\n\n\n\nThere are also several review papers on the topic. @krosnick2010 suggest that 7-point Likert scales are probably optimal. @lietz2010 concludes a desirable Likert-scale consists of 5 to 8 response options. Similarly, @coxiii1980 recommends to use between 5 and 9 response options. @symonds1924, in 1924, claims the optimum number is 7. @gehlbach2011 recommends using 5-points for unipolar items and 7-point for biopolar items.\nBesides psychometric properties it may also be worth taking into account respondent preference. This involves ease of use of the scale and whether the response options allow for sufficient variation for respondents to express their view. @preston2000 found that respondents found scales with 5, 7, and 10 points easy to use (compared to fewer options and a VAS) and that they preferred scales with more response options to allow them to express themselves (7 or more). Other studies also show that respondents favor more options [@cox2017].\nNote that if time is of the essence, fewer response options are preferred.\nAnother relevant factor is whether the scale is bipolar or unipolar. Bipolar scales are symmetrical which means the number of options naturally increase as they need to match both sides of the spectrum. Unipolar items are only about one side, usually ranging from the absence of something to the presence of something (to a certain degree). Since it is harder to label a larger number of options for a unipolar scale, the number of options are likely to be smaller.\nConclusion: It appears that few response options (2 or 3) should definitely be avoided. More response options therefore seems better, but benefits seem to quickly level off. Given other concerns, such as ease of use and interpretability, a 7-point Likert scale seems to be preferred for bipolar scales and a 5-point Likert scale for unipolar scales.\n\n\nOdd vs. even response options\nThe middle option of a scale can have an ambiguous meaning. Participants may use it to indicate a moderate standing on the issue (Rugg and Cantril, 1944), a lack of an opinion (Nadler, Weston, and Voyles, 2014), ambivalence (Klopfer and Madden, 1980; Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), indifference (Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), uncertainty (Baka, Figgou, and Triga, 2012; Nadler, Weston, and Voyles, 2014), confusion, or to signal context dependence (e.g., “it depends” or disputing the question, see Baka, Figgou, and Triga, 2012).\nThe middle option may also be used for certain response styles, such as socially desirable responding (Sturgis, Roberts, and Smith, 2012) or satisficing (Krosnick, 1991), although there is not much research showing it actually leads to satisficing @wang2020.\nAf a middle alternative is explicitly oﬀered, the proportion endorsing it increases dramatically (e.g. Ayidiya & McClendon, 1990; Bishop, 1987; Bishop, Hippler, Schwarz, & Strack, 1988; Kalton, Collins, & Brook, 1978; Kalton, Roberts, & Holt, 1980; Rugg & Cantril, 1944).\nSome studies show that not including a middle option decreases validity and increases measurement error (O’Muircheartaigh, Krosnick, and Helic, 1999; Kahn, and Dhar, 2002)\nRecent study on this: @wang2020\nAn alternative approach to this issue is to use branching. Respondents could first be asked whether they fall at the midpoint or on one side, followed by a question about their extremity on a side. This approach was found to be more reliable and valid than using a 7-point scale (Krosnick and Berent, 1993; Malhotra, Krosnick, and Thomas, 2009).\nConclusion: If it is possible that respondents may have a moderate view, it seems crucial for it to be possible to capture this view. Limitations of a middle option could then be addressed in other ways (e.g., clear questions).\n\n\nResponse option labeling\nThere are several studies that show all response options should be labelled, rather than only labeling the end points [@weng2004; @krosnick1993].\nFor an example of biopolar labels for a 2- to 11-point Likert scale, see Table 1.\n\n\nTable 1: Likert response labels from @simms2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabel\n2-point\n3-point\n4-point\n5-point\n6-point\n7-point\n8-point\n9-point\n10-point\n11-point\n\n\n\n\nVery strongly disagree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\nStrongly disagree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nDisagree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nMostly disagree\n\n\n\n\n\n\n\n\nx\nx\n\n\nSlightly disagree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nNeither agree nor disagree\n\nx\n\nx\n\nx\n\nx\n\nx\n\n\nSlightly agree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nMostly agree\n\n\n\n\n\n\n\n\nx\nx\n\n\nAgree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nStrongly agree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nVery strongly agree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\n\n\nIt is also recommended to avoid agree-disagree response labels because asking respondents to rate their level of agreement is a cognitively demanding task that increases respondent error and reduces responding effort [@gehlbach2011].\n\n\nAcquiescence and response options"
  },
  {
    "objectID": "chapters/surveys/index.html#steps",
    "href": "chapters/surveys/index.html#steps",
    "title": "Survey design",
    "section": "Steps",
    "text": "Steps\nAs suggested in some review papers (e.g., @gehlbach2011), the construction of a new survey/scale is best done in several steps:\n\nReview the literature to define the construct of interest and to find existing or related measures of the construct\nInterview respondents to see whether respondents think the same way about the construct as you do\nReconcile potential differences between the literature review and the interview data\nDevelop items (see next sections for more details on how to best design items)\nValidate with experts\nDo cognitive pretesting to see how respondents understand and respond to each item\nRun large-scaled reliability and validity tests"
  },
  {
    "objectID": "chapters/surveys/item-development.html#number-of-items",
    "href": "chapters/surveys/item-development.html#number-of-items",
    "title": "Item Development",
    "section": "Number of items",
    "text": "Number of items\nThere are no hard-and-fast rules guiding this decision, but keeping a measure short is an effective means of minimizing response biases caused by boredom or fatigue (Schmitt & Stults, 1985; Schriesheim & Eisenbach, 1990). Additional items also demand more time in both the development and administration of a measure (Carmines & Zeller, 1979). Harvey, Billings, and Nilan (1985) suggest that at least four items per scale are needed to test the homogeneity of items within each latent construct. Adequate internal consistency reliabilities can be obtained with as few as three items (Cook et al., 1981), and adding items indefinitely makes progressively less impact on scale reliability (Carmines & Zeller, 1979). It is difficult to improve on the internal consistency reliabilities of five appropriate items by adding items to a scale (Hinkin, 1985; Hinkin & Schriesheim, 1989; Schriesheim & Hinkin, 1990). Cortina (1993) found that scales with many\nitems may have high internal consistency reliabilities even if item intercorrelations are low, an argument in favor of shorter scales with high internal consistency. It is also important to assure that the domain has been adequately sampled, as inadequate sampling is a primary source of measurement error (Churchill, 1979). As Thurstone (1947) points out, scales should possess simple structure, or parsimony. Not only should any one measure have the simplest possible factor constitution, but any scale should require the contribution of a minimum number of items that adequately tap the domain of interest. These findings would suggest that the eventual goal will be the retention of four to six items for most constructs, but the final determination must be made only with accumulated evidence in support of the construct validity of the measure. It should be anticipated that approximately one half of the created items will be retained for use in the final scales, so at least twice as many items as will be needed in the final scales should be generated to be administered in a survey questionnaire.\nhttps://twitter.com/dingding_peng/status/1481683536499331079\nhttps://psyarxiv.com/4kra2/"
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "About",
    "section": "Contribute",
    "text": "Contribute\nThis book will forever be a work in progress because the content will be about what are currently regarded as best practices. These are likely to change over time, so the book will have to change with it.\nIt’s also very likely that there are mistakes in this book. It’s possible to comment on these by clicking on the Github link on each page and creating an Issue."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe following people have contributed to this book:\n\nWillem Sleegers\n\nIf I’ve missed your contributions and you deserve to be on this list, please don’t hesitate to contact me or add yourself via a Pull Request on GitHub!"
  }
]