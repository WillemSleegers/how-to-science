[
  {
    "objectID": "sections/statistics/index.html",
    "href": "sections/statistics/index.html",
    "title": "Overview",
    "section": "",
    "text": "This section is about statistics-related topics. At the moment there are two chapters. One of exploratory factor analyses and one of confirmatory factor analyses."
  },
  {
    "objectID": "sections/statistics/factor-analysis/examples.html",
    "href": "sections/statistics/factor-analysis/examples.html",
    "title": "Examples",
    "section": "",
    "text": "References\n\nKahane, G., Everett, J. A. C., Earp, B. D., Caviola, L., Faber, N. S., Crockett, M. J., & Savulescu, J. (2018). Beyond sacrificial harm: A two-dimensional model of utilitarian psychology. Psychological Review, 125(2), 131–164. https://doi.org/10.1037/rev0000093"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html",
    "href": "sections/statistics/factor-analysis/EFA.html",
    "title": "Exploratory Factor Analysis",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\nThe goal of an exploratory factor analysis (EFA) is to study latent factors that underlie responses to a larger number of items. In other words, the goal is to explore the data and reduce the number of variables. It is a popular technique in the development and validation of assessment instruments.\nUnlike confirmatory factor analysis (CFA), EFA is used when there is little or no a priori justification for specifying a particular structural model. This means that there is reasonable uncertainty about the number of underlying factors and which items load on which factor. The hope is to resolve some of that uncertainty empirically."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#preparation",
    "href": "sections/statistics/factor-analysis/EFA.html#preparation",
    "title": "Exploratory Factor Analysis",
    "section": "Preparation",
    "text": "Preparation\nBefore you perform a factor analysis, make sure that the design of your study is suitable for this type of analysis. This means you should have an appropriate sample, sample size, and indicators (items).\n\nSample\nIt may be obvious, but make sure to target participants that are likely to vary in the attitude that you’re interested in measuring and that the sample is representative of the population that you’re interested in.\n\n\nSample size\nRules of thumb are one way to determine an appropriate sample size, but the general consensus is that rules of thumb are a bad idea [MacCallum et al. (1999), Costello & Osborne (2005)]. The reason for that is that rules of thumb only involve one of two factors that are relevant for determining the sample size: a set number of participants or a particular participant-to-item ratio. The reality is that the appropriate sample size is dependent on many more factors, such as communalities, factor loadings, the number of indicators per factor, and the number of factors [Gagne & Hancock (2006), MacCallum et al. (1999), MacCallum et al. (2001), Velicer & Fava (1998)].\nGiven the replication failures in the social sciences, it should be no surprise that also in the EFA literature there appears to be a sample size problem. This is perhaps best demonstrated by Costello & Osborne (2005). They reviewed two years’ worth of PsychINFO articles and found that the majority of the studies (62.9%) had participants to item ratios of 10:1 or less. Of course, the participant to item ratio is not a good benchmark for the appropriate sample size, so this is not enough to demonstrate that the sample size is insufficient. They did find support that this is not enough by sampling data of various sample sizes from a large data set of responses to the Marsh’s Self-Description Questionnaire. They found that only 70% of their large (20:1) samples produced correct solutions, leading them to conclude that a 20:1 participant to item ratio produces error rates well above the field standard alpha = .05 level.\nThis is confirmed by, in my opinion, an incredibly useful study by Mundfrom et al. (2005). They performed a simulation study to determine the minimum sample size for a study, taking into account the number of factors, the ratio of items to factors, and several different communalities. They generated a variety of population correlation matrices under different conditions, repeatedly sampled from these population structures, and determined coefficients of congruence between the sample solutions and population structures (similar to Velicer & Fava (1998)). This allowed them to produce a table with minimum recommend sample sizes for different situations.\n\nRecommended minimum sample sizes by Mundfrom et al. (2005)\n\n\n\n\n\n\n\n\nCriterion\nCommunality\nItems per factor\nMinimum sample size\n\n\n\n\nExcellent (0.98)\nHigh (.6 to .8)\n4\n500\n\n\n\n\n6\n250\n\n\n\n\n8\n100\n\n\n\nWide (.2 to .8)\n4\n900\n\n\n\n\n6\n200\n\n\n\n\n8\n130\n\n\n\nLow (.2 to .4)\n4\n1400\n\n\n\n\n6\n260\n\n\n\n\n8\n130\n\n\nGood (0.92)\nHigh (.6 to .8)\n5\n130\n\n\n\n\n7\n55\n\n\n\nWide (.2 to .8)\n5\n140\n\n\n\n\n8\n60\n\n\n\nLow (.2 to .4)\n5\n200\n\n\n\n\n8\n80\n\n\n\nAs can be seen in the table, the appropriate sample size is dependent on several factors and is not simply a function of sample size alone or a particular participant to item ratio.\n\n\nNumber of items\nThe previous section on sample size shows that a relevant factor in determining the minimum sample size is the number of items per factor, so how many items should you create and include in the factor analysis?\nSince this is an exploratory factor analysis, it is not clear in advance how many items will actually survive the exploration. It could be that certain items will not be well understood by participants (although this should be caught in pilot testing prior to running an EFA) or that multiple items simple do not fit in the expected factor structure. Since not all items will likely behave well, you should always include more items than you intend to keep.\nIt’s also useful to keep in mind that for a single factor model to be identifiable, it must consist of at least three indicators and preferably four to also allow for the model to be statistically testable [Gana & Broc (2019), also see Velicer & Fava (1998) and Russell (2002)]. This is useful for when you want to do a CFA later or if you want others to be able to perform CFAs on your factor analysis results.\nThe number of indicators also depends on the quality of indicators. Costello & Osborne (2005) notes that 5 or more strongly loading items (.50 or better) indicate a solid factor. MacCallum et al. (1999) writes that more indicators is generally better [also see Marsh et al. (1998), but see Hayduk & Littvay (2012)]. Within the range of indicators they studied (three to seven per factor), it is better to have more indicators than fewer. The critical point, however, is that these indicators must be reasonably valid and reliable.\nVelicer & Fava (1998) suggest that 6-10 initial items per factor is recommended as 25% to 50% will not perform as expected and the end goal should be to have four- or five to-one as a minimum. The bare minimum of three variables was found to be insufficient (based on two simulation studies) and a more prudent target would be to have four- or five to-one as a minimum. A ratio of 20-30 initial items per factor is also possible as an appropriate target for extensive oversampling, which may be needed in cases where it is not possible to obtain a large sample size.\nFinally, you should also take into account the desirable scale length. Shorter scales have the desirable property that they are quick to administer, meaning they can more easily be added to a study or included in short studies to obtain a larger sample size. The goal may therefore be to find only a handful of solid indicators for a factor."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#data-analysis",
    "href": "sections/statistics/factor-analysis/EFA.html#data-analysis",
    "title": "Exploratory Factor Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nRelevant items\nOne design issue that is especially important is about which measured variables to include in the study. If measured variables irrelevant to the domain of interest are included, then spurious common factors might emerge or true common factors might be obscured (Fabrigar et al., 1999).\n\n\nSampling Adequacy\nThe Kaiser–Meyer–Olkin (KMO) test is a statistical measure to determine how suitable the data is for factor analysis. The test measures sampling adequacy for the complete model and each variable in the model. The statistic indicates the degree to which each variable in a set is predicted without error by the other variables.\nThe KMO value ranges from 0 to 1, with 0.60 considered suitable for factor analysis Tabachnick & Fidell (2013). Kaiser himself (1974) suggested that KMO > .9 were marvelous, in the .80s, meritorious, in the .70s, middling, in the .60s, mediocre, in the 50s, miserable, and less than .5, unacceptable.\nBartlett’s (1954) test of sphericity is a notoriously sensitive test of the hypothesis that the correlations in a correlation matrix are zero. Because of its sensitivity and its dependence on N, the test is likely to be significant with samples of substantial size even if correlations are very low. Therefore, use of the test is recommended only if there are fewer than around five participants per variable (Tabachnick & Fidell, 2013).\n\n\nPrincipal Components vs. Factor Analysis\nAn important consideration is whether to use a principal components analysis (PCA) or a factor analysis. PCA is a data reduction technique and while a factor analysis is also a type of data reduction technique, the focus with PCA is more on simply reducing the data, without regard for a theoretical interpretation. With PCA, the variables themselves are of interest, rather than a hypothetical latent construct. Constructs are conceptualized as being causally determined by the observations; that is, CFA reflects a formative model rather than a reflective one (Edwards & Bagozzi, 2000). If you are a psychologist trying to create a measure of a psychological construct, this is probaly not what you want as principal component scores are “caused” by their indicators in much the same way that sum scores are “caused” by item scores.” [Borsboom (2006), p. 426]. Instead, you likely want the opposite causal relationship in which a latent factor causes the indicator scores.\n\n\nCode\nDiagrammeR::grViz(\"\n  digraph dot {\n    graph [\n      layout = dot, \n      rankdir = LR, \n      fontname = 'Source Sans Pro', \n      label = 'Illustration of a PCA model (left) and a factor analysis model (right).'\n    ]\n    \n    node [fontname = 'Source Sans Pro']\n    node [shape = square]\n    var1 [label = 'item 1']\n    var2 [label = 'item 2']\n    var3 [label = 'item 3']\n    var4 [label = 'item 4']\n    var5 [label = 'item 1']\n    var6 [label = 'item 2']  \n    var7 [label = 'item 3']  \n    var8 [label = 'item 4']\n  \n    node [shape = circle]\n    construct_a [label = 'construct A']\n    construct_b [label = 'construct B']\n  \n    edge [color = black, minlen = 3]\n    {var1 var2 var3 var4} -> construct_a\n    construct_b -> {var5 var6 var7 var8}\n  \n    edge[style = invis, minlen = 1];\n    construct_a -> construct_b\n  }\"\n)\n\n\n\n\n\n\nA related issue is that factor analysis assumes that the total variance can be partitioned into common and unique variance and PCA assumes that the common variances takes up all of total variance. This means that PCA assumes that all variables are measured without error. It is usually more reasonable to assume that you have not measured your set of items perfectly.\nAlthough some argue that the two methods have the same results (Velicer & Jackson, 1990), there is evidence that those similarities are mistaken and that factor analysis has better results than PCA (Widaman, 1993).\nConclusion: Do not use PCA.\n\n\nFactor Extraction Method\nExtraction is the general term for the process of reducing the number of dimensions being analyzed from the number of variables in the data set (and matrix of associations) into a smaller number of factors.\nThere are multiple factor extract methods, such as:\n\nminres\nunweighted least squares (ULS)\ngeneralized least squares (GLS)\nmaximum likelihood\nprincipal axis factor(ing)\nalpha factor(ing)\nimage factor(ing)\n\nIt’s not entirely clear which factor extraction method is the best and some authors use different terms for some of the methods, making it more difficult to compare them.\nFabrigar, Wegener, MacCallum and Strahan -Fabrigar et al. (1999) argued that if data are relatively normally distributed, maximum likelihood is the best choice because “it allows for the computation of a wide range of indexes of the goodness of fit of the model and permits statistical significance testing of factor loadings and correlations among factors and the computation of confidence intervals.” (p. 277). In case the data is not generally normally distributed, they recommend principal axis factoring. This is recommended in several sources (Costello & Osborne (2005); Osborne (2014)).\nConclusion: Use the maximum likelihood method if the data is generally normally distributed and to use principal axis factoring if the data is non-normal.\n\n\nRotation methods\nThe goal of rotation is to clarify the factor structure and make the results of the EFA more interpretable.\nRotation methods can be categorized into one of two categories: orthogonal or oblique. Orthogonal rotations keep axes at a 90 degree angle, forcing the factors to be uncorrelated. Oblique rotations allow angles that are not 90 degree , thus allowing factors to be correlated if that is optimal for the solution.\nOrthogonal rotation methods include:\n\nvarimax\nquartimax\nequamax\n\nOblique rotation methods include:\n\ndirect oblimin\nquartimin\npromax\n\nIn the social sciences, we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution. If the factors are truly uncorrelated, orthogonal and oblique rotation produce nearly identical results. Since oblique rotation will reproduce an orthogonal solution but not vice versa, it makes sense to go for oblique rotation.\nThere is no widely preferred method of oblique rotation; all tend to produce similar results (Fabrigar et al., 1999).\nConclusion: Use any oblique rotation.\n\n\nNumber of factors\nThere are several methods to determine how many factors to retain:\n\nTheory\nKaiser criterion (eigenvalues greater than 1; Kaiser, 1960)\nScree test (Cattell, 1966)\nParallel analysis (Horn, 1965)\nVelicer’s multiple average partial (MAP) procedure (Velicer, 1976)\nAkaike information criterion (AIC; Akaike, 1974)\nBayesian information criterion (BIC; Schwarz, 1978)\nComparison data (CD; Ruscio & Brendan Roche (2012))\nVery Simple Structure (VSS)\nThe root-mean-square error of approximation (RMSEA; Browne & Cudeck, 1992)\nLikelihood ratio statistic\n\nDetermining the number of factors is probably the most difficult part of the exploratory factor analysis. As Henry Kaiser said:\n\nSolving the number of factors problem is easy, I do it everyday before breakfast. But knowing the right solution is harder. (Horn and Engstrom, 1979)\n\nThe difficulty of this step means that a certain flexibility is warranted.\nIt makes sense to begin with the expectation that you will see the theorized factor structure. Instruments are rarely perfect (especially the first time it is examined), and theoretical expectations are not always supported, but unless one is on a totally blind fishing expedition, this is a good place to start.\nThe default in most statistical software packages is to use the Kaiser criterion. It makes some sense, as an eigenvalue represents the sum of the squared factor loadings in a column, and to get a sum of 1.0 or more, one must have rather large factor loadings to square and sum. However, this is easily achieved with more items and there are now alternative methods. Hence, there is broad consensus in the literature that this is among the least accurate methods for selecting the number of factors to retain (Velicer & Jackson, 1990).\nThe scree test involves examining the graph of the eigenvalues and looking for the natural bend or “elbow” in the data where the slope of the curve changes markedly. Although the scree plot itself is not considered sufficient to determine how many factors should be extracted (Velicer et al., 2000), it does appear to be a relatively reliable method. The main down side seems to be its ambiguity as the bend is not always clear and sometimes there are even multiple bends.\nParallel analysis involves generating random uncorrelated data, and comparing eigenvalues from the EFA to the eigenvalues from the random data. Using this process, only factors with eigenvalues that are above random eigenvalues should be retained, although it is not clear how much above the random eigenvalues they should be. Several authors have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).\nThe Minimum Average Partial (MAP) criterion involves partialing out common variance as each successive component is created. As each successive component is partialed out, common variance will decrease to a minimum. Velicer argued that minimum point should be considered the criterion for the number of factors to extract.\nVSS involves degrading the initial rotated factor solution by assuming that the nonsalient loadings are zero, even though in actuality they rarely are. What VSS does is test how well the factor matrix we think about and talk about actually fits the correlation matrix. It is not a confirmatory procedure for testing the significance of a particular loading, but rather it is an exploratory procedure for testing the relative utility of interpreting the correlation matrix in terms of a family of increasingly more complex factor models. The simplest model tested by VSS is that each item is of complexity one, and that all items are embedded in a more complex factor matrix of rank k. This is the model most appropriate for scale construction and is the one we use most frequently when we talk about factor solutions. More complicated models may also be evaluated by VSS.\nZwick and Velicer (1986) tested the scree test, Horn’s parallel test, and Velicer’s MAP test (among others) in simulation studies using a data set with a clear factor structure. Both the parallel test and MAP test seemed to work well.\nRuscio & Brendan Roche (2012) notes that PA is considered to be the method of choice among methodologists and recommend that researchers take advantage of PA as a starting point, perhaps supplemented by CD. They note that researchers can also use more than one method.\nOsborne (2014) notes that MAP has been considered superior to the “classic” criteria, and probably is superior to parallel analysis, although neither is perfect, and all must be used in the context of a search for conceptually interpretable factors. He recommends to use parallel analysis or MAP criteria, along with theory (and any of the classic criteria that suits you and is defensible).\nFinally, empirical research suggests that overfactoring introduces much less error to factor loading estimatesihan underfactoring (Fava & Velicer. 1992; Wood et..;!., 1996). Although be wary of solutions with too many factors mightprompt a researcher to postulate the existence of cons tracts with little theoretical value and thereby develop unnecessarily complex theories.\nConclusion: Use multiple criteria, including theory, to make a judgment call about how many factors to extract. When in doubt, favor more factors rather than fewer factors."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#interpretation",
    "href": "sections/statistics/factor-analysis/EFA.html#interpretation",
    "title": "Exploratory Factor Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nRemember that the goal of exploratory factor analysis is to explore whether your data fits a model that makes sense. Ideally, you have a conceptual or theoretical framework for the analysis. Even if you do not, the results should be sensible in some way. You should be able to construct a simple narrative describing how each factor, and its items, makes sense and is easily labeled.\n\nFactor Loadings\nAfter determining the number of factors and rotation, you will be able to produce a table of factor loadings. The question is now to see whether there are items that load sufficiently strongly on each factor. If the goal is to have unidimensional factors, then cross loadings should also be examined. Items that don’t perform well may be removed.\nWhat counts as a strong or sufficient factor loading?\nPeterson (2000) found that the average factor loading cutoff threshold is .40.\nTabachnick and Fidell (2001) cite .32 as a good rule of thumb for the minimum loading of an item, assuming the sample size is larger than 300. Others say item loadings above .30 (Costello).\nClark and Watson, 1995 say > .35 Clark & Watson (1995)\nOthers say a factor loading coefficient ≥ 0.40 is considered necessary to judge the quality (validity) of an item as an indicator of one factor or another. Some may think that this criterion is quite lax since a factor loading of 0.40 means that only 16% of the explained variance depends on the factor of which one item is the indicator (Gana & Broc)\nSome psychometricians recommend that we choose items whose R² is greater than 0.40 (thus showing a factorial factor loading greater than 0.63).\nComrey and Lee (1992) suggest that loadings in excess of .71 (50% overlapping variance) are considered excellent, .63 (40% overlapping variance) very good, .55 (30% overlapping variance) good, .45 (20% overlapping variance) fair, and .32 (10% overlapping variance) poor.\none must be careful not to prematurely drop poorly performing items, especially when such items were predicted a priori to be strong markers of a given factor.\n\n\n\n\n\nReliability\nNote that Cronbach’s alpha relies on the assumption of unidimensionality.\nInternal consistency:\nRecommended cut off points are:\n\n.80 in a basic science setting (Clark & Watson, 1995) Nunally 1978\n.9 or .5 in applied settings Nunally 1978\n\nTest-retest reliability:"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#recommended-reading",
    "href": "sections/statistics/factor-analysis/EFA.html#recommended-reading",
    "title": "Exploratory Factor Analysis",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThe summaries and conclusions above were largely taken from the following sources:\n\nBest Practices in Exploratory Factor Analysis by Osborne (2014)\nIn Search of Underlying Dimensions: The Use (and Abuse) of Factor Analysis in Personality and Social Psychology Bulletin by Russell (2002)\nEvaluating the Use of Exploratory Factor Analysis in Psychological Research by Fabrigar et al. (1999)\nCostello & Osborne (2005)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#glossary",
    "href": "sections/statistics/factor-analysis/EFA.html#glossary",
    "title": "Exploratory Factor Analysis",
    "section": "Glossary",
    "text": "Glossary\n\ncommunality: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\neigenvalue: the amount of variance explained by a factor; the sum of squared factor loadings\nh2: See communality.\nu2: 1 - h2. residual variance, a.k.a. uniqueness"
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA.html",
    "href": "sections/statistics/factor-analysis/CFA.html",
    "title": "Confirmatory Factor Analysis",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\nNote that if you have a clear theoretical basis about the number of factors and which items should load on which factor, CFA is more appropriate than EFA because it allows you to directly test your model. You could also do both, but a CFA should be conducted on new data to test its validity.\nTo incorporate:\nFor confirmatory factor analyses there are also several ways to determine the sample size, including:"
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA.html#recommended-reading",
    "href": "sections/statistics/factor-analysis/CFA.html#recommended-reading",
    "title": "Confirmatory Factor Analysis",
    "section": "Recommended Reading",
    "text": "Recommended Reading\n\nDynamic Fit Index Cutoffs for Confirmatory Factor Analysis Models by McNeish & Wolf (2021)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/r-packages.html",
    "href": "sections/statistics/factor-analysis/r-packages.html",
    "title": "R-packages",
    "section": "",
    "text": "psych\nlavaan\nlavaan.survey\nsemTools\nsemPower"
  },
  {
    "objectID": "sections/surveys/recommended-reading.html",
    "href": "sections/surveys/recommended-reading.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "References\n\nGehlbach, H., & Brinkworth, M. E. (2011). Measure twice, cut down error: A process for enhancing the validity of survey scales. Review of General Psychology, 15(4), 380–387. https://doi.org/10.1037/a0025704\n\n\nWolf, M. G., Ihm, E., Maul, A., & Taves, A. (2019). Survey item validation. https://doi.org/10.31234/osf.io/k27w3"
  },
  {
    "objectID": "sections/surveys/response-options.html",
    "href": "sections/surveys/response-options.html",
    "title": "Response Options",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\nThere are several decisions to make that involve response options. How many response options should you use? Should you use an even or odd number of response options? Should you label them? This section contains a summary of best practices that one can use to address these questions."
  },
  {
    "objectID": "sections/surveys/response-options.html#number-of-response-options",
    "href": "sections/surveys/response-options.html#number-of-response-options",
    "title": "Response Options",
    "section": "Number of Response Options",
    "text": "Number of Response Options\nThe question of how many response options to use centers around two main concerns. The first is that more options means you can obtain a more fine-grained assessment of the characteristic that is being evaluated (e.g., attitude). In other words, your assessment is more precise. However, the question is how the number of options affects the reliability and the validity of the measurement. With more options, it becomes more difficult for people to distinguish between the different options (e.g., is “Strongly agree” reliably different from “Very strongly agree”?).\nTable 1 shows an overview of various studies in which the topic of response options was addressed. The studies vary in many ways, so the final conclusion should be a holistic interpretation of the results, rather than a simple tallying of the results. Note also that only empirical studies are included and not simulation studies. Simulation studies seem limited because they cannot address the plausible psychological limitation of people being unable to distinguish between many options.\n\nTable 1: Overview of empirical studies on the topic of response options.\n\n\n\n\n\n\n\nSource\nComparisons\nConclusion\n\n\n\n\nDonnellan & Rakhshani (2020)\n2- to 7-, and 11-point Likert\n5-point Likert or higher\n\n\nSimms et al. (2019)\n2- to 11-point Likert + VAS\n6-point Likert\n\n\nSung & Wu (2018)\n5-point Likert and VAS-RRP\nVAS-RPP\n\n\nCox et al. (2017)\n2- and 4-point Likert\nMixed\n\n\nLewis (2017)\n7-, 11-point Likert and VAS\nNo difference\n\n\nKuhlmann et al. (2017)\n5-point Likert and VAS\nNo difference\n\n\nHilbert (2016)\n2- and 5-point and VAS\nIt depends\n\n\nCapik & Gozum (2015)\n2- and 5-point Likert\nNo difference\n\n\nEutsler & Lang (2015)\n5-, 7-, 9-, and 11-point Likert\n7-point Likert\n\n\nFinn et al. (2015)\n2- and 4-point Likert\n4-point Likert\n\n\nRevilla et al. (2014)\n5-, 7-, 11-point Likert\n5-point Likert\n\n\nCox et al. (2012)\n2- and 4-point Likert\n4-point Likert\n\n\nJanhunen (2012)\n7-point Likert and 30-point VAR\nVAR\n\n\nDawes (2008)\n5-, 7-, and 10-point Likert\nNo difference\n\n\nWeng (2004)\n3- to 9-point Likert\n5-point or higher\n\n\nPreston & Colman (2000)\n2- to 11-point Likert and VAS\n7-, 9-, or 10-point Likert\n\n\nAlwin (1997)\n7- and 11-point Likert\n11-point Likert\n\n\nJaeschke et al. (1990)\n7-point Likert and VAS\nNo difference\n(slightly favor 7-point Likert)\n\n\nFlamer (1983)\n2- and 9-point Likert\n9-point Likert\n\n\nMatell & Jacoby (1971)\n2-point to 18-point Likert\nNo difference\n\n\nBendig (1954)\n2-, 3-, 5-, 7-, and 9-point Likert\nNo difference\n(maybe 3-point or higher)\n\n\n\nThere are also several review papers on the topic. Krosnick & Presser (2010) suggest that 7-point Likert scales are probably optimal. Lietz (2010) concludes a desirable Likert-scale consists of 5 to 8 response options. Similarly, Cox III (1980) recommends to use between 5 and 9 response options. Symonds (1924), in 1924, claims the optimum number is 7. Gehlbach & Brinkworth (2011) recommends using 5-points for unipolar items and 7-point for biopolar items.\nBesides psychometric properties it may also be worth taking into account respondent preference. This involves ease of use of the scale and whether the response options allow for sufficient variation for respondents to express their view. Preston & Colman (2000) found that respondents found scales with 5, 7, and 10 points easy to use (compared to fewer options and a VAS) and that they preferred scales with more response options to allow them to express themselves (7 or more). Other studies also show that respondents favor more options (Cox et al., 2017).\nNote that if time is of the essence, fewer response options are preferred.\nAnother relevant factor is whether the scale is bipolar or unipolar. Bipolar scales are symmetrical which means the number of options naturally increase as they need to match both sides of the spectrum. Unipolar items are only about one side, usually ranging from the absence of something to the presence of something (to a certain degree). Since it is harder to label a larger number of options for a unipolar scale, the number of options are likely to be smaller.\nConclusion: It appears that few response options (2 or 3) should definitely be avoided. More response options therefore seems better, but benefits seem to quickly level off. Given other concerns, such as ease of use and interpretability, a 7-point Likert scale seems to be preferred for bipolar scales and a 5-point Likert scale for unipolar scales."
  },
  {
    "objectID": "sections/surveys/response-options.html#odd-vs.-even-response-options",
    "href": "sections/surveys/response-options.html#odd-vs.-even-response-options",
    "title": "Response Options",
    "section": "Odd vs. Even Response Options",
    "text": "Odd vs. Even Response Options\nThe middle option of a scale can have an ambiguous meaning. Participants may use it to indicate a moderate standing on the issue (Rugg and Cantril, 1944), a lack of an opinion (Nadler, Weston, and Voyles, 2014), ambivalence (Klopfer and Madden, 1980; Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), indifference (Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), uncertainty (Baka, Figgou, and Triga, 2012; Nadler, Weston, and Voyles, 2014), confusion, or to signal context dependence (e.g., “it depends” or disputing the question, see Baka, Figgou, and Triga, 2012).\nThe middle option may also be used for certain response styles, such as socially desirable responding (Sturgis, Roberts, and Smith, 2012) or satisficing (Krosnick, 1991), although there is not much research showing it actually leads to satisficing Wang & Krosnick (2020).\nAf a middle alternative is explicitly oﬀered, the proportion endorsing it increases dramatically (e.g. Ayidiya & McClendon, 1990; Bishop, 1987; Bishop, Hippler, Schwarz, & Strack, 1988; Kalton, Collins, & Brook, 1978; Kalton, Roberts, & Holt, 1980; Rugg & Cantril, 1944).\nSome studies show that not including a middle option decreases validity and increases measurement error (O’Muircheartaigh, Krosnick, and Helic, 1999; Kahn, and Dhar, 2002)\nRecent study on this: Wang & Krosnick (2020)\nAn alternative approach to this issue is to use branching. Respondents could first be asked whether they fall at the midpoint or on one side, followed by a question about their extremity on a side. This approach was found to be more reliable and valid than using a 7-point scale (Krosnick and Berent, 1993; Malhotra, Krosnick, and Thomas, 2009).\nConclusion: If it is possible that respondents may have a moderate view, it seems crucial for it to be possible to capture this view. Limitations of a middle option could then be addressed in other ways (e.g., clear questions)."
  },
  {
    "objectID": "sections/surveys/response-options.html#response-option-labeling",
    "href": "sections/surveys/response-options.html#response-option-labeling",
    "title": "Response Options",
    "section": "Response Option Labeling",
    "text": "Response Option Labeling\nThere are several studies that show all response options should be labelled, rather than only labeling the end points (Krosnick & Berent, 1993; Weng, 2004).\nFor an example of biopolar labels for a 2- to 11-point Likert scale, see Table 1.\n\n\nTable 1: Likert response labels from Simms et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabel\n2-point\n3-point\n4-point\n5-point\n6-point\n7-point\n8-point\n9-point\n10-point\n11-point\n\n\n\n\nVery strongly disagree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\nStrongly disagree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nDisagree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nMostly disagree\n\n\n\n\n\n\n\n\nx\nx\n\n\nSlightly disagree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nNeither agree nor disagree\n\nx\n\nx\n\nx\n\nx\n\nx\n\n\nSlightly agree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nMostly agree\n\n\n\n\n\n\n\n\nx\nx\n\n\nAgree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nStrongly agree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nVery strongly agree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\n\n\nIt is also recommended to avoid agree-disagree response labels because asking respondents to rate their level of agreement is a cognitively demanding task that increases respondent error and reduces responding effort (Gehlbach & Brinkworth, 2011).\nAgreement: Strongly agree, Moderately agree, Neither agree nor disagree, Moderately disagree, Strongly disagree (another version removes the “moderately” qualifier and/or uses “neutral”)\nComparison: Much X, Slightly X, About the same, Slightly (opposite of X), Much (opposite of X)\nEase: Very easy, Moderately easy, Neither easy nor difficult, Moderately difficult, Very difficult\nExpectations: Exceeds expectations, Fully meets expectations, Does not fully meet expectations, Does not meet expectations at all\nExtent (5 pt): A great deal (Completely, if appropriate), Considerably, Moderately, Slightly, Not at all\nExtent (4 pt): Significantly, Moderately, Slightly, Not at all\nFrequency (no set time): Always, Often, Occasionally, Rarely, Never\nFrequency (general): Daily, Weekly, Monthly, Once a semester, Once a year, Never\nFrequency (based on time frame): More than 5 times, 4 - 5 times, 2 - 3 times, 1 time, Less than 1 time, Never\nFrequency (extended): More than once a week, Once a week, Once a month, Once a semester, Once a year, Less than once a year, Never\nHelpfulness: Extremely helpful, Very helpful, Moderately helpful, Slightly helpful, Not at all helpful\nImportance: Extremely important, Very important, Moderately important, Slightly important, Not at all important\nInterest: Extremely interested, Very interested, Moderately interested, Slightly interested, Not at all interested\nLikelihood: Very likely, Moderately likely, Neither likely nor unlikely, Moderately unlikely, Very unlikely\nNumeric Scales: Less than #, About the same, More than #\nProbability: Definitely would, Probably would, Probably wouldn’t, Definitely wouldn’t\nProficiency: Beginner, Developing, Competent, Advanced, Expert (typical for Rubrics)\nQuality: Excellent, Good, Average, Below average, Poor\nSatisfaction: Very satisfied, Moderately satisfied, Neither satisfied nor dissatisfied, Moderately dissatisfied, Very dissatisfied (another version removes the “moderately” qualifier and/or uses “neutral”)\nTaken from https://baselinesupport.campuslabs.com/hc/en-us/articles/204305485-Recommended-Scales"
  },
  {
    "objectID": "sections/surveys/index.html",
    "href": "sections/surveys/index.html",
    "title": "Overview",
    "section": "",
    "text": "This chapter is about best practices and common issues in designing surveys.\nAs suggested in some review papers Simms (2008), the construction of a new survey or scale consists of several steps:\nI will try to cover as many of these topics as possible. Currently, there is a chapter on response options and one containing some recommended readings. There is also a Statistics section on this website that covers factor analyses."
  },
  {
    "objectID": "sections/surveys/item-development.html",
    "href": "sections/surveys/item-development.html",
    "title": "Item Development",
    "section": "",
    "text": "This step is an important step in the development of a scale as serious problems with the item pool will reverberate through all subsequent data analyses and scale construction efforts.\nitems should be written that are (i) relevant to the constructs to be measured, and (ii) representative of all potentially important aspects of the target construct. Having formal construct definitions is particularly important here, as such definitions should guide the item writing process.\nBesides including items to cover all the different facets of a particular construct, it’s also important that the item pool includes items reflecting all levels of the construct.\nItem writing guidelines (Simms, 2008):"
  },
  {
    "objectID": "sections/surveys/item-development.html#pilot-testing",
    "href": "sections/surveys/item-development.html#pilot-testing",
    "title": "Item Development",
    "section": "Pilot testing",
    "text": "Pilot testing\nAfter the initial item pool is complete, it makes sense to pilot test the items before running a large-scaled exploratory study.\nFactor loadings can be improved by using multiple response (Likert-type) items, as they generally result in higher loadings than two-choice items (Comrey & Montag, 1982; Oswald & Velicer, 1980; Velicer, DiClemente, & Corriveau, 1984; Velicer, Govia, Cherico, & Corriveau, 1985; Velicer & Stevenson, 1978). Likewise, the quality of item writing can affect the size of the loadings, that is, the expression of an item in simple language, restricting the item to a single idea, or using content that is appropriate to a majority of respondents are all ways of improving items."
  },
  {
    "objectID": "sections/WIP/item-development.html",
    "href": "sections/WIP/item-development.html",
    "title": "Item Development",
    "section": "",
    "text": "Reverse-scored or reverse worded items can be included to determine whether participants are paying attention and don’t just select the same response on each item. However, there is some evidence that reverse-scored items reduce the reliability of the scale or produce an unexpected factor structure [@swain2008].\nAnother important consideration is that reverse-worded items can affect the model fit. Factor analyses of scales with some RW items frequently indicate the presence of method covariance obscuring or confounding substantive covariance (e.g., Brown, 2003; Roszkowski & Soven, 2010)."
  },
  {
    "objectID": "sections/WIP/item-development.html#number-of-items",
    "href": "sections/WIP/item-development.html#number-of-items",
    "title": "Item Development",
    "section": "Number of items",
    "text": "Number of items\nThere are no hard-and-fast rules guiding this decision, but keeping a measure short is an effective means of minimizing response biases caused by boredom or fatigue (Schmitt & Stults, 1985; Schriesheim & Eisenbach, 1990). Additional items also demand more time in both the development and administration of a measure (Carmines & Zeller, 1979). Harvey, Billings, and Nilan (1985) suggest that at least four items per scale are needed to test the homogeneity of items within each latent construct. Adequate internal consistency reliabilities can be obtained with as few as three items (Cook et al., 1981), and adding items indefinitely makes progressively less impact on scale reliability (Carmines & Zeller, 1979). It is difficult to improve on the internal consistency reliabilities of five appropriate items by adding items to a scale (Hinkin, 1985; Hinkin & Schriesheim, 1989; Schriesheim & Hinkin, 1990). Cortina (1993) found that scales with many\nitems may have high internal consistency reliabilities even if item intercorrelations are low, an argument in favor of shorter scales with high internal consistency. It is also important to assure that the domain has been adequately sampled, as inadequate sampling is a primary source of measurement error (Churchill, 1979). As Thurstone (1947) points out, scales should possess simple structure, or parsimony. Not only should any one measure have the simplest possible factor constitution, but any scale should require the contribution of a minimum number of items that adequately tap the domain of interest. These findings would suggest that the eventual goal will be the retention of four to six items for most constructs, but the final determination must be made only with accumulated evidence in support of the construct validity of the measure. It should be anticipated that approximately one half of the created items will be retained for use in the final scales, so at least twice as many items as will be needed in the final scales should be generated to be administered in a survey questionnaire.\nhttps://twitter.com/dingding_peng/status/1481683536499331079\nhttps://psyarxiv.com/4kra2/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a website to organize my conclusions about topics in methodology and statistics.\nThe website is divided up into multiple sections. At this moment, there is a section on survey design and a section on statistics. The survey design section is about various issues pertaining to designing surveys, such as how to create items, what kind of response options to use, and so on. The statistics section is currently only about factor analyses, including both exploratory and confirmatory factor analyses.\nMore sections will be added in the future."
  },
  {
    "objectID": "index.html#word-of-caution",
    "href": "index.html#word-of-caution",
    "title": "About",
    "section": "Word of Caution",
    "text": "Word of Caution\nSome sections in the book are not yet finished. They’re work in progress. Sometimes I think they’re already a bit useful, so I make them public. If I think they’re not finished/polished yet, I put a warning at the start of the page to indicate that.\nHaving said that, this website will forever be a work in progress because the content is about current best practices. As best practices are likely to change over time, so the content of this website will change with it. If I realize that something is heavily outdated, I will make a note of it.\nIt’s also very likely that there are mistakes. These mistakes can theoretically range from gross errors to simple typos, or a reliance on outdated information. If you find a mistake, please contact me or click on the GitHub link on each page and create an Issue."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe following people have contributed to this book:\n\nWillem Sleegers\n\nIf I’ve missed your contributions and you deserve to be on this list, please don’t hesitate to contact me or add yourself via a Pull Request on GitHub!"
  }
]