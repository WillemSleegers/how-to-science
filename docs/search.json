[
  {
    "objectID": "sections/statistics/index.html",
    "href": "sections/statistics/index.html",
    "title": "Overview",
    "section": "",
    "text": "This section is about statistics-related topics. At the moment there are two chapters. One of exploratory factor analyses and one of confirmatory factor analyses."
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA-examples.html",
    "href": "sections/statistics/factor-analysis/CFA-examples.html",
    "title": "CFA - Examples",
    "section": "",
    "text": "# Load packages\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-11\nlavaan is FREE software! Please report any bugs.\n\nlibrary(dynamic)\n\nBeta version. Please report bugs: https://github.com/melissagwolf/dynamic/issues."
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA-examples.html#rosenberg-self-esteem-scale",
    "href": "sections/statistics/factor-analysis/CFA-examples.html#rosenberg-self-esteem-scale",
    "title": "CFA - Examples",
    "section": "Rosenberg Self-Esteem Scale",
    "text": "Rosenberg Self-Esteem Scale\nThe Rosenberg self-esteem scale (Rosenberg, 1989) is a unidimensional scale to assess global self-esteem. It is one of the most commonly used scales in psychology and could therefore serve as a nice illustration of how to assess whether the data fits a single factor model.\nIt should be noted, however, that apparently there is some controversy whether the scale is actually unifactorial (Donnellan et al., 2016).\n\nThe Data\n\n# Load data\nitems <- read_csv(\"RSECSV.csv\")\n\nRows: 1127 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): R1, R2, R3, R4, R5, R6, R7, R8, R9, R10\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Data preparation\n# Replace all -9999 values with NA\nitems <- mutate(items, across(everything(), ~ na_if(.x, -9999)))\n\nThe data was taken from Donnellan et al. (2016), who conducted a study to assess the factorial structure of the RSE. The data consists of nrow(items) participants, who were college students at a public university in the Southwestern United States.\n\n\nThe Model\n\nmodel <- \"\n  self_esteem =~ R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10\n\"\n\nfit <- cfa(model = model, data = items, estimator = \"MLR\")\nfitmeasures(fit, \n  fit.measures = c(\"pvalue\", \"pvalue.scaled\", \"srmr\", \"rmsea\", \"cfi\"))\n\n       pvalue pvalue.scaled          srmr         rmsea           cfi \n        0.000         0.000         0.082         0.165         0.802 \n\n#fit_cutoffs <- cfaOne(fit)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA-examples.html#marshs-self-description-questionnaire",
    "href": "sections/statistics/factor-analysis/CFA-examples.html#marshs-self-description-questionnaire",
    "title": "CFA - Examples",
    "section": "Marsh’s Self-Description Questionnaire",
    "text": "Marsh’s Self-Description Questionnaire\n\n# Load data\nmarsh <- read_csv(\"marsh.csv\")\n\nRows: 15661 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (13): Eng1, Eng2, Eng3, Eng4, Math1, Math2, Math3, Math4, Par1, Par2, Pa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nData for this example is from 15,661 students in 10th grade who participated in the National Education Longitudinal Study of 1988. Items from this scale include:\nParents:\n\nMy parents treat me fairly\nI do not like my parents very much\nI get along well with my parents\nMy parents are usually unhappy or disappointed with what I do\nMy parents understand me\n\nEnglish\n\nI learn things quickly in English classes\nEnglish is one of my best subjects\nI get good marks in English\nI’m hopeless in English classes\n\nMathematics\n\nMathematics is one of my best subjects\nI have always done well in mathematics\nI get good marks in mathematics\nI do badly in tests of mathematics\n\n\nmarsh_model <- \"\n  english =~ Eng1 + Eng2 + Eng3 + Eng4\n  math    =~ Math1 + Math2 + Math3 + Math4\n  parent  =~ Par1 + Par2 + Par3 + Par4 + Par5\n\"\n\nmarsh_fit <- cfa(model = marsh_model, data = marsh, estimator = \"MLR\")\nfitmeasures(marsh_fit, \n  fit.measures = c(\"pvalue\", \"pvalue.scaled\", \"srmr\", \"rmsea\", \"cfi\")\n)\n\n       pvalue pvalue.scaled          srmr         rmsea           cfi \n        0.000         0.000         0.051         0.083         0.935 \n\n#marsh_fit_cutoffs <- cfaHB(marsh_fit)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/examples.html",
    "href": "sections/statistics/factor-analysis/examples.html",
    "title": "Examples",
    "section": "",
    "text": "References\n\nKahane, G., Everett, J. A. C., Earp, B. D., Caviola, L., Faber, N. S., Crockett, M. J., & Savulescu, J. (2018). Beyond sacrificial harm: A two-dimensional model of utilitarian psychology. Psychological Review, 125(2), 131–164. https://doi.org/10.1037/rev0000093"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html",
    "href": "sections/statistics/factor-analysis/EFA.html",
    "title": "Exploratory Factor Analysis",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\nThe goal of an exploratory factor analysis (EFA) is to study latent factors that underlie responses to a larger number of items. In other words, the goal is to explore the data and reduce the number of variables. It is a popular technique in the development and validation of assessment instruments.\nUnlike confirmatory factor analysis (CFA), EFA is used when there is little or no a priori justification for specifying a particular structural model. This means that there is reasonable uncertainty about the number of underlying factors and which items load on which factor. The hope is to resolve some of that uncertainty empirically."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#preparation",
    "href": "sections/statistics/factor-analysis/EFA.html#preparation",
    "title": "Exploratory Factor Analysis",
    "section": "Preparation",
    "text": "Preparation\nBefore you perform a factor analysis, make sure that the design of your study is suitable for this type of analysis. This means you should have an appropriate sample, sample size, and indicators (items).\n\nSample\nIt may be obvious, but make sure to target participants that are likely to vary in the attitude that you’re interested in measuring and that the sample is representative of the population that you’re interested in.\n\n\nSample size\nRules of thumb are one way to determine an appropriate sample size, but the general consensus is that rules of thumb are a bad idea [MacCallum et al. (1999), Costello & Osborne (2005)]. The reason for that is that rules of thumb only involve one of two factors: a set number of participants or a particular participant-to-item ratio. The reality is that the appropriate sample size is dependent on many more factors, such as communalities, factor loadings, the number of indicators per factor, and the number of factors [Gagne & Hancock (2006), MacCallum et al. (1999), MacCallum et al. (2001), Velicer & Fava (1998)].\nGiven the replication failures in the social sciences, it should be no surprise that also in the EFA literature there appears to be a sample size problem. This is perhaps best demonstrated by Costello & Osborne (2005). They reviewed two years’ worth of PsychINFO articles and found that the majority of the studies (62.9%) had participants to item ratios of 10:1 or less. Of course, the participant to item ratio is not a good benchmark for the appropriate sample size, so this is not enough to demonstrate that the sample size is insufficient. They did find support that this is not enough by sampling data of various sample sizes from a large data set of responses to the Marsh’s Self-Description Questionnaire. They found that only 70% of their large (20:1) samples produced correct solutions, leading them to conclude that a 20:1 participant to item ratio produces error rates well above the field standard alpha = .05 level.\nSo how many participants should you recruit? In my opinion, the most useful study is a simulation study by Mundfrom et al. (2005). They ran simulations to determine the minimum sample size for a study, taking into account the number of factors, the ratio of items to factors, and several different communalities. They generated a variety of population correlation matrices under different conditions, repeatedly sampled from these population structures, and determined coefficients of congruence between the sample solutions and population structures (similar to Velicer & Fava (1998)). This allowed them to produce a table with minimum recommend sample sizes for different situations.\n\nRecommended minimum sample sizes by Mundfrom et al. (2005)\n\n\n\n\n\n\n\n\nCriterion\nCommunality\nItems per factor\nMinimum sample size\n\n\n\n\nExcellent (0.98)\nHigh (.6 to .8)\n4\n500\n\n\n\n\n6\n250\n\n\n\n\n8\n100\n\n\n\nWide (.2 to .8)\n4\n900\n\n\n\n\n6\n200\n\n\n\n\n8\n130\n\n\n\nLow (.2 to .4)\n4\n1400\n\n\n\n\n6\n260\n\n\n\n\n8\n130\n\n\nGood (0.92)\nHigh (.6 to .8)\n5\n130\n\n\n\n\n7\n55\n\n\n\nWide (.2 to .8)\n5\n140\n\n\n\n\n8\n60\n\n\n\nLow (.2 to .4)\n5\n200\n\n\n\n\n8\n80\n\n\n\nAs can be seen in the table, the appropriate sample size is dependent on several factors and is not simply a function of sample size alone or a particular participant to item ratio.\n\n\nNumber of items\nThe previous section on sample size shows that a relevant factor in determining the minimum sample size is the number of items per factor, so how many items should you create and include in the factor analysis?\nSince this is an exploratory factor analysis, it is not clear in advance how many items will actually survive the exploration. It could be that certain items will not be well understood by participants (although this should be caught in pilot testing prior to running an EFA) or that multiple items simple do not fit in the expected factor structure. Since not all items will likely behave well, you should always include more items than you intend to keep.\nIt’s also useful to keep in mind that for a single factor model to be identifiable, it must consist of at least three indicators and preferably four to also allow for the model to be statistically testable [Gana & Broc (2019), also see Velicer & Fava (1998) and Russell (2002)]. This is useful for when you want to do a CFA later or if you want others to be able to perform CFAs on your factor analysis results.\nThe number of indicators also depends on the quality of indicators. Costello & Osborne (2005) notes that 5 or more strongly loading items (.50 or better) indicate a solid factor. MacCallum et al. (1999) writes that more indicators is generally better [also see Marsh et al. (1998), but see Hayduk & Littvay (2012)]. Within the range of indicators they studied (three to seven per factor), it is better to have more indicators than fewer. The critical point, however, is that these indicators must be reasonably valid and reliable.\nVelicer & Fava (1998) suggest that 6-10 initial items per factor is recommended as 25% to 50% will not perform as expected and the end goal should be to have four- or five to-one as a minimum. The bare minimum of three variables was found to be insufficient (based on two simulation studies) and a more prudent target would be to have four- or five to-one as a minimum. A ratio of 20-30 initial items per factor is also possible as an appropriate target for extensive oversampling, which may be needed in cases where it is not possible to obtain a large sample size.\nFinally, you should also take into account the desirable scale length. Shorter scales have the desirable property that they are quick to administer, meaning they can more easily be added to a study or included in short studies to obtain a larger sample size. The goal may therefore be to find only a handful of solid indicators for a factor."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#data-analysis",
    "href": "sections/statistics/factor-analysis/EFA.html#data-analysis",
    "title": "Exploratory Factor Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nRelevant items\nOne design issue that is especially important is about which measured variables to include in the study. If measured variables irrelevant to the domain of interest are included, then spurious common factors might emerge or true common factors might be obscured (Fabrigar et al., 1999).\n\n\nSampling Adequacy\nThe Kaiser–Meyer–Olkin (KMO) test is a statistical measure to determine how suitable the data is for factor analysis. The test measures sampling adequacy for the complete model and each variable in the model. The statistic indicates the degree to which each variable in a set is predicted without error by the other variables.\nThe KMO value ranges from 0 to 1, with 0.60 considered suitable for factor analysis (Tabachnick & Fidell, 2013). Kaiser (1974) himself suggested that KMO > .9 were marvelous, in the .80s, meritorious, in the .70s, middling, in the .60s, mediocre, in the 50s, miserable, and less than .5, unacceptable.\nBartlett’s test of sphericity is a notoriously sensitive test of the hypothesis that the correlations in a correlation matrix are zero. Because of its sensitivity and its dependence on N, the test is likely to be significant with samples of substantial size even if correlations are very low. Therefore, use of the test is recommended only if there are fewer than around five participants per variable (Tabachnick & Fidell, 2013).\n\n\nPrincipal Components vs. Factor Analysis\nAn important consideration is whether to use a principal components analysis (PCA) or a factor analysis. PCA is a data reduction technique and while a factor analysis is also a type of data reduction technique, the focus with PCA is more on simply reducing the data, without regard for a theoretical interpretation. With PCA, the variables themselves are of interest, rather than a hypothetical latent construct. Constructs are conceptualized as being causally determined by the observations; that is, CFA reflects a formative model rather than a reflective one (Edwards & Bagozzi, 2000). If you are a psychologist trying to create a measure of a psychological construct, this is probably not what you want as principal component scores are “caused” by their indicators in much the same way that sum scores are “caused” by item scores.” [Borsboom (2006), p. 426]. Instead, you likely want the opposite causal relationship in which a latent factor causes the indicator scores.\n\n\nCode\nDiagrammeR::grViz(\"\n  digraph dot {\n    graph [\n      layout = dot, \n      rankdir = LR, \n      fontname = 'Source Sans Pro', \n      label = 'Illustration of a PCA model (left) and a factor analysis model (right).'\n    ]\n    \n    node [fontname = 'Source Sans Pro']\n    node [shape = square]\n    var1 [label = 'item 1']\n    var2 [label = 'item 2']\n    var3 [label = 'item 3']\n    var4 [label = 'item 4']\n    var5 [label = 'item 1']\n    var6 [label = 'item 2']  \n    var7 [label = 'item 3']  \n    var8 [label = 'item 4']\n  \n    node [shape = circle]\n    construct_a [label = 'construct A']\n    construct_b [label = 'construct B']\n  \n    edge [color = black, minlen = 3]\n    {var1 var2 var3 var4} -> construct_a\n    construct_b -> {var5 var6 var7 var8}\n  \n    edge[style = invis, minlen = 1];\n    construct_a -> construct_b\n  }\"\n)\n\n\n\n\n\n\nA related issue is that factor analysis assumes that the total variance can be partitioned into common and unique variance and PCA assumes that the common variances takes up all of total variance. This means that PCA assumes that all variables are measured without error. It is usually more reasonable to assume that you have not measured your set of items perfectly.\nAlthough some argue that the two methods have the same results (Velicer & Jackson, 1990), there is evidence that those similarities are mistaken and that factor analysis has better results than PCA (Widaman, 1993).\nConclusion: Do not use PCA.\n\n\nFactor Extraction Method\nExtraction is the general term for the process of reducing the number of dimensions being analyzed from the number of variables in the data set (and matrix of associations) into a smaller number of factors.\nThere are multiple factor extract methods, such as:\n\nminres\nunweighted least squares (ULS)\ngeneralized least squares (GLS)\nmaximum likelihood\nprincipal axis factor(ing)\nalpha factor(ing)\nimage factor(ing)\n\nIt’s not entirely clear which factor extraction method is the best and some authors use different terms for some of the methods, making it more difficult to compare them.\nFabrigar et al. (1999) argued that if data are relatively normally distributed, maximum likelihood is the best choice because “it allows for the computation of a wide range of indexes of the goodness of fit of the model and permits statistical significance testing of factor loadings and correlations among factors and the computation of confidence intervals.” (p. 277). In case the data is not generally normally distributed, they recommend principal axis factoring. This is recommended in several sources (Costello & Osborne (2005); Osborne (2014)).\nConclusion: Use the maximum likelihood method if the data is generally normally distributed and to use principal axis factoring if the data is non-normal.\n\n\nRotation methods\nThe goal of rotation is to clarify the factor structure and make the results of the EFA more interpretable.\nRotation methods can be categorized into one of two categories: orthogonal or oblique. Orthogonal rotations keep axes at a 90 degree angle, forcing the factors to be uncorrelated. Oblique rotations allow angles that are not 90 degrees , thus allowing factors to be correlated if that is optimal for the solution.\nOrthogonal rotation methods include:\n\nvarimax\nquartimax\nequamax\n\nOblique rotation methods include:\n\ndirect oblimin\nquartimin\npromax\n\nIn the social sciences, we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution. If the factors are truly uncorrelated, orthogonal and oblique rotation produce nearly identical results. Since oblique rotation will reproduce an orthogonal solution but not vice versa, it makes sense to go for oblique rotation.\nThere is no widely preferred method of oblique rotation; all tend to produce similar results (Fabrigar et al., 1999).\nConclusion: Use any oblique rotation.\n\n\nNumber of factors\nThere are several methods to determine how many factors to retain:\n\nTheory\nKaiser criterion\nScree test\nParallel analysis\nVelicer’s multiple average partial (MAP) procedure\nAkaike information criterion\nBayesian information criterion\nComparison data\nVery Simple Structure (VSS)\nThe root-mean-square error of approximation\nLikelihood ratio statistic\n\nDetermining the number of factors is probably the most difficult part of the exploratory factor analysis. As Henry Kaiser said:\n\nSolving the number of factors problem is easy, I do it everyday before breakfast. But knowing the right solution is harder.\n\nThe difficulty of this step means that a certain flexibility is warranted.\nIt makes sense to begin with the expectation that you will see the theorized factor structure. Instruments are rarely perfect (especially the first time it is examined), and theoretical expectations are not always supported, but unless one is on a totally blind fishing expedition, this is a good place to start.\nThe default in most statistical software packages is to use the Kaiser criterion. It makes some sense, as an eigenvalue represents the sum of the squared factor loadings in a column, and to get a sum of 1.0 or more, one must have rather large factor loadings to square and sum. However, this is easily achieved with more items and there are now alternative methods. Hence, there is broad consensus in the literature that this is among the least accurate methods for selecting the number of factors to retain (Velicer & Jackson, 1990).\nThe scree test involves examining the graph of the eigenvalues and looking for the natural bend or “elbow” in the data where the slope of the curve changes markedly. Although the scree plot itself is not considered sufficient to determine how many factors should be extracted (Velicer et al., 2000), it does appear to be a relatively reliable method. The main down side seems to be its ambiguity as the bend is not always clear and sometimes there are even multiple bends.\nParallel analysis involves generating random uncorrelated data, and comparing eigenvalues from the EFA to the eigenvalues from the random data. Using this process, only factors with eigenvalues that are above random eigenvalues should be retained, although it is not clear how much above the random eigenvalues they should be. Several authors have endorsed this as the most robust and accurate process for determining the number of factors to extract [Ledesma & Valero-Mora (2007), Velicer et al. (2000)].\nThe Minimum Average Partial (MAP) criterion involves partialing out common variance as each successive component is created. As each successive component is partialed out, common variance will decrease to a minimum. Velicer argued that minimum point should be considered the criterion for the number of factors to extract.\nVSS involves degrading the initial rotated factor solution by assuming that the nonsalient loadings are zero, even though in actuality they rarely are. What VSS does is test how well the factor matrix we think about and talk about actually fits the correlation matrix. It is not a confirmatory procedure for testing the significance of a particular loading, but rather it is an exploratory procedure for testing the relative utility of interpreting the correlation matrix in terms of a family of increasingly more complex factor models. The simplest model tested by VSS is that each item is of complexity one, and that all items are embedded in a more complex factor matrix of rank k. This is the model most appropriate for scale construction and is the one we use most frequently when we talk about factor solutions. More complicated models may also be evaluated by VSS.\nZwick & Velicer (1986) tested the scree test, Horn’s parallel test, and Velicer’s MAP test (among others) in simulation studies using a data set with a clear factor structure. Both the parallel test and MAP test seemed to work well. Ruscio & Brendan Roche (2012) notes that PA is considered to be the method of choice among methodologists and recommend that researchers take advantage of PA as a starting point, perhaps supplemented by CD. They note that researchers can also use more than one method. Osborne (2014) notes that MAP has been considered superior to the “classic” criteria, and probably is superior to parallel analysis, although neither is perfect, and all must be used in the context of a search for conceptually interpretable factors. He recommends to use parallel analysis or MAP criteria, along with theory (and any of the classic criteria that suits you and is defensible).\nFinally, empirical research suggests that overfactoring introduces much less error to factor loading estimates than underfactoring [Fava & Velicer (1996), Wood et al. (1996)]. Although you should be skeptical of solutions with too many factors because the factors may not be meaningful and parsimony should also be considered.\nConclusion: Use multiple criteria, including theory, to make a judgment call about how many factors to extract. When in doubt, favor more factors rather than fewer factors."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#interpretation",
    "href": "sections/statistics/factor-analysis/EFA.html#interpretation",
    "title": "Exploratory Factor Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nRemember that the goal of exploratory factor analysis is to explore whether your data fits a model that makes sense. Ideally, you have a conceptual or theoretical framework for the analysis. Even if you do not, the results should be sensible in some way. You should be able to construct a simple narrative describing how each factor, and its items, makes sense and is easily labeled.\n\nFactor Loadings\nAfter determining the number of factors and rotation, you will be able to produce a table of factor loadings. The question is now to see whether there are items that load sufficiently strongly on each factor. If the goal is to have unidimensional factors, then cross loadings should also be examined. Items that don’t perform well may be removed.\nThere are different recommendations about what kind of factor loading cutoff threshold to use. Comrey & Lee (1992) suggest that loadings in excess of .71 (50% overlapping variance) are considered excellent, .63 (40% overlapping variance) very good, .55 (30% overlapping variance) good, .45 (20% overlapping variance) fair, and .32 (10% overlapping variance) poor. Tabachnick & Fidell (2013) cite .32 as a good rule of thumb for the minimum loading of an item, assuming the sample size is larger than 300. Others say item loadings above .30 (Costello & Osborne, 2005). Clark & Watson (1995) say larger than .35.\nIn a review on the topic, Peterson (2000) found that the average factor loading cutoff threshold is .40.\nUsing a particular threshold is, however, insufficient if sample size is not taken into account. Factor loadings, like many other statistics, are estimated statistics and may be associated with large uncertainty intervals, depending on the sample size. It’s possible for a sample factor loading of .70 to be 0 in the population, if the sample size was low (Cudeck & O’Dell, 1994). A first benchmark, therefore, should be whether the factor loading is significantly different from 0. The standard errors of factor loadings can be calculated in different ways. If computational power is not an issue, non-parametric bootstrapping seems to be the preferred method (Zhang, 2014).\nNote that a multiple comparisons correction (e.g., Bonferonni) must be used to control familywise Type 1 errors.\nFinally, one must be careful not to prematurely drop poorly performing items, especially when such items were predicted a priori to be strong markers of a given factor.\nConclusion: Calculate standard errors for factor loadings and use significance tests as the first benchmark to retain an item.\n\n\nReliability\nThere are different types of reliabilities that can be calculated to assess the reliability of the scale:\n\nInternal consistency\nTest-retest reliability\n\nCronbach’s alpha is the most popular measure of internal reliability, with recommended cutoffs of .80 in a basic science setting and .9 or .95 in an applied setting [Clark & Watson (1995)]. Note that Cronbach’s alpha relies on the assumption of unidimensionality, which means it cannot be used as evidence for unidimensionality. In fact, Cronbach’s alpha’s assumes equally sized factor loadings. Given that this is an unlikely assumption, and that Cronbach’s alpha is also influenced by the number of items, alternative measures of reliability should be used [e.g., see Peters (2014), Hayes & Coutts (2020)].\nMcDonald’s omega does not assume equally sized factor loadings. Cronbach’s alpha is actually a special case of McDonald’s omega, assuming the untenable assumption of equally sized factor loadings.\nConclusion: Use McDonald’s omega."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#an-example",
    "href": "sections/statistics/factor-analysis/EFA.html#an-example",
    "title": "Exploratory Factor Analysis",
    "section": "An Example",
    "text": "An Example\n\n\nCode\n# Load packages\nlibrary(psych)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#recommended-reading",
    "href": "sections/statistics/factor-analysis/EFA.html#recommended-reading",
    "title": "Exploratory Factor Analysis",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nI found the following best practices papers useful:\n\nEvaluating the use of exploratory factor analysis in psychological research by Fabrigar et al. (1999)\nIn search of underlying dimensions: The use (and abuse) of factor analysis in personality and social psychology bulletin by Russell (2002)\nBest practices in exploratory factor analysis by Osborne (2014)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#glossary",
    "href": "sections/statistics/factor-analysis/EFA.html#glossary",
    "title": "Exploratory Factor Analysis",
    "section": "Glossary",
    "text": "Glossary\n\ncommunality: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\neigenvalue: the amount of variance explained by a factor; the sum of squared factor loadings\nh2: See communality.\nu2: 1 - h2. residual variance, a.k.a. uniqueness"
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA.html",
    "href": "sections/statistics/factor-analysis/CFA.html",
    "title": "Confirmatory Factor Analysis",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\nNote that if you have a clear theoretical basis about the number of factors and which items should load on which factor, CFA is more appropriate than EFA because it allows you to directly test your model. You could also do both, but a CFA should be conducted on new data to test its validity.\nTo incorporate:\nFor confirmatory factor analyses there are also several ways to determine the sample size, including:"
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA.html#recommended-reading",
    "href": "sections/statistics/factor-analysis/CFA.html#recommended-reading",
    "title": "Confirmatory Factor Analysis",
    "section": "Recommended Reading",
    "text": "Recommended Reading\n\nDynamic Fit Index Cutoffs for Confirmatory Factor Analysis Models by McNeish & Wolf (2021)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/r-packages.html",
    "href": "sections/statistics/factor-analysis/r-packages.html",
    "title": "R-packages",
    "section": "",
    "text": "psych\nlavaan\nlavaan.survey\nsemTools\nsemPower"
  },
  {
    "objectID": "sections/statistics/group-differences/sequential-analyses.html",
    "href": "sections/statistics/group-differences/sequential-analyses.html",
    "title": "Sequential analyses",
    "section": "",
    "text": "Code\n# Load packages\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(brms)\nlibrary(emmeans)\nlibrary(BayesFactor)\nlibrary(tidybayes)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set seed\nset.seed(1)\nThis section is about performing sequential analyses."
  },
  {
    "objectID": "sections/statistics/group-differences/sequential-analyses.html#bayesian",
    "href": "sections/statistics/group-differences/sequential-analyses.html#bayesian",
    "title": "Sequential analyses",
    "section": "Bayesian",
    "text": "Bayesian\n\nTwo groups\nFor this scenario we simulate data from two different groups (say a control group and an experimental group).\n\n\nCode\n# Set the simulation parameters\nMs <- c(0, 0.25)\nSDs <- 1\nn <- 250\nlabels <- c(\"control\", \"experimental\")\n\n# Produce the variance-covariance matrix\nSigma <- matrix(\n  nrow = length(Ms), \n  ncol = length(Ms), \n  data = c(\n    SDs^2, 0,\n    0, SDs^2\n  )\n)\n\n# Simulate\nm <- mvrnorm(n = n, mu = Ms, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(m) <- labels\n\ndata <- as_tibble(m)\n\ndata <- pivot_longer(\n  data = data, \n  cols = everything(), \n  names_to = \"condition\", \n  values_to = \"DV\"\n)\n\ndata <- mutate(data, id = 1:n(), .before = condition)\n\n\nThere are different Bayesian ways to analyze this data. One can focus on estimation or on Bayes factors.\n\nEstimation\nIf estimation is the goal, we run the model (or models) and obtain the posterior distribution of the estimates of interest. Below we run subsequent models with each model using more of the data.\n\n\nCode\nns <- c(10, 20, 30, 40, 50, 75, 100, 150, 200, 250)\nresults <- tibble()\n\nfor (i in 1:length(ns)) {\n  # Get the sample size\n  n <- ns[i]\n  \n  # Draw a sample of size n\n  sample <- slice_head(data, n = n)\n  \n  # If this is the first iteration, run the full brms model\n  # Else update the model\n  if (i == 1) {\n    model <- brm(\n      formula = DV ~ 0 + Intercept + condition, \n      data = sample, \n      family = gaussian(), \n      prior = c(\n        set_prior(coef = \"Intercept\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"b\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"sigma\", prior = \"normal(1, 1)\")\n      )\n    )\n  } else {\n    model <- update(model, newdata = sample)\n  }\n  \n  # Get the posteriors of the model estimates\n  draws <- as_draws_df(model)\n  \n  # Add sample information\n  draws <- mutate(draws, step = i, n = n)\n  \n  # Add the draws to the results data frame\n  results <- bind_rows(\n    results, \n    draws\n  )\n}\n\n\nNow we have the draws of the posterior distribution of the model estimates. We can plot these using the following code.\n\n\nCode\nfinal_quantiles <- results %>%\n  filter(n == 250) %>%\n  pull(b_conditionexperimental) %>%\n  quantile(probs = c(.025, .975))\n\nggplot(results, aes(x = factor(n), y = b_conditionexperimental)) +\n  stat_halfeye() +\n  geom_hline(yintercept = final_quantiles, linetype = \"dashed\", alpha = .5) +\n  labs(x = \"Sample size per condition (n)\", y = \"\")\n\n\n\n\n\nHorizontal lines show the 95% interval of the estimate from the largest sample.\n\n\n\n\n\n\nBayes factors\nThis is still a work in progress. I have yet to figure out the best way to obtain Bayes factors using the brms package.\nBelow I use both brms and BayesFactor to calculate Bayes factors for the effect of condition across various sample sizes.\n\n\nCode\nns <- c(10, 20, 30, 40, 50, 75, 100, 150, 200, 250)\nresults <- tibble()\n\nfor (i in 1:length(ns)) {\n  # Get the sample size\n  n <- ns[i]\n  \n  # Draw a sample of size n\n  sample <- slice_head(data, n = n)\n  \n  # If this is the first iteration, run the full brms model\n  # Else update the model\n  if (i == 1) {\n    model <- brm(\n      formula = DV ~ 0 + Intercept + condition, \n      data = sample, \n      family = gaussian(), \n      prior = c(\n        set_prior(coef = \"Intercept\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"b\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"sigma\", prior = \"normal(1, 1)\")\n      ), \n      sample_prior = TRUE\n    )\n  } else {\n    model <- update(model, newdata = as.data.frame(sample))\n  }\n  \n  # Calculate the BF\n  BF_brms <- hypothesis(model, \"conditionexperimental = 0\")\n\n  # Also calculate the BF with the testBF() function from BayesFactor\n  BF_BF <- ttestBF(formula = DV ~ condition, data = sample)\n  \n  # Add the information to the bayes factors data frame\n  results <- bind_rows(\n    results, \n    tibble(\n      step = i,\n      n = n,\n      brms = BF_brms$hypothesis$Evid.Ratio,\n      BayesFactor = extractBF(BF_BF)$bf\n    )\n  )\n}\n\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\nWarning: data coerced from tibble to data frame\n\n\nNext we plot the Bayes factors for each sample size and for each method of calculating the Bayes factor.\n\n\nCode\nresults_long <- results %>%\n  mutate(BayesFactor = 1 / BayesFactor) %>%\n  pivot_longer(\n    cols = c(brms, BayesFactor), \n    names_to = \"method\", \n    values_to = \"BF\")\n\nggplot(\n    data = results_long, \n    mapping = aes(x = factor(n), y = BF, linetype = method, group = method)\n  ) +\n  geom_line() +\n  labs(\n    x = \"Sample size per condition (n)\", \n    y = expression(BF[\"10\"]), \n    linetype = \"Method\"\n  )\n\n\n\n\n\nBayes factors per sample size\n\n\n\n\n\n\n\nFour groups\nLet’s simulate data for a scenario in which we have 4 different between-subjects conditions. The conditions differ from each by a small amount and for simplicity’s sake each condition has a standard deviation of 1.\n\n\nCode\n# Set the simulation parameters\nMs <- c(0, 0.2, 0.4, 0.6)\nSDs <- 1\nn <- 250\nlabels <- c(\"A\", \"B\", \"C\", \"D\")\n\n# Produce the variance-covariance matrix\nSigma <- matrix(\n  nrow = length(Ms), \n  ncol = length(Ms), \n  data = c(\n    SDs^2, 0, 0, 0,\n    0, SDs^2, 0, 0,\n    0, 0, SDs^2, 0,\n    0, 0, 0, SDs^2\n  )\n)\n\n# Simulate\nm <- mvrnorm(n = n, mu = Ms, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(m) <- labels\n\ndata <- as_tibble(m)\n\ndata <- pivot_longer(\n  data = data, \n  cols = everything(), \n  names_to = \"condition\", \n  values_to = \"DV\"\n)\n\ndata <- mutate(data, id = 1:n(), .before = condition)\n\n\n\nEstimation\nWith this data we can run multiple sequential models (like in the 2 groups scenario), except this time we calculate contrasts between all the levels of the condition factor. We again obtain the posteriors of these contrasts and store them so we can plot them afterwards.\n\n\nCode\nns <- c(10, 20, 30, 40, 50, 75, 100, 150, 200, 250)\nresults <- tibble()\n\nfor (i in 1:length(ns)) {\n  # Get the sample size\n  n <- ns[i]\n  \n  # Draw a sample of size n\n  sample <- slice_head(data, n = n)\n  \n  # If this is the first iteration, run the full brms model\n  # Else update the model\n  if (i == 1) {\n    model <- brm(\n      formula = DV ~ 0 + Intercept + condition, \n      data = sample, \n      family = gaussian(), \n      prior = c(\n        set_prior(coef = \"Intercept\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"b\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"sigma\", prior = \"normal(1, 1)\")\n      )\n    )\n  } else {\n    model <- update(model, newdata = sample)\n  }\n  \n  # Get the estimated marginal means\n  emmeans <- emmeans(model, specs = pairwise ~ condition)\n  contrasts <- emmeans$contrasts\n  \n  # Get draws of the posterior of each contrast\n  draws <- gather_emmeans_draws(contrasts)\n  \n  # Add sample information\n  draws <- mutate(draws, step = i, n = n)\n  \n  # Add the draws to the results data frame\n  results <- bind_rows(\n    results, \n    draws\n  )\n}\n\n\nNow that we have a data frame that contains the posterior draws of each contrast, we can plot the posteriors as well as some summary statistics (e.g., the median, a 95% interval) for each contrast.\n\n\nCode\nfinal_quantiles <- results %>%\n  filter(n == 250) %>%\n  group_by(contrast) %>%\n  summarize(\n    final_lower = quantile(.value, .025),\n    final_upper = quantile(.value, .975)\n  ) %>%\n  pivot_longer(cols = -contrast, names_to = \"bound\", \"value\")\n\nggplot(results, aes(x = factor(n), y = .value)) +\n  stat_slabinterval() +\n  geom_hline(\n    mapping = aes(yintercept = value), \n    data = final_quantiles, \n    linetype = \"dashed\", \n    alpha = .5\n  ) +  \n  facet_wrap(~ contrast, ncol = 1, scales = \"free_y\") +\n  labs(x = \"Sample size per condition (n)\", y = \"Contrast estimate\") +\n  scale_color_viridis(option = \"mako\", discrete = TRUE)\n\n\n\n\n\nHorizontal lines show the 95% interval of the estimate from the largest sample.\n\n\n\n\n\n\nBayes factors\nBelow we run the same models but this time we calculate Bayes factors for each contrast using the hypothesis() function.\n\n\nCode\nns <- c(10, 20, 30, 40, 50, 75, 100, 150, 200, 250)\nresults <- tibble()\n\nfor (i in 1:length(ns)) {\n  # Get the sample size\n  n <- ns[i]\n  \n  # Draw a sample of size n\n  sample <- slice_head(data, n = n)\n  \n  # If this is the first iteration, run the full brms model\n  # Else update the model\n  if (i == 1) {\n    model <- brm(\n      formula = DV ~ 0 + Intercept + condition, \n      data = sample, \n      family = gaussian(), \n      prior = c(\n        set_prior(coef = \"Intercept\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"b\", prior = \"normal(0, 1)\"),\n        set_prior(class = \"sigma\", prior = \"normal(1, 1)\")\n      ), \n      sample_prior = TRUE\n    )\n  } else {\n    model <- update(model, newdata = sample)\n  }\n  \n  # Get the bayes factors for each contrast\n  BF_AB <- hypothesis(model, \"Intercept = conditionB\")\n  BF_AC <- hypothesis(model, \"Intercept = conditionC\")\n  BF_AD <- hypothesis(model, \"Intercept = conditionD\")\n  BF_BC <- hypothesis(model, \"conditionB = conditionC\")\n  BF_BD <- hypothesis(model, \"conditionB = conditionD\")\n  BF_CD <- hypothesis(model, \"conditionC = conditionD\")\n  \n  # Create a tibble with the Bayes factors and sample information\n  bayes_factors <- tibble(\n    i = i,\n    n = n,\n    `A - B` = BF_AB$hypothesis$Evid.Ratio,\n    `A - C` = BF_AC$hypothesis$Evid.Ratio,\n    `A - D` = BF_AD$hypothesis$Evid.Ratio,\n    `B - C` = BF_BC$hypothesis$Evid.Ratio,\n    `B - D` = BF_BD$hypothesis$Evid.Ratio,\n    `C - D` = BF_CD$hypothesis$Evid.Ratio\n  )\n\n  # Add the draws to the results data frame\n  results <- bind_rows(\n    results, \n    bayes_factors\n  )\n}\n\n\nNext, we plot the Bayes factors over time, for each contrast.\n\n\nCode\nresults_long <- results %>%\n  pivot_longer(cols = -c(i, n), names_to = \"contrast\", values_to = \"BF\") %>%\n  mutate(BF = if_else(BF < 1, log(BF), BF))\n\nggplot(data = results_long, mapping = aes(x = factor(n), y = BF, group = 1)) +\n  geom_line() +\n  facet_wrap(~ contrast) + \n  labs(\n    x = \"Sample size per condition (n)\", \n    y = expression(BF[\"10\"]), \n    linetype = \"Method\"\n  )"
  },
  {
    "objectID": "sections/statistics/group-differences/pairwise-comparisons.html",
    "href": "sections/statistics/group-differences/pairwise-comparisons.html",
    "title": "Pairwise comparisons",
    "section": "",
    "text": "# Load packages\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(brms)\nlibrary(emmeans)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\nThis section is about different statistical techniques to analyze group differences."
  },
  {
    "objectID": "sections/statistics/group-differences/pairwise-comparisons.html#bayesian",
    "href": "sections/statistics/group-differences/pairwise-comparisons.html#bayesian",
    "title": "Pairwise comparisons",
    "section": "Bayesian",
    "text": "Bayesian\n\nPairwise comparisons\nIn this scenario we simulate data from a study with 5 different groups. The conditions differ from each by a small amount and for simplicity’s sake each condition has a standard deviation of 1. The sample size per condition is 250.\n\n# Set the simulation parameters\nMs <- c(0, 0.1, 0.2, 0.3, 0.4)\nSDs <- 1\nn <- 250\n\n# Produce the variance-covariance matrix\nSigma <- matrix(\n  nrow = length(Ms), \n  ncol = length(Ms), \n  data = c(\n    SDs^2, 0, 0, 0, 0,\n    0, SDs^2, 0, 0, 0,\n    0, 0, SDs^2, 0, 0,\n    0, 0, 0, SDs^2, 0,\n    0, 0, 0, 0, SDs^2\n  )\n)\n\n# Simulate the values\nm <- mvrnorm(n = n, mu = Ms, Sigma = Sigma, empirical = TRUE)\n\n# Prepare the data by converting it to a data frame and making it tidy\ncolnames(m) <- c(\"A\", \"B\", \"C\", \"D\", \"E\")\n\ndata <- as_tibble(m)\n\ndata <- pivot_longer(\n  data = data, \n  cols = everything(), \n  names_to = \"condition\", \n  values_to = \"DV\"\n)\n\ndata <- mutate(data, id = 1:n(), .before = condition)\n\nTo perform the pairwise comparisons we first fit a model with brms. If we also want to calculate Bayes factors, we need to set a prior for the intercept. For technical reasons, this needs to be done by explicitly including the intercept in the formula. After that we need to set 3 priors: 1 for the intercept, 1 for all the other coefficients, and one for sigma. We’ll set some weak priors because we don’t have any additional information about this simulated data.\n\nmodel <- brm(\n  formula = DV ~ 0 + Intercept + condition, \n  data = data, \n  family = gaussian(), \n  prior = c(\n    set_prior(coef = \"Intercept\", prior = \"normal(0, 1)\"),\n    set_prior(class = \"b\", prior = \"normal(0, 1)\"),\n    set_prior(class = \"sigma\", prior = \"normal(1, 1)\")\n  ), \n  sample_prior = TRUE\n)\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include <complex>\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '26506c9e129ffc36bd1658ecb9d890c1' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.066074 seconds (Warm-up)\nChain 1:                0.057278 seconds (Sampling)\nChain 1:                0.123352 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '26506c9e129ffc36bd1658ecb9d890c1' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.066055 seconds (Warm-up)\nChain 2:                0.066104 seconds (Sampling)\nChain 2:                0.132159 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '26506c9e129ffc36bd1658ecb9d890c1' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.06382 seconds (Warm-up)\nChain 3:                0.067943 seconds (Sampling)\nChain 3:                0.131763 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '26506c9e129ffc36bd1658ecb9d890c1' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.065475 seconds (Warm-up)\nChain 4:                0.063031 seconds (Sampling)\nChain 4:                0.128506 seconds (Total)\nChain 4: \n\nmodel\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: DV ~ 0 + Intercept + condition \n   Data: data (Number of observations: 1250) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      0.00      0.06    -0.12     0.12 1.00     1032     1632\nconditionB     0.10      0.09    -0.07     0.27 1.00     1510     2073\nconditionC     0.20      0.09     0.02     0.36 1.00     1514     2436\nconditionD     0.30      0.09     0.13     0.47 1.00     1312     2129\nconditionE     0.40      0.09     0.22     0.57 1.00     1502     2179\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.00      0.02     0.96     1.04 1.00     3775     2982\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimates range, as expected, from 0 for the intercept to 0.40 for condition E.\nIf we want pairwise comparisons, we can use the emmeans package to obtain them. We use the emmeans() function and set the specs argument to pairwise ~ condition. pairwise is a reserved term to use for exactly this purpose. The result is an object that contains estimated marginal means and contrasts. Since we’re interested in the pairwise comparisons we only print the contrasts.\n\nemmeans <- emmeans(model, specs = pairwise ~ condition)\nemmeans$contrasts\n\n contrast estimate lower.HPD upper.HPD\n A - B     -0.1000    -0.275    0.0668\n A - C     -0.1961    -0.362   -0.0257\n A - D     -0.2941    -0.481   -0.1446\n A - E     -0.3961    -0.578   -0.2250\n B - C     -0.0990    -0.270    0.0713\n B - D     -0.1977    -0.384   -0.0283\n B - E     -0.2967    -0.475   -0.1243\n C - D     -0.0979    -0.273    0.0690\n C - E     -0.1979    -0.377   -0.0314\n D - E     -0.0985    -0.272    0.0758\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\nThis gives us the estimates as well as lower and upper bounds of a highest probability density intervals. We can also plot them using the following code.\n\ncontrasts <- as_tibble(emmeans$contrasts)\n\nggplot(contrasts, aes(x = contrast, y = estimate)) +\n  geom_pointrange(aes(ymin = lower.HPD, ymax = upper.HPD)) +\n  labs(x = \"Contrast\", y = \"Estimate with 95% HPD\")\n\n\n\n\nPairwise comparisons via emmeans\n\n\n\n\nAlternatively, we can also calculate specific contrasts using the hypothesis() function from brms. The added value of calculating contrasts this way is that it also provides us with a Bayes factor if we set priors for all parts of the model.\nFor example, we can get the contrast between condition A and B by subtracting the Intercept from the condition B coefficient. We can then get an evidence ratio for the test that this value is larger than 0. This value is simply the ratio of the number of samples larger (or smaller) than a value to the number of samples smaller (or larger) than the value.\n\ncontrast_A_B <- hypothesis(model, \"conditionB - Intercept > 0\")\ncontrast_A_B\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (conditionB-Inter... > 0     0.09      0.14    -0.13     0.33          3\n  Post.Prob Star\n1      0.75     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n# sum(contrast_A_B$samples$H1 > 0) / sum(contrast_A_B$samples$H1 < 0)\n\nThis gives us an estimate of 0.1 (as expected) and an evidence ratio of 3.004004.\nWe can also test whether this contrast is equal to 0. This is a Bayes factor computed via the Savage-Dickey density ratio method. That is, the posterior density at a point of interest is divided by the prior density at the same point.\n\ncontrast_A_B_null <- hypothesis(model, \"conditionB - Intercept = 0\")\ncontrast_A_B_null\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (conditionB-Inter... = 0     0.09      0.14    -0.17     0.37       8.52\n  Post.Prob Star\n1       0.9     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThis gives us a Bayes factor of 8.5246971.\nAlternatively, we can compare another contrast, say, D vs. B. We can get this contrast by subtracting the coefficient for condition B from the coefficient for condition D.\n\ncontrast_D_B <- hypothesis(model, \"conditionD - conditionB > 0\")\ncontrast_D_B\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (conditionD-condi... > 0      0.2      0.09     0.05     0.35      82.33\n  Post.Prob Star\n1      0.99    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nAs expected, we see an estimate of 0.2 (0.4 - 0.2). We also see an evidence ratio of 82.3333333 for the hypothesis that this is larger than 0."
  },
  {
    "objectID": "sections/surveys/recommended-reading.html",
    "href": "sections/surveys/recommended-reading.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "References\n\nGehlbach, H., & Brinkworth, M. E. (2011). Measure twice, cut down error: A process for enhancing the validity of survey scales. Review of General Psychology, 15(4), 380–387. https://doi.org/10.1037/a0025704\n\n\nWolf, M. G., Ihm, E., Maul, A., & Taves, A. (2019). Survey item validation. https://doi.org/10.31234/osf.io/k27w3"
  },
  {
    "objectID": "sections/surveys/response-options.html",
    "href": "sections/surveys/response-options.html",
    "title": "Response Options",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is still a work in progress.\nThere are several decisions to make that involve response options. How many response options should you use? Should you use an even or odd number of response options? Should you label them? This section contains a summary of best practices that one can use to address these questions."
  },
  {
    "objectID": "sections/surveys/response-options.html#number-of-response-options",
    "href": "sections/surveys/response-options.html#number-of-response-options",
    "title": "Response Options",
    "section": "Number of Response Options",
    "text": "Number of Response Options\nThe question of how many response options to use centers around two main concerns. The first is that more options means you can obtain a more fine-grained assessment of the characteristic that is being evaluated (e.g., attitude). In other words, your assessment is more precise. However, the question is how the number of options affects the reliability and the validity of the measurement. With more options, it becomes more difficult for people to distinguish between the different options (e.g., is “Strongly agree” reliably different from “Very strongly agree”?).\nTable 1 shows an overview of various studies in which the topic of response options was addressed. The studies vary in many ways, so the final conclusion should be a holistic interpretation of the results, rather than a simple tallying of the results. Note also that only empirical studies are included and not simulation studies. Simulation studies seem limited because they cannot address the plausible psychological limitation of people being unable to distinguish between many options.\n\nTable 1: Overview of empirical studies on the topic of response options.\n\n\n\n\n\n\n\nSource\nComparisons\nConclusion\n\n\n\n\nDonnellan & Rakhshani (2020)\n2- to 7-, and 11-point Likert\n5-point Likert or higher\n\n\nSimms et al. (2019)\n2- to 11-point Likert + VAS\n6-point Likert\n\n\nSung & Wu (2018)\n5-point Likert and VAS-RRP\nVAS-RPP\n\n\nCox et al. (2017)\n2- and 4-point Likert\nMixed\n\n\nLewis (2017)\n7-, 11-point Likert and VAS\nNo difference\n\n\nKuhlmann et al. (2017)\n5-point Likert and VAS\nNo difference\n\n\nHilbert (2016)\n2- and 5-point and VAS\nIt depends\n\n\nCapik & Gozum (2015)\n2- and 5-point Likert\nNo difference\n\n\nEutsler & Lang (2015)\n5-, 7-, 9-, and 11-point Likert\n7-point Likert\n\n\nFinn et al. (2015)\n2- and 4-point Likert\n4-point Likert\n\n\nRevilla et al. (2014)\n5-, 7-, 11-point Likert\n5-point Likert\n\n\nCox et al. (2012)\n2- and 4-point Likert\n4-point Likert\n\n\nJanhunen (2012)\n7-point Likert and 30-point VAR\nVAR\n\n\nDawes (2008)\n5-, 7-, and 10-point Likert\nNo difference\n\n\nWeng (2004)\n3- to 9-point Likert\n5-point or higher\n\n\nPreston & Colman (2000)\n2- to 11-point Likert and VAS\n7-, 9-, or 10-point Likert\n\n\nAlwin (1997)\n7- and 11-point Likert\n11-point Likert\n\n\nJaeschke et al. (1990)\n7-point Likert and VAS\nNo difference\n(slightly favor 7-point Likert)\n\n\nFlamer (1983)\n2- and 9-point Likert\n9-point Likert\n\n\nMatell & Jacoby (1971)\n2-point to 18-point Likert\nNo difference\n\n\nBendig (1954)\n2-, 3-, 5-, 7-, and 9-point Likert\nNo difference\n(maybe 3-point or higher)\n\n\n\nThere are also several review papers on the topic. Krosnick & Presser (2010) suggest that 7-point Likert scales are probably optimal. Lietz (2010) concludes a desirable Likert-scale consists of 5 to 8 response options. Similarly, Cox III (1980) recommends to use between 5 and 9 response options. Symonds (1924), in 1924, claims the optimum number is 7. Gehlbach & Brinkworth (2011) recommends using 5-points for unipolar items and 7-point for biopolar items.\nBesides psychometric properties it may also be worth taking into account respondent preference. This involves ease of use of the scale and whether the response options allow for sufficient variation for respondents to express their view. Preston & Colman (2000) found that respondents found scales with 5, 7, and 10 points easy to use (compared to fewer options and a VAS) and that they preferred scales with more response options to allow them to express themselves (7 or more). Other studies also show that respondents favor more options (Cox et al., 2017).\nNote that if time is of the essence, fewer response options are preferred.\nAnother relevant factor is whether the scale is bipolar or unipolar. Bipolar scales are symmetrical which means the number of options naturally increase as they need to match both sides of the spectrum. Unipolar items are only about one side, usually ranging from the absence of something to the presence of something (to a certain degree). Since it is harder to label a larger number of options for a unipolar scale, the number of options are likely to be smaller.\nConclusion: It appears that few response options (2 or 3) should definitely be avoided. More response options therefore seems better, but benefits seem to quickly level off. Given other concerns, such as ease of use and interpretability, a 7-point Likert scale seems to be preferred for bipolar scales and a 5-point Likert scale for unipolar scales."
  },
  {
    "objectID": "sections/surveys/response-options.html#odd-vs.-even-response-options",
    "href": "sections/surveys/response-options.html#odd-vs.-even-response-options",
    "title": "Response Options",
    "section": "Odd vs. Even Response Options",
    "text": "Odd vs. Even Response Options\nThe middle option of a scale can have an ambiguous meaning. Participants may use it to indicate a moderate standing on the issue (Rugg and Cantril, 1944), a lack of an opinion (Nadler, Weston, and Voyles, 2014), ambivalence (Klopfer and Madden, 1980; Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), indifference (Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), uncertainty (Baka, Figgou, and Triga, 2012; Nadler, Weston, and Voyles, 2014), confusion, or to signal context dependence (e.g., “it depends” or disputing the question, see Baka, Figgou, and Triga, 2012).\nThe middle option may also be used for certain response styles, such as socially desirable responding (Sturgis, Roberts, and Smith, 2012) or satisficing (Krosnick, 1991), although there is not much research showing it actually leads to satisficing Wang & Krosnick (2020).\nAf a middle alternative is explicitly oﬀered, the proportion endorsing it increases dramatically (e.g. Ayidiya & McClendon, 1990; Bishop, 1987; Bishop, Hippler, Schwarz, & Strack, 1988; Kalton, Collins, & Brook, 1978; Kalton, Roberts, & Holt, 1980; Rugg & Cantril, 1944).\nSome studies show that not including a middle option decreases validity and increases measurement error (O’Muircheartaigh, Krosnick, and Helic, 1999; Kahn, and Dhar, 2002)\nRecent study on this: Wang & Krosnick (2020)\nAn alternative approach to this issue is to use branching. Respondents could first be asked whether they fall at the midpoint or on one side, followed by a question about their extremity on a side. This approach was found to be more reliable and valid than using a 7-point scale (Krosnick and Berent, 1993; Malhotra, Krosnick, and Thomas, 2009).\nConclusion: If it is possible that respondents may have a moderate view, it seems crucial for it to be possible to capture this view. Limitations of a middle option could then be addressed in other ways (e.g., clear questions)."
  },
  {
    "objectID": "sections/surveys/response-options.html#response-option-labeling",
    "href": "sections/surveys/response-options.html#response-option-labeling",
    "title": "Response Options",
    "section": "Response Option Labeling",
    "text": "Response Option Labeling\nThere are several studies that show all response options should be labelled, rather than only labeling the end points (Krosnick & Berent, 1993; Weng, 2004).\nFor an example of biopolar labels for a 2- to 11-point Likert scale, see Table 1.\n\n\nTable 1: Likert response labels from Simms et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabel\n2-point\n3-point\n4-point\n5-point\n6-point\n7-point\n8-point\n9-point\n10-point\n11-point\n\n\n\n\nVery strongly disagree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\nStrongly disagree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nDisagree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nMostly disagree\n\n\n\n\n\n\n\n\nx\nx\n\n\nSlightly disagree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nNeither agree nor disagree\n\nx\n\nx\n\nx\n\nx\n\nx\n\n\nSlightly agree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nMostly agree\n\n\n\n\n\n\n\n\nx\nx\n\n\nAgree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nStrongly agree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nVery strongly agree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\n\n\nIt is also recommended to avoid agree-disagree response labels because asking respondents to rate their level of agreement is a cognitively demanding task that increases respondent error and reduces responding effort (Gehlbach & Brinkworth, 2011).\nAgreement: Strongly agree, Moderately agree, Neither agree nor disagree, Moderately disagree, Strongly disagree (another version removes the “moderately” qualifier and/or uses “neutral”)\nComparison: Much X, Slightly X, About the same, Slightly (opposite of X), Much (opposite of X)\nEase: Very easy, Moderately easy, Neither easy nor difficult, Moderately difficult, Very difficult\nExpectations: Exceeds expectations, Fully meets expectations, Does not fully meet expectations, Does not meet expectations at all\nExtent (5 pt): A great deal (Completely, if appropriate), Considerably, Moderately, Slightly, Not at all\nExtent (4 pt): Significantly, Moderately, Slightly, Not at all\nFrequency (no set time): Always, Often, Occasionally, Rarely, Never\nFrequency (general): Daily, Weekly, Monthly, Once a semester, Once a year, Never\nFrequency (based on time frame): More than 5 times, 4 - 5 times, 2 - 3 times, 1 time, Less than 1 time, Never\nFrequency (extended): More than once a week, Once a week, Once a month, Once a semester, Once a year, Less than once a year, Never\nHelpfulness: Extremely helpful, Very helpful, Moderately helpful, Slightly helpful, Not at all helpful\nImportance: Extremely important, Very important, Moderately important, Slightly important, Not at all important\nInterest: Extremely interested, Very interested, Moderately interested, Slightly interested, Not at all interested\nLikelihood: Very likely, Moderately likely, Neither likely nor unlikely, Moderately unlikely, Very unlikely\nNumeric Scales: Less than #, About the same, More than #\nProbability: Definitely would, Probably would, Probably wouldn’t, Definitely wouldn’t\nProficiency: Beginner, Developing, Competent, Advanced, Expert (typical for Rubrics)\nQuality: Excellent, Good, Average, Below average, Poor\nSatisfaction: Very satisfied, Moderately satisfied, Neither satisfied nor dissatisfied, Moderately dissatisfied, Very dissatisfied (another version removes the “moderately” qualifier and/or uses “neutral”)\nTaken from https://baselinesupport.campuslabs.com/hc/en-us/articles/204305485-Recommended-Scales"
  },
  {
    "objectID": "sections/surveys/index.html",
    "href": "sections/surveys/index.html",
    "title": "Overview",
    "section": "",
    "text": "This chapter is about best practices and common issues in designing surveys.\nAs suggested in some review papers Simms (2008), the construction of a new survey or scale consists of several steps:\nI will try to cover as many of these topics as possible. Currently, there is a chapter on response options and one containing some recommended readings. There is also a Statistics section on this website that covers factor analyses."
  },
  {
    "objectID": "sections/surveys/item-development.html",
    "href": "sections/surveys/item-development.html",
    "title": "Item Development",
    "section": "",
    "text": "This step is an important step in the development of a scale as serious problems with the item pool will reverberate through all subsequent data analyses and scale construction efforts.\nitems should be written that are (i) relevant to the constructs to be measured, and (ii) representative of all potentially important aspects of the target construct. Having formal construct definitions is particularly important here, as such definitions should guide the item writing process.\nBesides including items to cover all the different facets of a particular construct, it’s also important that the item pool includes items reflecting all levels of the construct.\nItem writing guidelines (Simms, 2008):"
  },
  {
    "objectID": "sections/surveys/item-development.html#pilot-testing",
    "href": "sections/surveys/item-development.html#pilot-testing",
    "title": "Item Development",
    "section": "Pilot testing",
    "text": "Pilot testing\nAfter the initial item pool is complete, it makes sense to pilot test the items before running a large-scaled exploratory study.\nFactor loadings can be improved by using multiple response (Likert-type) items, as they generally result in higher loadings than two-choice items (Comrey & Montag, 1982; Oswald & Velicer, 1980; Velicer, DiClemente, & Corriveau, 1984; Velicer, Govia, Cherico, & Corriveau, 1985; Velicer & Stevenson, 1978). Likewise, the quality of item writing can affect the size of the loadings, that is, the expression of an item in simple language, restricting the item to a single idea, or using content that is appropriate to a majority of respondents are all ways of improving items."
  },
  {
    "objectID": "sections/WIP/item-development.html",
    "href": "sections/WIP/item-development.html",
    "title": "Item Development",
    "section": "",
    "text": "Reverse-scored or reverse worded items can be included to determine whether participants are paying attention and don’t just select the same response on each item. However, there is some evidence that reverse-scored items reduce the reliability of the scale or produce an unexpected factor structure [@swain2008].\nAnother important consideration is that reverse-worded items can affect the model fit. Factor analyses of scales with some RW items frequently indicate the presence of method covariance obscuring or confounding substantive covariance (e.g., Brown, 2003; Roszkowski & Soven, 2010)."
  },
  {
    "objectID": "sections/WIP/item-development.html#number-of-items",
    "href": "sections/WIP/item-development.html#number-of-items",
    "title": "Item Development",
    "section": "Number of items",
    "text": "Number of items\nThere are no hard-and-fast rules guiding this decision, but keeping a measure short is an effective means of minimizing response biases caused by boredom or fatigue (Schmitt & Stults, 1985; Schriesheim & Eisenbach, 1990). Additional items also demand more time in both the development and administration of a measure (Carmines & Zeller, 1979). Harvey, Billings, and Nilan (1985) suggest that at least four items per scale are needed to test the homogeneity of items within each latent construct. Adequate internal consistency reliabilities can be obtained with as few as three items (Cook et al., 1981), and adding items indefinitely makes progressively less impact on scale reliability (Carmines & Zeller, 1979). It is difficult to improve on the internal consistency reliabilities of five appropriate items by adding items to a scale (Hinkin, 1985; Hinkin & Schriesheim, 1989; Schriesheim & Hinkin, 1990). Cortina (1993) found that scales with many\nitems may have high internal consistency reliabilities even if item intercorrelations are low, an argument in favor of shorter scales with high internal consistency. It is also important to assure that the domain has been adequately sampled, as inadequate sampling is a primary source of measurement error (Churchill, 1979). As Thurstone (1947) points out, scales should possess simple structure, or parsimony. Not only should any one measure have the simplest possible factor constitution, but any scale should require the contribution of a minimum number of items that adequately tap the domain of interest. These findings would suggest that the eventual goal will be the retention of four to six items for most constructs, but the final determination must be made only with accumulated evidence in support of the construct validity of the measure. It should be anticipated that approximately one half of the created items will be retained for use in the final scales, so at least twice as many items as will be needed in the final scales should be generated to be administered in a survey questionnaire.\nhttps://twitter.com/dingding_peng/status/1481683536499331079\nhttps://psyarxiv.com/4kra2/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a website to organize my conclusions about topics in methodology and statistics.\nThe website is divided up into multiple sections. At this moment, there is a section on survey design and a section on statistics. The survey design section is about various issues pertaining to designing surveys, such as how to create items, what kind of response options to use, and so on. The statistics section is currently only about factor analyses, including both exploratory and confirmatory factor analyses.\nMore sections will be added in the future."
  },
  {
    "objectID": "index.html#word-of-caution",
    "href": "index.html#word-of-caution",
    "title": "About",
    "section": "Word of Caution",
    "text": "Word of Caution\nSome sections in the book are not yet finished. They’re work in progress. Sometimes I think they’re already a bit useful, so I make them public. If I think they’re not finished/polished yet, I put a warning at the start of the page to indicate that.\nHaving said that, this website will forever be a work in progress because the content is about current best practices. As best practices are likely to change over time, so the content of this website will change with it. If I realize that something is heavily outdated, I will make a note of it.\nIt’s also very likely that there are mistakes. These mistakes can theoretically range from gross errors to simple typos, or a reliance on outdated information. If you find a mistake, please contact me or click on the GitHub link on each page and create an Issue."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe following people have contributed to this book:\n\nWillem Sleegers\n\nIf I’ve missed your contributions and you deserve to be on this list, please don’t hesitate to contact me or add yourself via a Pull Request on GitHub!"
  }
]