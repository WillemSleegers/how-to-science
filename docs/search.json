[
  {
    "objectID": "sections/statistics/index.html",
    "href": "sections/statistics/index.html",
    "title": "Overview",
    "section": "",
    "text": "This section is about statistics-related topics. At the moment there are two chapters. One of exploratory factor analyses and one of confirmatory factor analyses."
  },
  {
    "objectID": "sections/statistics/factor-analysis/examples.html",
    "href": "sections/statistics/factor-analysis/examples.html",
    "title": "Examples",
    "section": "",
    "text": "This page lists several examples of papers in which EFA and/or CFA was used to construct a psychological scale.\n\n\n\n\nKahane, G., Everett, J. A. C., Earp, B. D., Caviola, L., Faber, N. S., Crockett, M. J., & Savulescu, J. (2018). Beyond sacrificial harm: A two-dimensional model of utilitarian psychology. Psychological Review, 125(2), 131–164. https://doi.org/10.1037/rev0000093"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#preparation",
    "href": "sections/statistics/factor-analysis/EFA.html#preparation",
    "title": "Exploratory Factor Analysis",
    "section": "Preparation",
    "text": "Preparation\nBefore you perform a factor analysis, make sure that the design of your study is suitable for this type of analysis. This means you should have an appropriate sample, sample size, and indicators (items).\n\nSample\nIt may be obvious, but make sure to target participants that are likely to vary in the attitude that you’re interested in measuring and that the sample is representative of the population that you’re interested in.\n\n\nNumber of items\nAt least three items per factor are required for a factor model to be identified; more items per factor results in overidentification of the model. A number of writers recommend that four or more items per factor be included in the factor analysis to ensure an adequate identification of the factors (Comrey & Lee, 1992; Fabrigar et al., 1999; Gorsuch, 1988). Fabrigar et al. (1999) suggest to include at least 4 and perhaps as many as six for each factor.\nMacCallum et al. (1999) found that in addition to the communality of the measures, the results were more accurate for given sample sizes if there were more measures per factor included in the analysis. Within the range of indicators studied (three to seven per factor), it is better to have more indicators than fewer. The critical point, however, is that these indicators must be reasonably valid and reliable.\nVelicer & Fava (1998) suggest 6-10 initial variables per factor is recommended as 25% to 50% will not perform as expected and the end goal should be to have four- or five to-one as a minimum. They say that the bare minimum of three variables per factor is a dangerous goal. They also write that if loading is low, a ratio of 10 or 20 to 1 may be required for model identification.\nFactor loadings can be improved by using multiple response (Likert-type) items, as they generally result in higher loadings than two-choice items (Comrey & Montag, 1982; Oswald & Velicer, 1980; Velicer, DiClemente, & Corriveau, 1984; Velicer, Govia, Cherico, & Corriveau, 1985; Velicer & Stevenson, 1978). Likewise, the quality of item writing can affect the size of the loadings, that is, the expression of an item in simple language, restricting the item to a single idea, or using content that is appropriate to a majority of respondents are all ways of improving items. Item quality can also be improved by using item parcels, that is, the total of several items, instead of individual items.\nSome additional arguments for why multiple indicators per factor are useful:\n\nPeople read items incorrectly or sometimes just don’t understand an item (despite all efforts to make them as clear as possible), so having a sample of items can counteract the idiosyncratic mistakes of respondents\nWith complicated topics, having multiple items also adds clarity about the overall goal of the scale. Each item is an indicator to the participant about what the purpose of the scale is.\n\n\n\nSample size\nRules of thumb are one way to determine an appropriate sample size, but the general consensus is that rules of thumb are a bad idea [MacCallum et al. (1999), Costello & Osborne (2005)]. The reason for that is that rules of thumb only involve one of two factors that are relevant for determining the sample size: a set number of participants or a particular participant-to-item ratio. The reality is that the appropriate sample size is dependent on many more factors, such as the:\n\nStudy design (e.g., cross-sectional vs. longitudinal)\nSize of the relationships among the indicators (e.g., high communalities without cross loadings; strong loadings)\nReliability of the indicators\nScaling (e.g., categorical, continuous) and distribution of the indicators\nEstimator type (e.g., ML, robust ML, WLSMV)\nAmount and patterns of missing data\nSize of the model (model complexity)\n\nFor example, MacCallum et al. (1999) found that results were consistent with population loadings with sample sizes as low as 60 if the communalities of the items were high (.60 or greater).\nFabrigar et al., 1999; MacCallum, Widaman, Preacher, & Hong, 2001; Mundfrom & Shaw, 2005; Velicer & Fava, 1998).\nCostello & Osborne (2005) found that sample sizes in two years’ worth of PsychINFO articles were relatively low. The majority of the studies in their survey (62.9%) had subject to item ratios of 10:1 or less. They also found that a ratio of 20:1 produces error rates well above the standard alpha (5%) level. This is based on a study in which different rules of thumb were used to analyze the factor structure of the Marsh’s Self-Description Questionnaire. As this questionnaire might be more typical of psychological scales (at least more typical than many simulated data sets), it shows that sample sizes should be larger rather than smaller. They caution researchers to remember that EFA is a large-sample procedure and that generalizable or replicable results are unlikely if the sample is too small.\nMundfrom et al. (2005) performed a simulation study to determine the minimum sample size for a study, taking into account the number of factors, the ratio of items to factors, and the level of communality.\n\nRecommended minimum sample sizes by Mundfrom et al. (2005)\n\n\nCriterion\nCommunality\np/f\nn\n\n\n\n\nExcellent\nHigh\n4\n500\n\n\n\n\n6\n250\n\n\n\n\n8\n100\n\n\n\nWide\n4\n900\n\n\n\n\n6\n200\n\n\n\n\n8\n130\n\n\n\nLow\n4\n1400\n\n\n\n\n6\n260\n\n\n\n\n8\n130\n\n\nGood\nHigh\n5\n130\n\n\n\n\n7\n55\n\n\n\nWide\n5\n140\n\n\n\n\n8\n60\n\n\n\nLow\n5\n200\n\n\n\n\n8\n80"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#data-analysis",
    "href": "sections/statistics/factor-analysis/EFA.html#data-analysis",
    "title": "Exploratory Factor Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nRelevant items\nOne design issue that is especially important is measured variables to include in the study. If measured variables irrelevant to the domain of interest are included, then spurious common factors might emerge or true common factors might be obscured (Fabrigar et al., 1999).\n\n\nSampling Adequacy\nThe Kaiser–Meyer–Olkin (KMO) test is a statistical measure to determine how suitable the data is for factor analysis. The test measures sampling adequacy for the complete model and each variable in the model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the higher the KMO value, the more suitable the data is.\nThe KMO value ranges from 0 to 1, with 0.60 considered suitable for factor analysis Tabachnick & Fidell (2013). Kaiser himself (1974) suggested that KMO > .9 were marvelous, in the .80s, meritorious, in the .70s, middling, in the .60s, mediocre, in the 50s, miserable, and less than .5, unacceptable.\nBartlett’s (1954) test of sphericity is a notoriously sensitive test of the hypothesis that the correlations in a correlation matrix are zero. Because of its sensitivity and its dependence on N, the test is likely to be significant with samples of substantial size even if correlations are very low. Therefore, use of the test is recommended only if there are fewer than, say, five cases per variable (Tabachnick & Fidell, 2013).\n\n\nPrincipal Components vs. Factor Analysis\nAn important consideration is whether to use a principal components analysis (PCA) or a factor analysis. PCA is a data reduction technique and while a factor analysis is also a type of data reduction technique, the focus with PCA is more on simply reducing the data, without regard for a theoretical interpretation. With PCA, the variables themselves are of interest, rather than a hypothetical latent construct. Constructs are conceptualized as being causally determined by the observations; that is, CFA reflects a formative model rather than a reflective one (Edwards & Bagozzi, 2000). If you are a psychologist trying to create a measure of a psychological construct, this is probaly not what you want as principal component scores are “caused” by their indicators in much the same way that sum scores are “caused” by item scores.” [Borsboom (2006), p. 426]. Instead, you likely want the opposite causal relationship in which a latent factor causes the indicator scores.\n\n\nCode\nDiagrammeR::grViz(\"\n  digraph dot {\n    graph [\n      layout = dot, \n      rankdir = LR, \n      fontname = 'Source Sans Pro', \n      label = 'Illustration of a PCA model (left) and a factor analysis model (right).'\n    ]\n    \n    node [fontname = 'Source Sans Pro']\n    node [shape = square]\n    var1 [label = 'item 1']\n    var2 [label = 'item 2']\n    var3 [label = 'item 3']\n    var4 [label = 'item 4']\n    var5 [label = 'item 1']\n    var6 [label = 'item 2']  \n    var7 [label = 'item 3']  \n    var8 [label = 'item 4']\n  \n    node [shape = circle]\n    construct_a [label = 'construct A']\n    construct_b [label = 'construct B']\n  \n    edge [color = black, minlen = 3]\n    {var1 var2 var3 var4} -> construct_a\n    construct_b -> {var5 var6 var7 var8}\n  \n    edge[style = invis, minlen = 1];\n    construct_a -> construct_b\n  }\"\n)\n\n\n\n\n\n\nA related issue is that factor analysis assumes that the total variance can be partitioned into common and unique variance and PCA assumes that the common variances takes up all of total variance. This means that PCA assumes that all variables are measured without error. It is usually more reasonable to assume that you have not measured your set of items perfectly.\nAlthough some argue that the two methods have the same results (Velicer & Jackson, 1990), there is evidence that those similarities are mistaken and that factor analysis has better results than PCA (Widaman, 1993).\nConclusion: Do not use PCA.\n\n\nFactor Extraction Method\nExtraction is the general term for the process of reducing the number of dimensions being analyzed from the number of variables in the data set (and matrix of associations) into a smaller number of factors.\nThere are multiple factor extract methods, such as:\n\nminres\nunweighted least squares (ULS)\ngeneralized least squares (GLS)\nmaximum likelihood\nprincipal axis factor(ing)\nalpha factor(ing)\nimage factor(ing)\n\nIt’s not entirely clear which factor extraction method is the best and some authors use different terms for some of the methods, making it more difficult to compare them.\nFabrigar, Wegener, MacCallum and Strahan -Fabrigar et al. (1999) argued that if data are relatively normally distributed, maximum likelihood is the best choice because “it allows for the computation of a wide range of indexes of the goodness of fit of the model and permits statistical significance testing of factor loadings and correlations among factors and the computation of confidence intervals.” (p. 277). In case the data is not generally normally distributed, they recommend principal axis factoring. This is recommended in several sources (Costello & Osborne (2005); Osborne (2014)).\nConclusion: Use the maximum likelihood method if the data is generally normally distributed and to use principal axis factoring if the data is non-normal.\n\n\nRotation methods\nThe goal of rotation is to clarify the factor structure and make the results of the EFA more interpretable.\nRotation methods can be categorized into one of two categories: orthogonal or oblique. Orthogonal rotations keep axes at a 90 degree angle, forcing the factors to be uncorrelated. Oblique rotations allow angles that are not 90 degree , thus allowing factors to be correlated if that is optimal for the solution.\nOrthogonal rotation methods include:\n\nvarimax\nquartimax\nequamax\n\nOblique rotation methods include:\n\ndirect oblimin\nquartimin\npromax\n\nIn the social sciences, we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution. If the factors are truly uncorrelated, orthogonal and oblique rotation produce nearly identical results. Since oblique rotation will reproduce an orthogonal solution but not vice versa, it makes sense to go for oblique rotation.\nThere is no widely preferred method of oblique rotation; all tend to produce similar results (Fabrigar et al., 1999).\nConclusion: Use any oblique rotation.\n\n\nNumber of factors\nThere are several methods to determine how many factors to retain:\n\nTheory\nKaiser criterion (eigenvalues greater than 1; Kaiser, 1960)\nScree test (Cattell, 1966)\nParallel analysis (Horn, 1965)\nVelicer’s multiple average partial (MAP) procedure (Velicer, 1976)\nAkaike information criterion (AIC; Akaike, 1974)\nBayesian information criterion (BIC; Schwarz, 1978)\nComparison data (CD; Ruscio & Brendan Roche (2012))\nVery Simple Structure (VSS)\nLikelihood ratio statistic\n\nIf the theoretical framework for the instrument is sound, we should start with the expectation that we should see that structure in the data. Instruments are rarely perfect (especially the first time it is examined), and theoretical expectations are not always supported. But unless one is on a fishing expedition in a data set with no a priori ideas about how the analysis should turn out, 12 this is as good a place as any to start.\nThe default in most statistical software packages is to use the Kaiser criterion. It makes some sense, as an eigenvalue represents the sum of the squared factor loadings in a column, and to get a sum of 1.0 or more, one must have rather large factor loadings to square and sum. However, this is easily achieved with more items and there are now alternative methods. Hence, there is broad consensus in the literature that this is among the least accurate methods for selecting the number of factors to retain (Velicer & Jackson, 1990).\nThe scree test involves examining the graph of the eigenvalues (available via every software package) and looking for the natural bend or “elbow” in the data where the slope of the curve changes (flattens) markedly. Although the scree plot itself is not considered sufficient to determine how many factors should be extracted (Velicer et al., 2000), many suggest that researchers examine solutions extracting the number of factors ranging from one to two factors above the elbow to one or two below.\nParallel analysis involves generating random uncorrelated data, and comparing eigenvalues from the EFA to those eigenvalues from those random data. Using this process, only factors with eigenvalues that are significantly above the mean (or preferably, the 95 th percentile) of those random eigenvalues should be retained. Several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).\nThe Minimum Average Partial (MAP) criterion involves partialing out common variance as each successive component is created. As each successive component is partialed out, common variance will decrease to a minimum. Velicer argued that minimum point should be considered the criterion for the number of factors to extract.\nVSS involves degrading the initial rotated factor solution by assuming that the nonsalient loadings are zero, even though in actuality they rarely are. What VSS does is test how well the factor matrix we think abaut and talk about actually fits the correlation matrix. It is not a confirmatory procedure for testing the significance of a particular loading, but rather it is an exploratory procedure for testing the relative utility of interpreting the correlation matrix in terms of a family of increasingly more complex factor models. The simplest model tasted by VSS is that each item is of complexity one, and that all items are embedded in a more complex factor matrix of rank k. This is the model most appropriate for scale construction and is the one we use most frequently when we talk about factor solutions. More complicated models may also be evaluated by VSS.\nZwick and Velicer (1986) tested the scree test, Horn’s parallel test, and Velicer’s MAP test (among others) in simulation studies using a data set with a clear factor structure. Both the parallel test and MAP test seemed to work well.\nRuscio & Brendan Roche (2012) notes that PA is considered to be the method of choice among methodologists and recommend that researchers take advantage of PA as a starting point, perhaps supplemented by CD. They note that researchers can also use more than one method. Similarly, several prominent authors and journals have endorsed this as the most robust and accurate process for determining the number of factors to extract (Ledesma & Valero-Mora, 2007; Velicer et al., 2000).\nOsborne (2014) notes that MAP has been considered superior to the “classic” criteria, and probably is superior to parallel analysis, although neither is perfect, and all must be used in the context of a search for conceptually interpretable factors. He recommends to use parallel analysis or MAP criteria, along with theory (and any of the classic criteria that suits you and is defensible).\nFinally, empirical research suggests that overfactoring introduces much less error to factor loading estimatesihan underfactoring (Fava & Velicer. 1992; Wood et..;!., 1996). Although be wary of Solutions with too many factors mightprompt a researcher to postulate the existence of cons tracts with little theoretical value and thereby develop unnecessarily complex theories.\nConclusion: Use multiple criteria, including theory, to make a judgment call about how many factors to extract. When in doubt, favor more factors rather than fewer factors."
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#interpretation",
    "href": "sections/statistics/factor-analysis/EFA.html#interpretation",
    "title": "Exploratory Factor Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nRemember that the goal of exploratory factor analysis is to explore whether your data fits a model that makes sense. Ideally, you have a conceptual or theoretical framework for the analysis- a theory or body of literature guiding the development of an instrument, for example. Even if you do not, the results should be sensible in some way. You should be able to construct a simple narrative describing how each factor, and its constituent variables, makes sense and is easily labeled.\n\nFactor Loadings\nTabachnick and Fidell (2001) cite .32 as a good rule of thumb for the minimum loading of an item. Others say item loadings above .30 (Costello).\nClark and Watson, 1995 say > .35 Clark & Watson (1995)\nOthers say a factor loading coefficient ≥ 0.40 is considered necessary to judge the quality (validity) of an item as an indicator of one factor or another. Some may think that this criterion is quite lax since a factor loading of 0.40 means that only 16% of the explained variance depends on the factor of which one item is the indicator (Gana & Broc)\nSome psychometricians recommend that we choose items whose R² is greater than 0.40 (thus showing a factorial factor loading greater than 0.63).\n\nA “crossloading” item is an item that loads at .32 or higher on two or more factors. The researcher needs to decide whether a crossloading item should be dropped from the analysis, which may be a good choice if there are several adequate to strong loaders (.50 or better) on each factor.\nA factor with fewer than three items is generally weak and unstable; 5 or more strongly loading items (.50 or better) are desirable and indicate a solid factor.\n\nComrey and Lee (1992) suggest that loadings in excess of .71 (50% overlapping variance) are considered excellent, .63 (40% overlapping variance) very good, .55 (30% overlapping variance) good, .45 (20% overlapping variance) fair, and .32 (10% overlapping variance) poor.\none must be careful not to prematurely drop poorly performing items, especially when such items were predicted a priori to be strong markers of a given factor.\n\n\nNumber of indicators\n\n, it is important for the latent variable to have at least four indicators (for example: four items). In effect, with only three indicators, the measurement model is just-identified, thus making useless its testability. (Gana & Broc)\n5 or more strongly loading items (.50 or better) indicate a solid factor (Costello & Osborne, 2005)\n\n\n\nReliability\nNote that Cronbach’s alpha relies on the assumption of unidimensionality.\nInternal consistency:\nRecommended cut off points are:\n\n.80 in a basic science setting (Clark & Watson, 1995) Nunally 1978\n.9 or .5 in applied settings Nunally 1978\n\nTest-retest reliability:"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#recommended-reading",
    "href": "sections/statistics/factor-analysis/EFA.html#recommended-reading",
    "title": "Exploratory Factor Analysis",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThe summaries and conclusions above were largely taken from the following sources:\n\nBest Practices in Exploratory Factor Analysis by Osborne (2014)\nIn Search of Underlying Dimensions: The Use (and Abuse) of Factor Analysis in Personality and Social Psychology Bulletin by Russell (2002)\nEvaluating the Use of Exploratory Factor Analysis in Psychological Research by Fabrigar et al. (1999)\nCostello & Osborne (2005)"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#glossary",
    "href": "sections/statistics/factor-analysis/EFA.html#glossary",
    "title": "Exploratory Factor Analysis",
    "section": "Glossary",
    "text": "Glossary\n\ncommunality: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\nh2: See communality.\nu2: 1 - h2. residual variance, a.k.a. uniqueness"
  },
  {
    "objectID": "sections/statistics/factor-analysis/EFA.html#references",
    "href": "sections/statistics/factor-analysis/EFA.html#references",
    "title": "Exploratory Factor Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nBorsboom, D. (2006). The attack of the psychometricians. Psychometrika, 71(3), 425. https://doi.org/10.1007/s11336-006-1447-6\n\n\nClark, L. A., & Watson, D. (1995). Constructing validity: Basic issues in objective scale development. Psychological Assessment, 7(3), 309–319. https://doi.org/10.1037/1040-3590.7.3.309\n\n\nCostello, A., & Osborne, J. (2005). Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis. Practical Assessment, Research, and Evaluation, 10(1). https://doi.org/10.7275/jyj1-4868\n\n\nEdwards, J. R., & Bagozzi, R. P. (2000). On the nature and direction of relationships between constructs and measures. Psychological Methods, 5(2), 155–174. https://doi.org/10.1037/1082-989X.5.2.155\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4(3), 272–299. https://doi.org/10.1037/1082-989X.4.3.272\n\n\nKaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39(1), 31–36. https://doi.org/10.1007/BF02291575\n\n\nMacCallum, R. C., Widaman, K. F., Zhang, S., & Hong, S. (1999). Sample size in factor analysis. Psychological Methods, 4(1), 84–99. https://doi.org/10.1037/1082-989X.4.1.84\n\n\nMundfrom, D. J., Shaw, D. G., & Ke, T. L. (2005). Minimum sample size recommendations for conducting factor analyses. International Journal of Testing, 5(2), 159–168. https://doi.org/10.1207/s15327574ijt0502_4\n\n\nOsborne, J. W. (2014). Best practices in exploratory factor analysis. Createspace publishing.\n\n\nRuscio, J., & Brendan Roche. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24(2), 282–292. https://doi.org/10.1037/a0025697\n\n\nRussell, D. W. (2002). In search of underlying dimensions: The use (and abuse) of factor analysis in Personality and Social Psychology Bulletin. Personality and Social Psychology Bulletin, 28(12), 1629–1646. https://doi.org/10.1177/014616702237645\n\n\nTabachnick, B. G., & Fidell, L. S. (2013). Using multivariate statistics (6th ed.). Pearson.\n\n\nVelicer, W. F., & Jackson, D. N. (1990). Component analysis versus common factor analysis: Some issues in selecting an appropriate procedure. Multivariate Behavioral Research, 25(1), 1–28. https://doi.org/10.1207/s15327906mbr2501_1\n\n\nWidaman, K. F. (1993). Common factor analysis versus principal component analysis: Differential bias in representing model parameters? Multivariate Behavioral Research, 28(3), 263–311. https://doi.org/10.1207/s15327906mbr2803_1"
  },
  {
    "objectID": "sections/statistics/factor-analysis/CFA.html#recommended-reading",
    "href": "sections/statistics/factor-analysis/CFA.html#recommended-reading",
    "title": "Confirmatory Factor Analysis",
    "section": "Recommended Reading",
    "text": "Recommended Reading\n\nDynamic Fit Index Cutoffs for Confirmatory Factor Analysis Models by McNeish & Wolf (2021)\n\n\n\n\n\nKoran, J. (2020). Indicators per factor in confirmatory factor analysis: More is not always better. Structural Equation Modeling: A Multidisciplinary Journal, 27(5), 765–772. https://doi.org/10.1080/10705511.2019.1706527\n\n\nMcNeish, D., & Wolf, M. G. (2021). Dynamic fit index cutoffs for confirmatory factor analysis models. Psychological Methods. https://doi.org/10.1037/met0000425"
  },
  {
    "objectID": "sections/statistics/factor-analysis/r-packages.html",
    "href": "sections/statistics/factor-analysis/r-packages.html",
    "title": "R-packages",
    "section": "",
    "text": "This chapter lists useful R package that relate to performing factor analyses.\n\npsych\nlavaan\nlavaan.survey\nsemTools\nsemPower"
  },
  {
    "objectID": "sections/surveys/recommended-reading.html",
    "href": "sections/surveys/recommended-reading.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "The following papers are useful overview or review papers that are, in my mind, particular useful to read:\n\n\n\n\nGehlbach, H., & Brinkworth, M. E. (2011). Measure twice, cut down error: A process for enhancing the validity of survey scales. Review of General Psychology, 15(4), 380–387. https://doi.org/10.1037/a0025704\n\n\nWolf, M. G., Ihm, E., Maul, A., & Taves, A. (2019). Survey item validation. https://doi.org/10.31234/osf.io/k27w3"
  },
  {
    "objectID": "sections/surveys/response-options.html#number-of-response-options",
    "href": "sections/surveys/response-options.html#number-of-response-options",
    "title": "Response Options",
    "section": "Number of Response Options",
    "text": "Number of Response Options\nThe question of how many response options to use centers around two main concerns. The first is that more options means you can obtain a more fine-grained assessment of the characteristic that is being evaluated (e.g., attitude). In other words, your assessment is more precise. However, the question is how the number of options affects the reliability and the validity of the measurement. With more options, it becomes more difficult for people to distinguish between the different options (e.g., is “Strongly agree” reliably different from “Very strongly agree”?).\nTable 1 shows an overview of various studies in which the topic of response options was addressed. The studies vary in many ways, so the final conclusion should be a holistic interpretation of the results, rather than a simple tallying of the results. Note also that only empirical studies are included and not simulation studies. Simulation studies seem limited because they cannot address the plausible psychological limitation of people being unable to distinguish between many options.\n\nTable 1: Overview of empirical studies on the topic of response options.\n\n\n\n\n\n\n\nSource\nComparisons\nConclusion\n\n\n\n\nDonnellan & Rakhshani (2020)\n2- to 7-, and 11-point Likert\n5-point Likert or higher\n\n\nSimms et al. (2019)\n2- to 11-point Likert + VAS\n6-point Likert\n\n\nSung & Wu (2018)\n5-point Likert and VAS-RRP\nVAS-RPP\n\n\nCox et al. (2017)\n2- and 4-point Likert\nMixed\n\n\nLewis (2017)\n7-, 11-point Likert and VAS\nNo difference\n\n\nKuhlmann et al. (2017)\n5-point Likert and VAS\nNo difference\n\n\nHilbert (2016)\n2- and 5-point and VAS\nIt depends\n\n\nCapik & Gozum (2015)\n2- and 5-point Likert\nNo difference\n\n\nEutsler & Lang (2015)\n5-, 7-, 9-, and 11-point Likert\n7-point Likert\n\n\nFinn et al. (2015)\n2- and 4-point Likert\n4-point Likert\n\n\nRevilla et al. (2014)\n5-, 7-, 11-point Likert\n5-point Likert\n\n\nCox et al. (2012)\n2- and 4-point Likert\n4-point Likert\n\n\nJanhunen (2012)\n7-point Likert and 30-point VAR\nVAR\n\n\nDawes (2008)\n5-, 7-, and 10-point Likert\nNo difference\n\n\nWeng (2004)\n3- to 9-point Likert\n5-point or higher\n\n\nPreston & Colman (2000)\n2- to 11-point Likert and VAS\n7-, 9-, or 10-point Likert\n\n\nAlwin (1997)\n7- and 11-point Likert\n11-point Likert\n\n\nJaeschke et al. (1990)\n7-point Likert and VAS\nNo difference\n(slightly favor 7-point Likert)\n\n\nFlamer (1983)\n2- and 9-point Likert\n9-point Likert\n\n\nMatell & Jacoby (1971)\n2-point to 18-point Likert\nNo difference\n\n\nBendig (1954)\n2-, 3-, 5-, 7-, and 9-point Likert\nNo difference\n(maybe 3-point or higher)\n\n\n\nThere are also several review papers on the topic. Krosnick & Presser (2010) suggest that 7-point Likert scales are probably optimal. Lietz (2010) concludes a desirable Likert-scale consists of 5 to 8 response options. Similarly, Cox III (1980) recommends to use between 5 and 9 response options. Symonds (1924), in 1924, claims the optimum number is 7. Gehlbach & Brinkworth (2011) recommends using 5-points for unipolar items and 7-point for biopolar items.\nBesides psychometric properties it may also be worth taking into account respondent preference. This involves ease of use of the scale and whether the response options allow for sufficient variation for respondents to express their view. Preston & Colman (2000) found that respondents found scales with 5, 7, and 10 points easy to use (compared to fewer options and a VAS) and that they preferred scales with more response options to allow them to express themselves (7 or more). Other studies also show that respondents favor more options (Cox et al., 2017).\nNote that if time is of the essence, fewer response options are preferred.\nAnother relevant factor is whether the scale is bipolar or unipolar. Bipolar scales are symmetrical which means the number of options naturally increase as they need to match both sides of the spectrum. Unipolar items are only about one side, usually ranging from the absence of something to the presence of something (to a certain degree). Since it is harder to label a larger number of options for a unipolar scale, the number of options are likely to be smaller.\nConclusion: It appears that few response options (2 or 3) should definitely be avoided. More response options therefore seems better, but benefits seem to quickly level off. Given other concerns, such as ease of use and interpretability, a 7-point Likert scale seems to be preferred for bipolar scales and a 5-point Likert scale for unipolar scales."
  },
  {
    "objectID": "sections/surveys/response-options.html#odd-vs.-even-response-options",
    "href": "sections/surveys/response-options.html#odd-vs.-even-response-options",
    "title": "Response Options",
    "section": "Odd vs. Even Response Options",
    "text": "Odd vs. Even Response Options\nThe middle option of a scale can have an ambiguous meaning. Participants may use it to indicate a moderate standing on the issue (Rugg and Cantril, 1944), a lack of an opinion (Nadler, Weston, and Voyles, 2014), ambivalence (Klopfer and Madden, 1980; Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), indifference (Schaeﬀer and Presser, 2003; Nadler, Weston, and Voyles, 2014), uncertainty (Baka, Figgou, and Triga, 2012; Nadler, Weston, and Voyles, 2014), confusion, or to signal context dependence (e.g., “it depends” or disputing the question, see Baka, Figgou, and Triga, 2012).\nThe middle option may also be used for certain response styles, such as socially desirable responding (Sturgis, Roberts, and Smith, 2012) or satisficing (Krosnick, 1991), although there is not much research showing it actually leads to satisficing Wang & Krosnick (2020).\nAf a middle alternative is explicitly oﬀered, the proportion endorsing it increases dramatically (e.g. Ayidiya & McClendon, 1990; Bishop, 1987; Bishop, Hippler, Schwarz, & Strack, 1988; Kalton, Collins, & Brook, 1978; Kalton, Roberts, & Holt, 1980; Rugg & Cantril, 1944).\nSome studies show that not including a middle option decreases validity and increases measurement error (O’Muircheartaigh, Krosnick, and Helic, 1999; Kahn, and Dhar, 2002)\nRecent study on this: Wang & Krosnick (2020)\nAn alternative approach to this issue is to use branching. Respondents could first be asked whether they fall at the midpoint or on one side, followed by a question about their extremity on a side. This approach was found to be more reliable and valid than using a 7-point scale (Krosnick and Berent, 1993; Malhotra, Krosnick, and Thomas, 2009).\nConclusion: If it is possible that respondents may have a moderate view, it seems crucial for it to be possible to capture this view. Limitations of a middle option could then be addressed in other ways (e.g., clear questions)."
  },
  {
    "objectID": "sections/surveys/response-options.html#response-option-labeling",
    "href": "sections/surveys/response-options.html#response-option-labeling",
    "title": "Response Options",
    "section": "Response Option Labeling",
    "text": "Response Option Labeling\nThere are several studies that show all response options should be labelled, rather than only labeling the end points (Krosnick & Berent, 1993; Weng, 2004).\nFor an example of biopolar labels for a 2- to 11-point Likert scale, see Table 1.\n\n\nTable 1: Likert response labels from Simms et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabel\n2-point\n3-point\n4-point\n5-point\n6-point\n7-point\n8-point\n9-point\n10-point\n11-point\n\n\n\n\nVery strongly disagree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\nStrongly disagree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nDisagree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nMostly disagree\n\n\n\n\n\n\n\n\nx\nx\n\n\nSlightly disagree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nNeither agree nor disagree\n\nx\n\nx\n\nx\n\nx\n\nx\n\n\nSlightly agree\n\n\n\n\nx\nx\nx\nx\nx\nx\n\n\nMostly agree\n\n\n\n\n\n\n\n\nx\nx\n\n\nAgree\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nStrongly agree\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nVery strongly agree\n\n\n\n\n\n\nx\nx\nx\nx\n\n\n\n\nIt is also recommended to avoid agree-disagree response labels because asking respondents to rate their level of agreement is a cognitively demanding task that increases respondent error and reduces responding effort (Gehlbach & Brinkworth, 2011)."
  },
  {
    "objectID": "sections/surveys/response-options.html#references",
    "href": "sections/surveys/response-options.html#references",
    "title": "Response Options",
    "section": "References",
    "text": "References\n\n\n\n\nAlwin, D. F. (1997). Feeling thermometers versus 7-point scales: Which are better? Sociological Methods & Research, 25(3), 318–340. https://doi.org/10.1177/0049124197025003003\n\n\nBendig, A. W. (1954). Reliability and the number of rating-scale categories. Journal of Applied Psychology, 38(1), 38–40. https://doi.org/10.1037/h0055647\n\n\nCapik, C., & Gozum, S. (2015). Psychometric features of an assessment instrument with Likert and dichotomous response formats. Public Health Nursing, 32(1), 81–86. https://doi.org/10.1111/phn.12156\n\n\nCox, A., Courrégé, S. C., Feder, A. H., & Weed, N. C. (2017). Effects of augmenting response options of the MMPI-2-RF: An extension of previous findings. Cogent Psychology, 4(1), 1323988. https://doi.org/10.1080/23311908.2017.1323988\n\n\nCox, A., Pant, H., Gilson, A. N., Rodriguez, J. L., Young, K. R., Kwon, S., & Weed, N. C. (2012). Effects of augmenting response options on MMPI2 RC scale psychometrics. Journal of Personality Assessment, 94(6), 613–619. https://doi.org/10.1080/00223891.2012.700464\n\n\nCox III, E. P. (1980). The optimal number of response alternatives for a scale: A review. Journal of Marketing Research, 17(4), 407. https://doi.org/10.2307/3150495\n\n\nDawes, J. (2008). Do data characteristics change according to the number of scale points used? An experiment using 5-point, 7-point and 10-point scales. International Journal of Market Research, 50(1), 61–104. https://doi.org/10.1177/147078530805000106\n\n\nDonnellan, B., & Rakhshani, A. (2020). How does the number of response options impact the psychometric properties of the rosenberg self-esteem scale? https://doi.org/10.31234/osf.io/fnywz\n\n\nEutsler, J., & Lang, B. (2015). Rating scales in accounting research: The impact of scale points and labels. Behavioral Research in Accounting, 27(2), 35–51. https://doi.org/10.2308/bria-51219\n\n\nFinn, J. A., Ben-Porath, Y. S., & Tellegen, A. (2015). Dichotomous versus polytomous response options in psychopathology assessment: method or meaningful variance? Psychological Assessment, 27(1), 184–193. https://doi.org/10.1037/pas0000044\n\n\nFlamer, S. (1983). Assessment of the multitrait-multimethod matrix validity of Likert scales via confirmatory factor analysis. Multivariate Behavioral Research, 18(3), 275–306. https://doi.org/10.1207/s15327906mbr1803_3\n\n\nGehlbach, H., & Brinkworth, M. E. (2011). Measure twice, cut down error: A process for enhancing the validity of survey scales. Review of General Psychology, 15(4), 380–387. https://doi.org/10.1037/a0025704\n\n\nHilbert, S. (2016). The influence of the response format in a personality questionnaire: An analysis of a dichotomous, a Likert-type, and a visual analogue scale. TPM - Testing, Psychometrics, Methodology in Applied Psychology, 1, 3–24. https://doi.org/10.4473/TPM23.1.1\n\n\nJaeschke, R., Singer, J., & Guyatt, G. H. (1990). A comparison of seven-point and visual analogue scales: Data from a randomized trial. Controlled Clinical Trials, 11(1), 43–51. https://doi.org/10.1016/0197-2456(90)90031-V\n\n\nJanhunen, K. (2012). A comparison of Likert-type rating and visually-aided rating in a simple moral judgment experiment. Quality & Quantity, 46(5), 1471–1477. https://doi.org/10.1007/s11135-011-9461-x\n\n\nKrosnick, J. A., & Berent, M. K. (1993). Comparisons of party identification and policy preferences: The impact of survey question format. American Journal of Political Science, 37(3), 941. https://doi.org/10.2307/2111580\n\n\nKrosnick, J. A., & Presser, S. (2010). Question and questionnaire design (Second edition). Emerald.\n\n\nKuhlmann, T., Dantlgraber, M., & Reips, U.-D. (2017). Investigating measurement equivalence of visual analogue scales and Likert-type scales in Internet-based personality questionnaires. Behavior Research Methods, 49(6), 2173–2181. https://doi.org/10.3758/s13428-016-0850-x\n\n\nLewis, J. R. (2017). User experience rating scales with 7, 11, or 101 points: Does it matter? Journal of Usability Studies, 12(2), 19.\n\n\nLietz, P. (2010). Research into questionnaire design: A summary of the literature. International Journal of Market Research, 52(2), 249–272. https://doi.org/10.2501/S147078530920120X\n\n\nMatell, M. S., & Jacoby, J. (1971). Is there an optimal number of alternatives for Likert scale items? Study I: Reliability and validity. Educational and Psychological Measurement, 31(3), 657–674. https://doi.org/10.1177/001316447103100307\n\n\nPreston, C. C., & Colman, A. M. (2000). Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences. Acta Psychologica, 104(1), 1–15. https://doi.org/10.1016/S0001-6918(99)00050-5\n\n\nRevilla, M. A., Saris, W. E., & Krosnick, J. A. (2014). Choosing the number of categories in agreedisagree scales. Sociological Methods & Research, 43(1), 73–97. https://doi.org/10.1177/0049124113509605\n\n\nSimms, L. J., Zelazny, K., Williams, T. F., & Bernstein, L. (2019). Does the number of response options matter? Psychometric perspectives using personality questionnaire data. Psychological Assessment, 31(4), 557–566. https://doi.org/10.1037/pas0000648\n\n\nSung, Y.-T., & Wu, J.-S. (2018). The Visual Analogue Scale for Rating, Ranking and Paired-Comparison (VAS-RRP): A new technique for psychological measurement. Behavior Research Methods, 50(4), 1694–1715. https://doi.org/10.3758/s13428-018-1041-8\n\n\nSymonds, P. M. (1924). On the loss of reliability in ratings due to coarseness of the scale. Journal of Experimental Psychology, 7(6), 456–461. https://doi.org/10.1037/h0074469\n\n\nWang, R., & Krosnick, J. A. (2020). Middle alternatives and measurement validity: A recommendation for survey researchers. International Journal of Social Research Methodology, 23(2), 169–184. https://doi.org/10.1080/13645579.2019.1645384\n\n\nWeng, L.-J. (2004). Impact of the number of response categories and anchor labels on coefficient alpha and test-retest reliability. Educational and Psychological Measurement, 64(6), 956–972. https://doi.org/10.1177/0013164404268674"
  },
  {
    "objectID": "sections/surveys/index.html#references",
    "href": "sections/surveys/index.html#references",
    "title": "Overview",
    "section": "References",
    "text": "References\n\n\n\n\nGehlbach, H., & Brinkworth, M. E. (2011). Measure twice, cut down error: A process for enhancing the validity of survey scales. Review of General Psychology, 15(4), 380–387. https://doi.org/10.1037/a0025704\n\n\nSimms, L. J. (2008). Classical and modern methods of psychological scale construction: Scale construction. Social and Personality Psychology Compass, 2(1), 414–433. https://doi.org/10.1111/j.1751-9004.2007.00044.x"
  },
  {
    "objectID": "sections/surveys/item-development.html#pilot-testing",
    "href": "sections/surveys/item-development.html#pilot-testing",
    "title": "Item Development",
    "section": "Pilot testing",
    "text": "Pilot testing\nAfter the initial item pool is complete, it makes sense to pilot test the items before running a large-scaled exploratory study.\n\n\n\n\nSimms, L. J. (2008). Classical and modern methods of psychological scale construction: Scale construction. Social and Personality Psychology Compass, 2(1), 414–433. https://doi.org/10.1111/j.1751-9004.2007.00044.x"
  },
  {
    "objectID": "sections/WIP/item-development.html#number-of-items",
    "href": "sections/WIP/item-development.html#number-of-items",
    "title": "Item Development",
    "section": "Number of items",
    "text": "Number of items\nThere are no hard-and-fast rules guiding this decision, but keeping a measure short is an effective means of minimizing response biases caused by boredom or fatigue (Schmitt & Stults, 1985; Schriesheim & Eisenbach, 1990). Additional items also demand more time in both the development and administration of a measure (Carmines & Zeller, 1979). Harvey, Billings, and Nilan (1985) suggest that at least four items per scale are needed to test the homogeneity of items within each latent construct. Adequate internal consistency reliabilities can be obtained with as few as three items (Cook et al., 1981), and adding items indefinitely makes progressively less impact on scale reliability (Carmines & Zeller, 1979). It is difficult to improve on the internal consistency reliabilities of five appropriate items by adding items to a scale (Hinkin, 1985; Hinkin & Schriesheim, 1989; Schriesheim & Hinkin, 1990). Cortina (1993) found that scales with many\nitems may have high internal consistency reliabilities even if item intercorrelations are low, an argument in favor of shorter scales with high internal consistency. It is also important to assure that the domain has been adequately sampled, as inadequate sampling is a primary source of measurement error (Churchill, 1979). As Thurstone (1947) points out, scales should possess simple structure, or parsimony. Not only should any one measure have the simplest possible factor constitution, but any scale should require the contribution of a minimum number of items that adequately tap the domain of interest. These findings would suggest that the eventual goal will be the retention of four to six items for most constructs, but the final determination must be made only with accumulated evidence in support of the construct validity of the measure. It should be anticipated that approximately one half of the created items will be retained for use in the final scales, so at least twice as many items as will be needed in the final scales should be generated to be administered in a survey questionnaire.\nhttps://twitter.com/dingding_peng/status/1481683536499331079\nhttps://psyarxiv.com/4kra2/"
  },
  {
    "objectID": "index.html#word-of-caution",
    "href": "index.html#word-of-caution",
    "title": "About",
    "section": "Word of Caution",
    "text": "Word of Caution\nSome sections in the book are not yet finished. They’re work in progress. Sometimes I think they’re already a bit useful, so I make them public. If I think they’re not finished/polished yet, I put a warning at the start of the page to indicate that.\nHaving said that, this website will forever be a work in progress because the content is about current best practices. As best practices are likely to change over time, so the content of this website will change with it. If I realize that something is heavily outdated, I will make a note of it.\nIt’s also very likely that there are mistakes. These mistakes can theoretically range from gross errors to simple typos, or a reliance on outdated information. If you find a mistake, please contact me or click on the GitHub link on each page and create an Issue."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe following people have contributed to this book:\n\nWillem Sleegers\n\nIf I’ve missed your contributions and you deserve to be on this list, please don’t hesitate to contact me or add yourself via a Pull Request on GitHub!"
  }
]